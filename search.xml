<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《Prompt engineering》笔记]]></title>
    <url>%2F2023%2F05%2F06%2Fprompt%2F</url>
    <content type="text"><![CDATA[[Updated] 本文总结自吴恩达的《Prompt engineering》。 Prompt 笔记两个原则 编写明确和具体的指令 在许多情况下，更长的提示实际上提供了更多清晰度和上下文，可以得到更详细更相关的输出。 策略： 使用分隔符清楚地指示输入的不同部分/特定文本 eg. 用双引号“”标识特定内容，prompt 中编写对双引号内的内容进行提炼汇总以明确提示 要求结构化输出 eg. 指定 html、json 格式，以 python 列表的形式输出内容 要求模型检查是否满足条件 eg. 添加指定条件，比如概括双引号内的文本，如果文本内容有明显先后顺序则按序号顺序输出，文否则分点输出并说明未按顺序输出 少量训练提示：在要求模型执行任务之前，提供成功执行任务的示例 给模型足够的时间思考 指定完成任务所需步骤与具体形式 e.g. 如第一步提取文本中的名字以“”输出，第二步提取文本中的书籍以《》输出 让模型在得出结论/回答之前给出推理过程/解决方案 e.g. 给出问题并附上我们的方案A，先让模型计算自己的方案B，再对比检查提交的方案A（直接给模型初步方案让其检查验证可能无法发现错漏）。如请你证明结论C并输出证明过程A，然后验证某证明过程B是否有误。注意，在验证 B 是否有误之前，你不能参考证明B，必须独立得到证明过程A。 模型结论出错，尝试重新构建查询请求相关推理的链或序列 复杂任务分解成逐个小任务 迭代 prompt 的开发模型局限性幻觉：模型不了解知识的边界，对于模糊的主题编造看似合理实则错误的内容。 策略：要求模型先从文本中找出任何相关的引用，让模型根据这些引用来回答问题，通过回溯文档帮助减少幻觉。 迭代开发每次 Prompt 基于上次回答不断进行调整，迭代逼近我们想要的内容。 应用场景 文本总结摘要 文本推理提取 将一段文本转换为另一段文本（转化格式、纠错校对） 文本拓展（根据一组说明、主题列表进行扩写、头脑风暴） 建立自定义聊天机器人 Tips 中文 prompt 可能是英文 prompt 的 1.x 倍的 token 数量，一方面成本偏高，另一方面较长 prompt 可能超过上下文上限]]></content>
      <categories>
        <category>chatgpt</category>
      </categories>
      <tags>
        <tag>chatgpt</tag>
        <tag>prompt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new bing 在说谎？]]></title>
    <url>%2F2023%2F04%2F16%2Fliar-chatgpt%2F</url>
    <content type="text"><![CDATA[[Updated] 是故意的还是不小心的？ 突发奇想测试 new bing 中英对照提问的回答是否会存在关联或相似内容，于是分别提问 How do you know that 1+1=2? 和 你怎么知道1+1=2？，结果得到了中英严格对照翻译的答案。 =。= 过程如下： 他最好只是在偷懒，而不是在说谎。]]></content>
      <categories>
        <category>chatgpt</category>
      </categories>
      <tags>
        <tag>chatgpt</tag>
        <tag>new bing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ChatGPT 与 SE 的区别]]></title>
    <url>%2F2023%2F04%2F12%2Fchatgpt-se%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录当前使用 ChatGPT 与 SE 时感受到的主要区别。 ChatGPT 与 SE 的区别 SE ChatGPT 使用形式 搜索框输入，返回排序后的相关链接 上下文对话 优 点 回馈信息量大而全；回馈迅速；内容形式多样（图文音频）；根据历史偏好个性化推荐 可以理解上下文根据历史对话进行回馈；交互形式更自然不机械；创造性自生成文本（主题写作、内容扩写）；能处理复杂文本任务（文本总结、代码生成） 缺 点 根据关键词进行匹配，单次回馈，不能理解上下文；信息可能不准确不可靠；排序结果受竞价影响 仅能回馈有限文本信息且知识边界依赖于训练数据；回馈可能不准确、不合理、不一致、不稳定；自生成文本可能瞎编甚至有违法律道德 ChatGPT 对搜索引擎的影响机会搜索引擎集成 ChatGPT 可有以下优势： 提高互动性：ChatGPT 集成进搜索引擎和浏览器，提供更直接和人性化的回答 返回内容更符合用户意图 丰富功能多样性（文本总结、代码生成） 威胁ChatGPT 对搜索引擎存在的威胁如下： 市场抢占：ChatGPT 某种程度上可以取代搜索引擎的功能，甚至交互方式更自然，理解与回答更准确简洁 质量威胁：ChatGPT 可以生成质量不一、真实存疑的内容，生成高质量内容的同时也可能产生垃圾，长此以往可能会影响传统搜索引擎的内容来源和质量 ChatGPT 对社会的影响正面 为用户提供更加个性便捷的服务 提高文字信息方面的生产效率和质量 给教育、咨询等文字沟通行业提供新的可能形式与发展潜力 负面 内容的真实性和可信度 数据的隐私和安全 改变就业市场格局，淘汰部分职业、取代部分职能，市场继续向信息化人群倾斜 人工智能的伦理安全 个人感受积极 利好搭建基于大模型结合垂直领域知识的专家系统 未来每个人都可以拥有独属且适合自身的专属助手 疑虑 信息真实性与可靠性 — 加强信息溯源与核实。 发展迅猛，对 AI 控制的边界 — 主动软件限制？被动法律限制？]]></content>
      <categories>
        <category>chatgpt</category>
      </categories>
      <tags>
        <tag>chatgpt</tag>
        <tag>new bing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[离线数仓笔记]]></title>
    <url>%2F2022%2F08%2F28%2Fdw%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录搭建离线过程中收集的知识点。 离线数仓笔记搭建过程思路 在满足日常需求的过程中总结常用需求，研发公共数据层； 获得领导支持，以获得更多研发资源； 在满足业务需求的过程中，根据业务需求的变化不断对公共数据层进行迭代和优化； 随着时间的推移 ，越来越多的日常业务需求可以用公共数据层来完成； 日常业务需求开发和公共数据层构建是相互促进的循环。 流程 汇总数据源到 ods 层 划分主题域，构建总线矩阵 整合事实表 整合维度表（构建一致性维度） ETL 开发 应用迁移 元数据元数据（Meta Data）是描述数据的数据。 作用： 元数据查看 浏览表的结构、字段信息、数据模型、指标信息等。 数据血缘（追溯上游） 定义：以历史事实的方式记录数据的来源、处理过程等。 场景：在数据分析中发现问题数据的时候，通过血缘关系追根溯源，快速地定位到问题数据的来源和加工流程，减少分析的时间和难度。 影响性分析（影响下游） 定义：分析出数据的下游流向。 场景：当系统进行升级改造的时候，如果修改了数据结构、ETL程序等元数据信息，通过影响性分析，可快速定位出元数据修改会影响到哪些下游系统，从而减少系统升级改造带来的风险。 数据冷热度分析 定义：对数据表的被使用情况进行统计。 场景：从访问频次和业务需求角度出发，进行数据冷热度分析，用图表的方式，展现表的重要性指数，对冷热度不同的数据做分层存储或评估下线，以更好地利用HDFS资源。 数据资产地图 在宏观层面组织信息，以全局视角对信息进行归并、整理，展现数据量、数据变化情况、数据存储情况、整体数据质量等信息，为数据管理部门和决策者提供参考。 数据建模模型表类型事实表（Fact Table）描述业务内特定事件的数据，譬如：用户的一个操作（访问、购买、退单）。 维度表（DIM，Dimension）又称维表或查找表（Lookup Table），保存了维度的属性值，可以跟事实表做关联，相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。维度表主要包含类： 高基数维度数据：一般是用户资料表、商品资料表类似的资料表，数据量可能是千万级或者上亿级别 低基数维度数据：一般是配置表，比如枚举字段对应的中文含义，或者日期维表等；数据量可能就是个位数或者几千几万 星型模型(Star Schema)最常用的维度建模方式。 星型模式是以事实表为中心，所有的维度表直接连接在事实表上，像星星一样。星形模式的维度建模由一个事实表和一组维度表成，且具有以下特点： 维表只和事实表关联，维表之间没有关联； 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键； 以事实表为核心，维表围绕核心呈星形分布； 雪花模型(Snowflake Schema)星形模式的扩展。 雪花模式的维度表可以拥有其他维度表的，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能也比星型模型要低。 所以一般不是很常用。 星座模型由星型模式延伸而来。星型模式是基于一张事实表的，而星座模式是基于多张事实表的，而且共享维度信息。 前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。 在业务发展后期，绝大部分维度建模都采用的是星座模式。 维度建模Kimball的多维体系结构（MD）中的三个关键性概念之一： 一致性维度（Conformed Dimension） 一致性事实（Conformed Fact） 总线矩阵（Bus Architecture） 数仓体系分层原因 数据结构清晰：每层都有其作用域和职责，使用时便于定位理解 空间换时间：允许冗余，通用的中间层能减少重复计算 统一数据口径：通过数据分层，提供统一的数据输入/输出口径 简化复杂任务：每层只解决特定的问题，逻辑简单保证正确性，出错后易溯源易调整 分层场景由简单到复杂依次为： 时间紧任务重，急于看结果 不分层：直接连各个业务数据库，抽取数据到大数据平台，根据需求组合join或者汇总count、sum就行。作者现在公司服务的数仓项目前身就是这样，将各个业务系统数据抽取到oracle，你看都没有大数据平台就做了。 公司业务简单，且相对比较固定，数据来源不多，结构也很清晰，需求也不多 通用的数仓架构：ODS起到解耦业务数据库+异构数据源的问题，DWD解决数据脏乱差的问题，DWS复用的指标计算，ADS直接面向前台业务需求。 公司业务复杂，业务变化较快 通用的数仓架构 + DWT 层：多一层DWT层做汇总，多一层解耦，业务变化的时候，我们只改DWS层就好了，最多穿透到DWT层。业务变化的时候调整一下，工作量也不会太大，最重要的是能保证底层结构的稳定和数据分析的可持续性。 公司业务较为复杂，集团性公司，下辖多个部门bu事业线，bu间业务内容交叉不大 通用的数仓架构 + DWT 层 + DM 层：各个数据集市层，单独供数，甚至有单独的计算资源，这样可以避免因为计算任务代码混在一起、数据权限拆分等问题带来的数据变更成本。 要点 越接近上层，服务范围越小、越精准、越易用，访问量级越小 中间层表 DWS/DWT 应该能撑起 80% 的需求，最好不要暴露原始数据（即 ODS） 如何分层 数据引入层ODS（Operation Data Store）：解耦业务数据库，汇总异构数据源，存放未经过处理的原始数据至数据仓库系统，结构上与源系统保持一致，是数据仓库的数据引入准备区和备份。 公共数据层CDM（Common Data Model，又称通用数据模型层）：包括DIM维度表、DWD和DWS，由ODS层数据加工而成。主要完成数据加工与整合，建立一致性的维度，构建可复用的面向分析和统计的明细事实表，以及汇总公共粒度的指标。 明细数据层（DWD，Data Warehouse Detail）：以业务过程作为建模驱动，基于每个具体的业务过程特点，构建最细粒度的明细层事实表。可以结合企业的数据使用特点，将明细事实表的某些重要维度属性字段做适当冗余，即宽表化处理。明细粒度事实层的表通常也被称为逻辑事实表。 e.g. 用户行为日志表中行为字段含 — 访问、加购、购买，那么分别抽取每个具体行为构建最细粒度事实表 — 用户访问表、用户加购表、用户购买表。 公共维度层（DIM）：基于维度建模理念思想，建立整个企业的一致性维度。降低数据计算口径和算法不统一风险。公共维度层的表通常也被称为逻辑维度表，维度和维度逻辑表通常一一对应。 e.g. 地区 Id、地区名、地区号；日期、日期所属周、星期、是否特殊节日、节日信息。 公共汇总层（DWS，Data Warehouse Summary）：以分析的主题对象作为建模驱动，基于上层的应用和产品的指标需求，构建公共粒度的汇总指标事实表，以宽表化手段物理化模型。构建命名规范、口径一致的统计指标，为上层提供公共指标，建立汇总宽表、明细事实表。公共汇总粒度事实层的表通常也被称为汇总逻辑表，用于存放派生指标数据。 e.g. 对用户访问表、用户加购表、用户购买表汇总成宽表 — 用户 Id、访问次数、加购次数、购买次数。 数据应用层：存放数据产品个性化的统计指标数据。根据 CDM 与 ODS 层加工生成。 衡量标准 https://zhuanlan.zhihu.com/p/401982516 完善度 DWD 跨层引用率：ODS层直接被DWS/ADS/DM层引⽤的表，占所有ODS层表（仅统计活跃表）⽐例。 跨层引⽤率越低越好，在数据中台模型设计规范中，要求不允许出现跨层引⽤，ODS层数据只能被DWD引⽤。 DWS/ADS/DIM层完善度：DWS/ADS/DM层的查询占所有查询的⽐例。 考核汇总数据的完善度，主要看汇总数据能直接满⾜多少查询需求（也就是⽤汇总层数据的查询⽐例衡量）。如果汇总数据⽆法满⾜需求，使⽤数据的⼈就必须使⽤明细数据，甚⾄是原始数据。 复用度 模型引用系数：⼀个模型被读取，直接产出下游模型的平均数量。引⽤系数越⾼，说明数仓的复⽤性越好。 e.g. ⼀张DWD层表被 N 张DWS层表引⽤，这张DWD层表的引⽤系数就是 N。所有DWD层表（有下游表的）引⽤系数取平均，即DWD层表平均模型引⽤系数，⼀般低于2⽐较差，3以上相对⽐较好（经验值）。 规范度 没有主题域、业务过程归属的表的数量占比 这类表难以查找，也无法复用。 不规范命名的表的数量占比 ⼀个规范的表命名应该包括 DM、主题域、分层、日期、全量/增量快照 等信息。 字段命名不一致的表的数量占比 相同的字段在不同的模型中，它的命名必须是⼀致的，避免歧义和不确定性。 Issue ods 与 dwd 区别 现实中接触的情况是 ods 层的数据很难保证质量，数据的来源和形式多种多样（e.g. ods 层文件的数据源不同，有些是 json，有些是日志，有些来自业务数据库）； 通过额外的一层 dwd 来屏蔽一些底层的差异（清洗、规范化），统合异构数据源。 ods 清洗到 dwd 尽量保留原始信息，又相对干净。简单处理、少清洗是为了避免增加溯源、排错成本。类似简单 EDA： 数据标准化，字段类型、命名统一 默认值填充 异常值处理转化 字段处理：选择、转换（ip转数值）、拆解 从 dwd 层到 dws 层后就可以直接给到应用层的指标汇总表算是dws层的表还是ads层的表？ dws 可以绕过 ads 被应用直接访问的。一般情况下，能被多个应用共享的，归入dws层；单个应用专属而不被其他使用的，归入ads。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hive 笔记集合]]></title>
    <url>%2F2022%2F08%2F16%2Fhive%2F</url>
    <content type="text"><![CDATA[[Updating] 本文持续记录与 Hive 相关的知识点。 Hive collections内部表和外部表的区别创建：未被 external 修饰的是内部表（managed table） ，被 external 修饰的为外部表（external table）。 区别： 管理权限 内部表由 Hive 管理，外部表由 hdfs 管理。 数据位置 内部表数据存储位置位于 hive.metastore.warehouse.dir（默认：/user/hive/warehouse），外部表数据的存储位置由自己制定（如果没有 LOCATION，Hive 将在 HDFS 上的 /user/hive/warehouse 文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）。 删除 内部表会删除直接删除元数据（metadata）及存储数据； 删除外部表仅仅会删除元数据，HDFS 上的文件并不会被删除。 ORDER BY / SORT BY / DISTRIBUTE BY / CLUSTER BY ORDER BY 会对输入做全局排序，因此只有一个 reducer（多个 reducer 无法保证全局有序）。由于只有一个 reducer，会导致大规模数据的 ORDER BY 计算时间很长，效率低下。 SORT BY 不是全局排序，其在数据进入 reducer 前完成排序。因此，如果用 SORT BY 进行排序，并且设置 mapred.reduce.tasks&gt;1，则 SORT BY 只保证每个 reducer 的输出有序，不保证全局有序。 DISTRIBUTE BY 对指定字段进行分区。 DISTRIBUTE BY 的分区规则是根据分区字段的 hash 码与 reduce 的个数进行模除后，余数相同的分到一个区。 Hive 要求 DISTRIBUTE BY 语句要写在 SORT BY 语句之前 不配合多 reducer 处理看不出效果 CLUSTER BY 当 DISTRIBUTE BY 和 SORT BY 字段相同时，可以使用 CLUSTER BY 方式，兼具二者功能，但只能是升序排序， 不能指定 ASC 或 DESC。 分区/分桶 分区 PARTITIONED BY 分区表实际上就是对应一个 HDFS 文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。 Hive 中的分区就是分目录，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过 WHERE 子句中的表达式选择查询所需要的指定的分区，提高查询效率。即： 隔离数据 优化查询 针对：数据的存储路径。 分桶 INTO &lt;BUCKETS_N&gt; BUCKETS 对于一张表或者分区， Hive 可以进一步组织成桶，即更细粒度的数据范围划分。 针对：数据的存储文件。 抽样查询 TABLESAMPLE(BUCKET x OUT OF y) 其中，x 必须 ≤ y。 数据倾斜排查大 key（绝大多数情况） 原因某些 reduce 处理的数据量明显大于其它 reduce 判断 【8088-&gt;job】Elapsed time 存在明显差异。注： 若每个 reduce 时间都特别长，则不一定是数据倾斜，而是 reduce 设置过少 若 task 的执行节点（硬件）存在问题，可能会导致任务跑的特别慢。 此时，mapreduce 的推测执行会重启一个任务，若新任务能在很短时间内完成，则说明是原 task 执行节点的问题导致个别 task 慢；反之，更能映证该 task 可能存在数据倾斜。 【8088-&gt;task】Counter value 存在明显差异。 sql 定位 定位卡住的 stage 通过 jobname（尾部默认带 stage） 定位 任务日志 打开执行慢的 task 的任务日志 Ctrl+F 搜索 CommonJoinOperator: JOIN struct ，其后即 join 的 key 信息（如：struct&lt;_col0:string, _col1:string,...） 查看 sql 的执行计划中， join 的 key 信息 所处的 stage 确定执行 sql 查看执行计划，根据定位的 stage，锁定问题 sql 区域（如：该 stage 涉及哪张表，对应哪段 sql） 任务超时被 kill 原因Reduce 处理的数据量巨大，在做 full gc 的时候，stop the world。导致响应超时（默认 600s），任务被杀掉。 判断常见报错如： 1... Timed out after 600 secs Container killed by the ApplicationMaster. Container killed onrequest. Exit code 143 ... 解决 过滤脏数据：去掉无意义的大 key；列裁剪 数据预处理：赋予大 key、 NULL 随机值，使其打散避免聚集 增加 reduce 个数：降低大 key 聚集的概率 1set mapred.reduce.tasks=15; 表一大一小时用 mapjoin 12set hive.auto.convert.join=true; -- 启用自动 mapjoinset hive.mapjoin.smalltable.filesize=100000000; -- mapjoin的表size大小 倾斜连接优化 12set hive.optimize.skewjoin=true; -- 启用set hive.skewjoin=200000; -- 超过20万行就认为该键是偏斜连接键 开大内存：适用于因内存超限导致任务被 kill 的情况，但不一定会明显降低执行时间 1set mapreduce.reduce.memory.mb=5120; -- reduce 内存大小 hive 优化小文件过多小文件过多的原因 通过查询方式加载数据（生产环境常见） 1insert overwrite table A select s_id,c_name,s_score from B; 这种方式是生产环境中常用的，也是最容易产生小文件的方式。insert 导入数据时会启动 MR 任务，MR 中 reduce 有多少个就输出多少个文件，所以，文件数量=ReduceTask数量×分区数。也有很多简单任务没有reduce，只有map阶段，则文件数量=MapTask数量×分区数。 每执行一次 insert 时 hive 中至少产生一个文件，因为 insert 导入时至少会有一个MapTask。像有的业务需要每10分钟就要把数据同步到 hive 中，这样产生的文件就会很多。 通过 load 方式加载数据 12load data local inpath '/export/score.csv' overwrite into table A; -- 导入文件load data local inpath '/export/score' overwrite into table A; -- 导入文件夹 使用 load 方式可以导入文件或文件夹，当导入一个文件时，hive表就有一个文件，当导入文件夹时，hive表的文件数量为文件夹下所有文件的数量。 直接向表中插入数据（生产环境罕见） 1insert into table A values (1,'zhangsan',88),(2,'lisi',61); 这种方式每次插入时都会产生一个文件，多次插入少量数据就会出现多个小文件。 小文件过多的影响 首先对底层存储 HDFS 来说，HDFS 本身就不适合存储大量小文件，小文件过多会导致namenode元数据特别大,占用太多内存，严重影响HDFS的性能 对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个 Map 任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 Map 数量是受限的。 小文件过多的解决方法 使用 hive 自带的 concatenate 命令，自动合并小文件 新集群 没有历史遗留问题的话，建议 hive 使用 orc 文件格式，以及启用 lzo 压缩。这样小文件过多可以使用hive自带命令 concatenate 快速合并。 现有集群 使用方法： 12345# 对于非分区表alter table A concatenate;# 对于分区表要指定分区alter table B partition(day=20201224) concatenate; 实例： 注意： concatenate 命令只支持 RCFILE 和 ORC 文件类型。 使用 concatenate 命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。 当多次使用 concatenate 后文件数量不再变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize=256mb 的设置有关，可设定每个文件的最小 size。 123456789101112131415161718# 执行以上三条语句，则A表下就会有三个小文件,hive (default)&gt; insert into table A values (1,'aa',67), (2,'bb',87);hive (default)&gt; insert into table A values (3,'cc',67), (4,'dd',87);hive (default)&gt; insert into table A values (5,'ee',67), (6,'ff',87);# 在hive命令行执行如下语句，查看A表下文件数量hive (default)&gt; dfs -ls /user/hive/warehouse/A;Found 3 items-rwxr-xr-x 3 root supergroup 378 2020-12-24 14:46 /user/hive/warehouse/A/000000_0-rwxr-xr-x 3 root supergroup 378 2020-12-24 14:47 /user/hive/warehouse/A/000000_0_copy_1-rwxr-xr-x 3 root supergroup 378 2020-12-24 14:48 /user/hive/warehouse/A/000000_0_copy_2# 可以看到有三个小文件，然后使用 concatenate 进行合并，再次查看A表下文件数量hive (default)&gt; alter table A concatenate;hive (default)&gt; dfs -ls /user/hive/warehouse/A;Found 1 items-rwxr-xr-x 3 root supergroup 778 2020-12-24 14:59 /user/hive/warehouse/A/000000_0# 已合并成一个文件 调整参数减少 Map 数量 设置map输入合并小文件的相关参数： 12345678910111213# 执行Map前进行小文件合并# CombineHiveInputFormat底层是 Hadoop的 CombineFileInputFormat 方法# 此方法是在mapper中将多个文件合成一个split作为输入set hive.input.format=org.apache.hadoop.hive.sql.io.CombineHiveInputFormat; -- 默认# 每个Map最大输入大小(这个值决定了合并后文件的数量)set mapred.max.split.size=256000000; -- 256M# 一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)set mapred.min.split.size.per.node=100000000; -- 100M# 一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)set mapred.min.split.size.per.rack=100000000; -- 100M 设置map输出和reduce输出进行合并的相关参数： 1234567891011# 设置map端输出进行合并，默认为trueset hive.merge.mapfiles = true;# 设置reduce端输出进行合并，默认为falseset hive.merge.mapredfiles = true;# 设置合并文件的大小；set hive.merge.size.per.task = 256*1000*1000; -- 256M# 当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件mergeset hive.merge.smallfiles.avgsize=16000000; -- 16M 启用压缩 12345# hive的查询结果输出是否进行压缩set hive.exec.compress.output=true;# MapReduce Job的结果输出是否使用压缩set mapreduce.output.fileoutputformat.compress=true; 减少 Reduce 的数量 123456789101112131415161718# reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量，# hive中的分区函数 distribute by 正好是控制MR中partition分区的，# 然后通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可。# 设置reduce的数量有两种方式，第一种是直接设置reduce个数set mapreduce.job.reduces=10;# 第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数set hive.exec.reducers.bytes.per.reducer=5120000000; -- 默认是1G，设置为5G# 执行以下语句，将数据均衡的分配到reduce中set mapreduce.job.reduces=10;insert overwrite table A partition(dt)select * from Bdistribute by rand();解释：如设置 reduce 数量为10，则使用 rand()， 随机生成一个数 x % 10 ，这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小 使用 hadoop 的 archive 将小文件归档 Hadoop Archive 简称 HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问。 123456789101112#用来控制归档是否可用set hive.archive.enabled=true;#通知Hive在创建归档时是否可以设置父目录set hive.archive.har.parentdir.settable=true;#控制需要归档文件的大小set har.partfile.size=1099511627776;#使用以下命令进行归档ALTER TABLE A ARCHIVE PARTITION(dt='2020-12-24', hr='12');#对已归档的分区恢复为原文件ALTER TABLE A UNARCHIVE PARTITION(dt='2020-12-24', hr='12'); 注意：归档的分区可以查看不能 insert overwrite，必须先 unarchive。 数据存储及压缩针对 hive 中表的存储格式通常用 orc 和 parquet，压缩格式一般使用 snappy（Google）。 相比与 textfile 格式表，orc 占有更少的存储。 因为 hive 底层使用 MR 计算架构，数据流是 hdfs 到磁盘再到 hdfs，而且会有很多次，所以使用 orc 数据格式和 snappy 压缩策略可以降低 IO 读写，还能降低网络传输量，这样在一定程度上可以节省存储，还能提升 hql 任务执行效率。 调参优化 并行执行，调节 parallel 参数； 调节 jvm 参数，重用 jvm； 设置 map、 reduce 的参数； 开启 strict mode 模式； 关闭推测执行设置； 有效地减小数据集将大表拆分成子表； 结合使用外部表和分区表。 SQL 优化 大表对大表：尽量减少数据集，可以通过分区表、列剪裁，避免扫描全表或者全字段； 大表对小表：设置自动识别小表，将小表放入内存中去执行。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 安装]]></title>
    <url>%2F2022%2F08%2F04%2Fhive%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录再 Hadoop 集群上安装 Hive 的过程。 Hive 安装详细步骤 下载并解压 Hive 访问 Apache Hive 官方下载页面，选择与 Hadoop 版本兼容的 Hive 版本，然后下载对应的 tar 包。例如，如果 Hadoop 版本是 3.2.1，可以下载 apache-hive-3.1.2-bin.tar.gz 文件。在 Ubuntu 命令行中，可以使用 wget 命令下载压缩的 Hive 文件。然后使用 tar 命令解压 Hive 包。 配置 Hive 环境变量 编辑 .bashrc 文件，添加 $HIVE_HOME 和 $PATH 环境变量，指向解压后的 Hive 目录和 bin 子目录。保存并退出 .bashrc 文件，然后使用 source 命令应用更改。 编辑 hive-config.sh 文件 Hive 需要能够与 Hadoop 分布式文件系统交互。在 hive-config.sh 文件中，添加 $HADOOP_HOME 环境变量，指向 Hadoop 的安装目录。 在 HDFS 中创建 Hive 目录 Hive 需要在 HDFS 中创建一些目录来存储元数据和临时数据。使用 hadoop fs 命令在 HDFS 中创建 /tmp 和 /user/hive/warehouse 目录，并设置相应的权限。 配置 hive-site.xml 文件 Hive 需要一个配置文件来指定元数据存储的位置和类型，以及其他一些参数。在 Hive 的 conf 目录中，复制 hive-default.xml.template 文件并重命名为 hive-site.xml。编辑 hive-site.xml 文件，根据需求修改一些配置属性，例如 jdbc 连接字符串，驱动程序名称，用户名和密码等。 初始化 Derby 数据库 Hive 默认使用 Derby 数据库来存储元数据。需要初始化 Derby 数据库，并运行一些初始化脚本来创建元数据表和视图。在 Hive 的 bin 目录中，运行 schematool 命令，并指定 -initSchema 和 -dbType derby 选项。 Issue…]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 集群搭建]]></title>
    <url>%2F2022%2F08%2F03%2FHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了使用 VMware Workstation 创建 3 台 Linux 服务器搭建 Hadoop 集群的过程。 Hadoop 集群搭建环境与基本配置 虚拟软件：VMware Workstation 虚拟系统：CentoOS 7.5 java 版本：JDK 1.8 Hadoop 版本：Hadoop 3.1.3 为什么 Hadoop 推荐至少 3 个节点？保证集群的高可用性和容错性。 Hadoop 集群中由一个 NameNode 节点负责管理元数据，和多个 DataNode 节点负责存储数据。 如果只有一个 DataNode 节点，那么一旦它出现故障，就会导致数据丢失。 如果只有两个 DataNode 节点，那么一旦其中一个出现故障，就会导致集群无法正常工作，因为无法达成多数投票。 因此，Hadoop 集群至少需要 3 个节点，才能保证在一个节点出现故障的情况下，集群仍然能够正常运行。 集群部署规划 NameNode 和 SecondaryNameNode 不安装在同一台服务器上 ResourceManager 很消耗内存，不和 NameNode、SecondaryNameNode 配置在同一台机器上 安装配置初步规划 创建 3 台虚拟机 先安装配置好 1 台，再分发、同步到其他虚拟机 集群启动测试 1. 虚拟机创建 注：本文使用 VMWare Workstation 虚拟机。 创建一台模板虚拟机 虚拟机设置 设备参数： 系统：CentOS 7.5 内存：设定后不可更改。视物理机内存使用情况和与其数据大小而定，越大越好，但要考虑 3 台的量物理机是否带的动。 硬盘：设定后可拓展。参考 = 系统大小 + 软件大小 + 预期数据大小 + 预留空间大小。 网络适配器：NAT/Bridge 都行，保证集群之间能互相联通即可。 虚拟 CentOS 设置 创建 hadoop 用户，并配置 root 权限，方便 sudo 执行命令 卸载自带 JDK 虚拟机克隆 两种方案： 将模板虚拟机克隆 3 份，克隆的 3 份组成集群，模板机作为备份，方便出错重做 将模板虚拟机克隆 2 份，模板机与克隆的 2 份组成集群 配置集群的静态 IP 地址和主机名 修改 3 台虚拟机的 hostname 和 hosts 文件，确认能够互相 ping 通。 2. 搭建 Hadoop 集群 单机安装 Hadoop 选择一台虚拟主机作为 master 节点，其他作为 worker 节点。先在 master 作以下操作： 下载安装 JDK，并配置环境变量 下载并解压 Hadoop 压缩包，并配置环境变量 单机测试能否运行 集群配置 在 master 节点上配置 ssh 免密登录，使得 master 节点可以无密码访问其他 worker 节点 master 生成公钥和私钥：ssh-keygen -t rsa 将公钥 id_rsa.pub 拷贝到要免密登录的节点上：ssh-copy-id &lt;目标免密登录节点&gt; 将 master 上安装好的 JDK 和 Hadoop 用 scp 拷贝到其他 worker 的相同目录下，保持各节点目录结构一致 修改集群各机器的配置文件 修改 master 上的配置文件 core-site.xml 指定 NameNode 的地址（8020） 指定 hdfs 网页登录使用的静态用户 hdfs-site.xml 指定 Namenode web 端访问地址（9870） 指定 Secondary Namenode web 端访问地址（9868） yarn-site.xml 指定 ResourceManager web 端的地址 mapred-site.xml 指定 MapReduce 程序运行在 Yarn 上 指定历史服务器 historyserver 的 web 端地址（19888） 使用 rsync 命令将以上配置文件同步到集群的其他 worker 配置 workers 在 master 的 {HADOOP_HOME}/etc/hadoop/workers 文件中添加集群所有节点（包括 master 与 workers）的主机名（文件中不允许空行，只许换行） 将修改后的文件同步至所有节点 集群启停启动集群 注：第一次启动需要在 master 节点格式化 NameNode：hdfs namenode -format 在 master 启动 HDFS：sbin/start-dfs.sh 测试：浏览器打开 http://&lt;master 主机名&gt;:9870 可以查看 hdfs 文件即成功。 在配置 ResourceManager 的节点启动 YARN：sbin/start-yarn.sh 测试：浏览器打开 http://&lt;ResourceManager 节点主机名&gt;:8088 可以查看 hdfs 文件即成功。 在 master 启动 historyserver：mapred --daemon start historyserver 测试：浏览器打开 http://&lt;master 主机名&gt;:19888/jobhistory 可以查看历史任务日志即成功。 通过 jps 命令可以查看各节点的进程是否正常运行 关闭集群关闭 HDFS在 master 进行： 12345# 整体关闭stop-dfs.sh# 指定组件关闭hdfs --daemon stop &#123;namenode | datanode | secondarynamenode&#125; 关闭 YARN在配置了 ResourceManager 的节点进行12345# 整体关闭stop-yarn.sh# 指定组件关闭yarn --daemon stop &#123;resourcemanager | nodemanager&#125;]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文选诗词撷取]]></title>
    <url>%2F2022%2F05%2F04%2F%E6%96%87%E9%80%89%E8%AF%97%E8%AF%8D%E6%92%B7%E5%8F%96%2F</url>
    <content type="text"><![CDATA[[Updated] 诗词是中华文化的瑰宝。 尽心·上孟子 尽其心者，知其性也。知其性，则知天矣。存其心，养其性，所以事天也。夭寿不贰，修身以俟之，所以立命也。 蒹葭作者：无名氏 蒹葭苍苍，白露为霜。所谓伊人，在水一方。溯洄从之，道阻且长。溯游从之，宛在水中央。蒹葭萋萋，白露未晞。所谓伊人，在水之湄。溯洄从之，道阻且跻。溯游从之，宛在水中坻。蒹葭采采，白露未已。所谓伊人，在水之涘。溯洄从之，道阻且右。溯游从之，宛在水中沚。 观沧海作者：曹操 东临碣石，以观沧海。水何澹澹，山岛竦峙。树木丛生，百草丰茂。秋风萧瑟，洪波涌起。日月之行，若出其中；星汉灿烂，若出其里。幸甚至哉，歌以咏志。 龟虽寿作者：曹操 神龟虽寿，犹有竟时；腾蛇乘雾，终为土灰。老骥伏枥，志在千里；烈士暮年，壮心不已。盈缩之期，不但在天；养怡之福，可得永年。幸甚至哉，歌以咏志。 短歌行作者：曹操 对酒当歌，人生几何！譬如朝露，去日苦多。慨当以慷，忧思难忘。何以解忧？唯有杜康。青青子衿，悠悠我心。但为君故，沉吟至今。呦呦鹿鸣，食野之苹。我有嘉宾，鼓瑟吹笙。明明如月，何时可掇？忧从中来，不可断绝。越陌度阡，枉用相存。契阔谈讌，心念旧恩。月明星稀，乌鹊南飞。绕树三匝，何枝可依？山不厌高，海不厌深。周公吐哺，天下归心。 归园田居·其三作者：陶渊明 种豆南山下，草盛豆苗稀。晨兴理荒秽，带月荷锄归。道狭草木长，夕露沾我衣。衣沾不足惜，但使愿无违。 归园田居·其一作者：陶渊明 少无适俗韵，性本爱丘山。误落尘网中，一去三十年。羁鸟恋旧林，池鱼思故渊。开荒南野际，守拙归园田。方宅十余亩，草屋八九间。榆柳荫后檐，桃李罗堂前。暧暧远人村，依依墟里烟。狗吠深巷中，鸡鸣桑树颠。户庭无尘杂，虚室有余闲。久在樊笼里，复得返自然。 饮酒·其五作者：陶渊明 结庐在人境，而无车马喧。问君何能尔？心远地自偏。采菊东篱下，悠然见南山。山气日夕佳，飞鸟相与还。此中有真意，欲辨已忘言。 归去来兮辞·并序作者：陶渊明 余家贫，耕植不足以自给。幼稚盈室，瓶无储粟，生生所资，未见其术。亲故多劝余为长吏，脱然有怀，求之靡途。会有四方之事，诸侯以惠爱为德，家叔以余贫苦，遂见用于小邑。于时风波未静，心惮远役，彭泽去家百里，公田之利，足以为酒。故便求之。及少日，眷然有归欤之情。何则？质性自然，非矫厉所得。饥冻虽切，违己交病。尝从人事，皆口腹自役。于是怅然慷慨，深愧平生之志。犹望一稔，当敛裳宵逝。寻程氏妹丧于武昌，情在骏奔，自免去职。仲秋至冬，在官八十余日。因事顺心，命篇曰《归去来兮》。乙巳岁十一月也。归去来兮，田园将芜胡不归？既自以心为形役，奚惆怅而独悲？悟已往之不谏，知来者之可追。实迷途其未远，觉今是而昨非。舟遥遥以轻飏，风飘飘而吹衣。问征夫以前路，恨晨光之熹微。乃瞻衡宇，载欣载奔。僮仆欢迎，稚子候门。三径就荒，松菊犹存。携幼入室，有酒盈樽。引壶觞以自酌，眄庭柯以怡颜。倚南窗以寄傲，审容膝之易安。园日涉以成趣，门虽设而常关。策扶老以流憩，时矫首而遐观。云无心以出岫，鸟倦飞而知还。景翳翳以将入，抚孤松而盘桓。归去来兮，请息交以绝游。世与我而相违，复驾言兮焉求？悦亲戚之情话，乐琴书以消忧。农人告余以春及，将有事于西畴。或命巾车，或棹孤舟。既窈窕以寻壑，亦崎岖而经丘。木欣欣以向荣，泉涓涓而始流。善万物之得时，感吾生之行休。已矣乎！寓形宇内复几时？曷不委心任去留？胡为乎遑遑欲何之？富贵非吾愿，帝乡不可期。怀良辰以孤往，或植杖而耘耔。登东皋以舒啸，临清流而赋诗。聊乘化以归尽，乐夫天命复奚疑！ 桃花源记作者：陶渊明 晋太元中，武陵人捕鱼为业。缘溪行，忘路之远近。忽逢桃花林，夹岸数百步，中无杂树，芳草鲜美，落英缤纷。渔人甚异之，复前行，欲穷其林。林尽水源，便得一山，山有小口，仿佛若有光。便舍船，从口入。初极狭，才通人。复行数十步，豁然开朗。土地平旷，屋舍俨然，有良田、美池、桑竹之属。阡陌交通，鸡犬相闻。其中往来种作，男女衣着，悉如外人。黄发垂髫，并怡然自乐。见渔人，乃大惊，问所从来。具答之。便要还家，设酒杀鸡作食。村中闻有此人，咸来问讯。自云先世避秦时乱，率妻子邑人来此绝境，不复出焉，遂与外人间隔。问今是何世，乃不知有汉，无论魏晋。此人一一为具言所闻，皆叹惋。余人各复延至其家，皆出酒食。停数日，辞去。此中人语云：“不足为外人道也。”既出，得其船，便扶向路，处处志之。及郡下，诣太守，说如此。太守即遣人随其往，寻向所志，遂迷，不复得路。南阳刘子骥，高尚士也，闻之，欣然规往。未果，寻病终，后遂无问津者。 与宋思元书作者：吴均 风烟俱净，天山共色，从流飘荡，任意东西。自富阳至桐庐一百许里，奇山异水，天下独绝。水皆缥碧，千丈见底;游鱼细石，直视无碍。急湍甚箭，猛浪若奔。夹岸高山，皆生寒树。负势竞上，互相轩邈，争高直指，千百成峰。泉水激石，泠泠作响;好鸟相鸣，嘤嘤成韵。蝉则千转不穷，猿则百叫无绝。鸢飞戾天者，望峰息心;经纶^②^世务者，窥谷忘反。横柯上蔽，在昼犹昏;疏条交映，有时见日。 三峡作者：郦道元 自三峡七百里中，两岸连山，略无阙处。重岩叠嶂，隐天蔽日，自非亭午夜分，不见曦月。至于夏水襄陵，沿溯阻绝。或王命急宣，有时朝发白帝，暮到江陵，其间千二百里，虽乘奔御风，不以疾也。春冬之时，则素湍绿潭，回清倒影，绝𪩘多生怪柏，悬泉瀑布，飞漱其间，清荣峻茂，良多趣味。每至晴初霜旦，林寒涧肃，常有高猿长啸，属引凄异，空谷传响，哀转久绝。故渔者歌曰：“巴东三峡巫峡长，猿鸣三声泪沾裳。” 登池上楼作者：谢灵运 潜虬媚幽姿，飞鸿响远音。薄霄愧云浮，栖川怍渊沉。进德智所拙，退耕力不任。徇禄反穷海，卧疴对空林。衾枕昧节候，褰开暂窥临。倾耳聆波澜，举目眺岖嵚。初景革绪风，新阳改故阴。池塘生春草，园柳变鸣禽。祁祁伤豳歌，萋萋感楚吟。索居易永久，离群难处心。持操岂独古，无闷征在今。 敕勒歌作者：北朝民歌 敕勒川，阴山下。天似穹庐，笼盖四野。天苍苍，野茫茫。风吹草低见牛羊。 春江花月夜作者：张若虚 春江潮水连海平，海上明月共潮生。滟滟随波千万里，何处春江无月明！江流宛转绕芳甸，月照花林皆似霰；空里流霜不觉飞，汀上白沙看不见。江天一色无纤尘，皎皎空中孤月轮。江畔何人初见月？江月何年初照人？人生代代无穷已，江月年年望相似。不知江月待何人，但见长江送流水。白云一片去悠悠，青枫浦上不胜愁。谁家今夜扁舟子？何处相思明月楼？可怜楼上月裴回，应照离人妆镜台。玉户帘中卷不去，捣衣砧上拂还来。此时相望不相闻，愿逐月华流照君。鸿雁长飞光不度，鱼龙潜跃水成文。昨夜闲潭梦落花，可怜春半不还家。江水流春去欲尽，江潭落月复西斜。斜月沉沉藏海雾，碣石潇湘无限路。不知乘月几人归，落月摇情满江树。 过故人庄作者：孟浩然 故人具鸡黍，邀我至田家。绿树村边合，青山郭外斜。开轩面场圃，把酒话桑麻。待到重阳日，还来就菊花。 宿建德江作者：孟浩然 移舟泊烟渚，日暮客愁新。野旷天低树，江清月近人。 望洞庭湖赠张丞相作者：孟浩然 八月湖水平，涵虚混太清。气蒸云梦泽，波撼岳阳城。欲济无舟楫，端居耻圣明。坐观垂钓者，徒有羡鱼情。 钓鱼湾作者：储光羲 垂钓绿湾春，春深杏花乱。潭清疑水浅，荷动知鱼散。日暮待情人，维舟绿杨岸。 鹿柴作者：王维 空山不见人，但闻人语响。返景入深林，复照青苔上。 山居秋暝作者：王维 空山新雨后，天气晚来秋。明月松间照，清泉石上流。竹喧归浣女，莲动下渔舟。随意春芳歇，王孙自可留。 鸟鸣涧作者：王维 人闲桂花落，夜静春山空。月出惊山鸟，时鸣春涧中。 竹里馆作者：王维 独坐幽篁里，弹琴复长啸。深林人不知，明月来相照。 辛夷坞作者：王维 木末芙蓉花，山中发红萼。涧户寂无人，纷纷开且落。 终南山王维 〔唐代〕 太乙近天都，连山接海隅。白云回望合，青霭入看无。分野中峰变，阴晴众壑殊。欲投人处宿，隔水问樵夫。 凉州词二首·其一作者：王之涣 黄河远上白云间，一片孤城万仞山。羌笛何须怨杨柳，春风不度玉门关。 将进酒作者：李白 君不见黄河之水天上来，奔流到海不复回。君不见高堂明镜悲白发，朝如青丝暮成雪。人生得意须尽欢，莫使金樽空对月。天生我材必有用，千金散尽还复来。烹羊宰牛且为乐，会须一饮三百杯。岑夫子，丹丘生，将进酒，杯莫停。与君歌一曲，请君为我倾耳听。钟鼓馔玉不足贵，但愿长醉不愿醒。古来圣贤皆寂寞，惟有饮者留其名。陈王昔时宴平乐，斗酒十千恣欢谑。主人何为言少钱，径须沽取对君酌。五花马、千金裘，呼儿将出换美酒，与尔同销万古愁。 行路难作者：李白 金樽清酒斗十千，玉盘珍馐直万钱。停杯投箸不能食，拔剑四顾心茫然。欲渡黄河冰塞川，将登太行雪满山。闲来垂钓坐溪上，忽复乘舟梦日边。行路难，行路难，多歧路，今安在。长风破浪会有时，直挂云帆济沧海。 望天门山作者：李白 天门中断楚江开，碧水东流至此回。两岸青山相对出，孤帆一片日边来。 黄鹤楼送孟浩然之广陵作者：李白 故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。 望岳作者：杜甫 岱宗夫如何？齐鲁青未了。造化钟神秀，阴阳割昏晓。荡胸生曾云，决眦入归鸟。会当凌绝顶，一览众山小。 滁州西涧作者：韦应物 独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。 渔歌子作者：张志和 西塞山前白鹭飞，桃花流水鳜鱼肥。青箬笠，绿蓑衣，斜风细雨不须归。 江雪作者：柳宗元 千山鸟飞绝，万径人踪灭。孤舟蓑笠翁，独钓寒江雪。 小石潭记作者：柳宗元 从小丘西行百二十步，隔篁竹，闻水声，如鸣珮环，心乐之。伐竹取道，下见小潭，水尤清冽。全石以为底，近岸，卷石底以出，为坻，为屿，为嵁，为岩。青树翠蔓，蒙络摇缀，参差披拂。潭中鱼可百许头，皆若空游无所依，日光下澈，影布石上。佁然不动，俶尔远逝，往来翕忽，似与游者相乐。潭西南而望，斗折蛇行，明灭可见。其岸势犬牙差互，不可知其源。坐潭上，四面竹树环合，寂寥无人，凄神寒骨，悄怆幽邃。以其境过清，不可久居，乃记之而去。同游者：吴武陵，龚古，余弟宗玄。隶而从者，崔氏二小生：曰恕己，曰奉壹。 赋得古原草送别作者：白居易 离离原上草，一岁一枯荣。野火烧不尽，春风吹又生。远芳侵古道，晴翠接荒城。又送王孙去，萋萋满别情。 长恨歌作者：白居易 汉皇重色思倾国，御宇多年求不得。杨家有女初长成，养在深闺人未识。天生丽质难自弃，一朝选在君王侧。回眸一笑百媚生，六宫粉黛无颜色。春寒赐浴华清池，温泉水滑洗凝脂。侍儿扶起娇无力，始是新承恩泽时。云鬓花颜金步摇，芙蓉帐暖度春宵。春宵苦短日高起，从此君王不早朝。承欢侍宴无闲暇，春从春游夜专夜。后宫佳丽三千人，三千宠爱在一身。金屋妆成娇侍夜，玉楼宴罢醉和春。姊妹弟兄皆列土，可怜光彩生门户。遂令天下父母心，不重生男重生女。骊宫高处入青云，仙乐风飘处处闻。缓歌慢舞凝丝竹，尽日君王看不足。渔阳鼙鼓动地来，惊破霓裳羽衣曲。九重城阙烟尘生，千乘万骑西南行。翠华摇摇行复止，西出都门百余里。六军不发无奈何，宛转蛾眉马前死。花钿委地无人收，翠翘金雀玉搔头。君王掩面救不得，回看血泪相和流。黄埃散漫风萧索，云栈萦纡登剑阁。峨嵋山下少人行，旌旗无光日色薄。蜀江水碧蜀山青，圣主朝朝暮暮情。行宫见月伤心色，夜雨闻铃肠断声。天旋日转回龙驭，到此踌躇不能去。马嵬坡下泥土中，不见玉颜空死处。君臣相顾尽沾衣，东望都门信马归。归来池苑皆依旧，太液芙蓉未央柳。芙蓉如面柳如眉，对此如何不泪垂。春风桃李花开夜，秋雨梧桐叶落时。西宫南苑多秋草，落叶满阶红不扫。梨园弟子白发新，椒房阿监青娥老。夕殿萤飞思悄然，孤灯挑尽未成眠。迟迟钟鼓初长夜，耿耿星河欲曙天。鸳鸯瓦冷霜华重，翡翠衾寒谁与共。悠悠生死别经年，魂魄不曾来入梦。临邛道士鸿都客，能以精诚致魂魄。为感君王辗转思，遂教方士殷勤觅。排空驭气奔如电，升天入地求之遍。上穷碧落下黄泉，两处茫茫皆不见。忽闻海上有仙山，山在虚无缥渺间。楼阁玲珑五云起，其中绰约多仙子。中有一人字太真，雪肤花貌参差是。金阙西厢叩玉扃，转教小玉报双成。闻道汉家天子使，九华帐里梦魂惊。揽衣推枕起徘徊，珠箔银屏迤逦开。云鬓半偏新睡觉，花冠不整下堂来。风吹仙袂飘飖举，犹似霓裳羽衣舞。玉容寂寞泪阑干，梨花一枝春带雨。含情凝睇谢君王，一别音容两渺茫。昭阳殿里恩爱绝，蓬莱宫中日月长。回头下望人寰处，不见长安见尘雾。惟将旧物表深情，钿合金钗寄将去。钗留一股合一扇，钗擘黄金合分钿。但令心似金钿坚，天上人间会相见。临别殷勤重寄词，词中有誓两心知。七月七日长生殿，夜半无人私语时。在天愿作比翼鸟，在地愿为连理枝。天长地久有时尽，此恨绵绵无绝期。 琵琶行作者：白居易 浔阳江头夜送客，枫叶荻花秋瑟瑟。主人下马客在船，举酒欲饮无管弦。醉不成欢惨将别，别时茫茫江浸月。忽闻水上琵琶声，主人忘归客不发。寻声暗问弹者谁，琵琶声停欲语迟。移船相近邀相见，添酒回灯重开宴。千呼万唤始出来，犹抱琵琶半遮面。转轴拨弦三两声，未成曲调先有情。弦弦掩抑声声思，似诉平生不得志。低眉信手续续弹，说尽心中无限事。轻拢慢捻抹复挑，初为《霓裳》后《六幺》。大弦嘈嘈如急雨，小弦切切如私语。嘈嘈切切错杂弹，大珠小珠落玉盘。间关莺语花底滑，幽咽泉流冰下难。冰泉冷涩弦凝绝，凝绝不通声暂歇。别有幽愁暗恨生，此时无声胜有声。银瓶乍破水浆迸，铁骑突出刀枪鸣。曲终收拨当心画，四弦一声如裂帛。东船西舫悄无言，唯见江心秋月白。沉吟放拨插弦中，整顿衣裳起敛容。自言本是京城女，家在虾蟆陵下住。十三学得琵琶成，名属教坊第一部。曲罢曾教善才服，妆成每被秋娘妒。五陵年少争缠头，一曲红绡不知数。钿头银篦击节碎，血色罗裙翻酒污。今年欢笑复明年，秋月春风等闲度。弟走从军阿姨死，暮去朝来颜色故。门前冷落鞍马稀，老大嫁作商人妇。商人重利轻别离，前月浮梁买茶去。去来江口守空船，绕船月明江水寒。夜深忽梦少年事，梦啼妆泪红阑干。我闻琵琶已叹息，又闻此语重唧唧。同是天涯沦落人，相逢何必曾相识！我从去年辞帝京，谪居卧病浔阳城。浔阳地僻无音乐，终岁不闻丝竹声。住近湓江地低湿，黄芦苦竹绕宅生。其间旦暮闻何物？杜鹃啼血猿哀鸣。春江花朝秋月夜，往往取酒还独倾。岂无山歌与村笛？呕哑嘲哳难为听。今夜闻君琵琶语，如听仙乐耳暂明。莫辞更坐弹一曲，为君翻作《琵琶行》。感我此言良久立，却坐促弦弦转急。凄凄不似向前声，满座重闻皆掩泣。座中泣下谁最多？江州司马青衫湿。 钱塘湖春行作者：白居易 孤山寺北贾亭西，水面初平云脚低。几处早莺争暖树，谁家新燕啄春泥。乱花渐欲迷人眼，浅草才能没马蹄。最爱湖东行不足，绿杨阴里白沙堤。 无题作者：李商隐 相见时难别亦难，东风无力百花残。春蚕到死丝方尽，蜡炬成灰泪始干。晓镜但愁云鬓改，夜吟应觉月光寒。蓬山此去无多路，青鸟殷勤为探看。 锦瑟作者：李商隐 锦瑟无端五十弦，一弦一柱思华年。庄生晓梦迷蝴蝶，望帝春心托杜鹃。沧海月明珠有泪，蓝田日暖玉生烟。此情可待成追忆？只是当时已惘然。 菩萨蛮·小山重叠金明灭作者：温庭筠 小山重叠金明灭，鬓云欲度香腮雪。懒起画蛾眉，弄妆梳洗迟。照花前后镜，花面交相映。新帖绣罗襦，双双金鹧鸪。 菩萨蛮·人人尽说江南好作者：韦庄 人人尽说江南好，游人只合江南老。春水碧于天，画船听雨眠。垆边人似月，皓腕凝霜雪。未老莫还乡，还乡须断肠。 谒金门·风乍起作者：冯延巳 风乍起,吹皱一池春水。 闲引鸳鸯香径里,手挼红杏蕊。 斗鸭阑干独倚,碧玉搔头斜坠。 终日望君君不至,举头闻鹊喜。 摊破浣溪沙作者：李璟 菡萏香销翠叶残，西风愁起绿波间。还与韶光共憔悴，不堪看。细雨梦回鸡塞远，小楼吹彻玉笙寒。多少泪珠何限恨，倚阑干。 乌夜啼作者：李煜 林花谢了春红，太匆匆，无奈朝来寒雨晚来风。 胭脂泪，相留醉，几时重，自是人生长恨水长东。 又作者：李煜 无言独上西楼，月如钩。寂寞梧桐深院锁清秋。剪不断，理还乱，是离愁。别有一番滋味在心头。 虞美人作者：李煜 春花秋月何时了？往事知多少。小楼昨夜又东风，故国不堪回首月明中。雕阑玉砌应犹在，只是朱颜改。问君能有几多愁？恰似一江春水向东流。 村行作者：王禹偁 马穿山径菊初黄，信马悠悠野兴长。万壑有声含晚籁，数峰无语立斜阳。棠梨叶落胭脂色，荞麦花开白雪香。何事吟余忽惆怅，村桥原树似吾乡。 天仙子作者：张先 时为嘉禾小倅，以病眠，不赴府会。 水调数声持酒听，午醉醒来愁未醒。送春春去几时回？临晚镜，伤流景，往事后期空记省。沙上并禽池上暝，云破月来花弄影。重重帘幕密遮灯，风不定，人初静，明日落红应满径。 浣溪沙·一曲新词酒一杯晏殊 〔宋代〕 一曲新词酒一杯，去年天气旧亭台。夕阳西下几时回？无可奈何花落去，似曾相识燕归来。小园香径独徘徊。 初晴游沧浪亭苏舜钦 夜雨连明春水生，娇云浓暖弄阴晴。帘虚日薄花竹静，时有乳鸠相对鸣。 望海潮·东南形胜柳永 〔宋代〕 东南形胜，三吴都会，钱塘自古繁华。烟柳画桥，风帘翠幕，参差十万人家。云树绕堤沙，怒涛卷霜雪，天堑无涯。市列珠玑，户盈罗绮，竞豪奢。(三吴 一作：江吴)重湖叠巘清嘉，有三秋桂子，十里荷花。羌管弄晴，菱歌泛夜，嬉嬉钓叟莲娃。千骑拥高牙，乘醉听箫鼓，吟赏烟霞。异日图将好景，归去凤池夸。 雨霖铃·秋别柳永 〔宋代〕 寒蝉凄切，对长亭晚，骤雨初歇。都门帐饮无绪，留恋处，兰舟催发。执手相看泪眼，竟无语凝噎。念去去，千里烟波，暮霭沉沉楚天阔。多情自古伤离别，更那堪，冷落清秋节！今宵酒醒何处？杨柳岸，晓风残月。此去经年，应是良辰好景虚设。便纵有千种风情，更与何人说？(好景 一作：美景) 临江仙·梦后楼台高锁晏几道 〔宋代〕 梦后楼台高锁，酒醒帘幕低垂。去年春恨却来时。落花人独立，微雨燕双飞。记得小蘋初见，两重心字罗衣。琵琶弦上说相思。当时明月在，曾照彩云归。(蘋 通：苹) 卜算子·黄州定慧院寓居作苏轼 〔宋代〕 缺月挂疏桐，漏断人初静。谁见幽人独往来，缥缈孤鸿影。(谁见 一作：时见)惊起却回头，有恨无人省。拣尽寒枝不肯栖，寂寞沙洲冷。 定风波·莫听穿林打叶声苏轼 〔宋代〕 三月七日，沙湖道中遇雨，雨具先去，同行皆狼狈，余独不觉。已而遂晴，故作此(词)。 莫听穿林打叶声，何妨吟啸且徐行。竹杖芒鞋轻胜马，谁怕？一蓑烟雨任平生。料峭春风吹酒醒，微冷，山头斜照却相迎。回首向来萧瑟处，归去，也无风雨也无晴。 念奴娇·赤壁怀古苏轼 〔宋代〕 大江东去，浪淘尽，千古风流人物。故垒西边，人道是，三国周郎赤壁。乱石穿空，惊涛拍岸，卷起千堆雪。(穿空 一作：崩云)江山如画，一时多少豪杰。 遥想公瑾当年，小乔初嫁了，雄姿英发。羽扇纶巾，谈笑间，樯橹灰飞烟灭。(樯橹 一作：强虏)故国神游，多情应笑我，早生华发。人生如梦，一尊还酹江月。(人生 一作：人间；尊 同：樽) 浣溪沙·游蕲水清泉寺苏轼 〔宋代〕 游蕲水清泉寺，寺临兰溪，溪水西流。 山下兰芽短浸溪，松间沙路净无泥，萧萧暮雨子规啼。(萧萧 一作：潇潇)谁道人生无再少？门前流水尚能西！休将白发唱黄鸡。 江城子·密州出猎作者：苏轼 老夫聊发少年狂，左牵黄，右擎苍，锦帽貂裘，千骑卷平冈。为报倾城随太守，亲射虎，看孙郎。酒酣胸胆尚开张，鬓微霜，又何妨！持节云中，何日遣冯唐？会挽雕弓如满月，西北望，射天狼。 浣溪沙·细雨斜风作晓寒苏轼 〔宋代〕 元丰七年十二月二十四日，从泗州刘倩叔游南山 细雨斜风作晓寒，淡烟疏柳媚晴滩。入淮清洛渐漫漫。雪沫乳花浮午盏，蓼茸蒿笋试春盘。人间有味是清欢。 前赤壁赋苏轼 〔宋代〕 壬戌之秋，七月既望，苏子与客泛舟游于赤壁之下。清风徐来，水波不兴。举酒属客，诵明月之诗，歌窈窕之章。少焉，月出于东山之上，徘徊于斗牛之间。白露横江，水光接天。纵一苇之所如，凌万顷之茫然。浩浩乎如冯虚御风，而不知其所止；飘飘乎如遗世独立，羽化而登仙。(冯 通：凭) 于是饮酒乐甚，扣舷而歌之。歌曰：“桂棹兮兰桨，击空明兮溯流光。渺渺兮予怀，望美人兮天一方。”客有吹洞箫者，倚歌而和之。其声呜呜然，如怨如慕，如泣如诉；余音袅袅，不绝如缕。舞幽壑之潜蛟，泣孤舟之嫠妇。 苏子愀然，正襟危坐，而问客曰：“何为其然也？”客曰：“‘月明星稀，乌鹊南飞。’此非曹孟德之诗乎？西望夏口，东望武昌，山川相缪，郁乎苍苍，此非孟德之困于周郎者乎？方其破荆州，下江陵，顺流而东也，舳舻千里，旌旗蔽空，酾酒临江，横槊赋诗，固一世之雄也，而今安在哉？况吾与子渔樵于江渚之上，侣鱼虾而友麋鹿，驾一叶之扁舟，举匏樽以相属。寄蜉蝣于天地，渺沧海之一粟。哀吾生之须臾，羡长江之无穷。挟飞仙以遨游，抱明月而长终。知不可乎骤得，托遗响于悲风。” 苏子曰：“客亦知夫水与月乎？逝者如斯，而未尝往也；盈虚者如彼，而卒莫消长也。盖将自其变者而观之，则天地曾不能以一瞬；自其不变者而观之，则物与我皆无尽也，而又何羡乎！且夫天地之间，物各有主，苟非吾之所有，虽一毫而莫取。惟江上之清风，与山间之明月，耳得之而为声，目遇之而成色，取之无禁，用之不竭。是造物者之无尽藏也，而吾与子之所共适。”(共适 一作：共食) 客喜而笑，洗盏更酌。肴核既尽，杯盘狼籍。相与枕藉乎舟中，不知东方之既白。 登快阁黄庭坚 〔宋代〕 痴儿了却公家事，快阁东西倚晚晴。落木千山天远大，澄江一道月分明。朱弦已为佳人绝，青眼聊因美酒横。万里归船弄长笛，此心吾与白鸥盟。 踏莎行·郴州旅舍踏莎行 或指《踏莎行·郴州旅舍》 秦观 〔宋代〕 雾失楼台，月迷津渡。桃源望断无寻处。可堪孤馆闭春寒，杜鹃声里斜阳暮。驿寄梅花，鱼传尺素。砌成此恨无重数。郴江幸自绕郴山，为谁流下潇湘去。 浣溪沙·漠漠轻寒上小楼秦观 〔宋代〕 漠漠轻寒上小楼，晓阴无赖似穷秋。淡烟流水画屏幽。自在飞花轻似梦，无边丝雨细如愁。宝帘闲挂小银钩。 鹊桥仙·纤云弄巧秦观 〔宋代〕 纤云弄巧，飞星传恨，银汉迢迢暗度。金风玉露一相逢，便胜却人间无数。柔情似水，佳期如梦，忍顾鹊桥归路！两情若是久长时，又岂在朝朝暮暮。 苏幕遮·怀旧范仲淹 〔宋代〕 碧云天，黄叶地，秋色连波，波上寒烟翠。山映斜阳天接水，芳草无情，更在斜阳外。黯乡魂，追旅思，夜夜除非，好梦留人睡。明月楼高休独倚，酒入愁肠，化作相思泪。(留人睡 一作：留人醉) 渔家傲·秋思作者：范仲淹 塞下秋来风景异，衡阳雁去无留意。四面边声连角起，千嶂里，长烟落日孤城闭。浊酒一杯家万里，燕然未勒归无计。羌管悠悠霜满地，人不寐，将军白发征夫泪。 岳阳楼记作者：范仲淹 庆历四年春，滕子京谪守巴陵郡。越明年，政通人和，百废具兴，乃重修岳阳楼，增其旧制，刻唐贤今人诗赋于其上，属予作文以记之。予观夫巴陵胜状，在洞庭一湖。衔远山，吞长江，浩浩汤汤，横无际涯，朝晖夕阴，气象万千，此则岳阳楼之大观也，前人之述备矣。然则北通巫峡，南极潇湘，迁客骚人，多会于此，览物之情，得无异乎？若夫淫雨霏霏，连月不开，阴风怒号，浊浪排空，日星隐曜，山岳潜形，商旅不行，樯倾楫摧，薄暮冥冥，虎啸猿啼。登斯楼也，则有去国怀乡，忧谗畏讥，满目萧然，感极而悲者矣。至若春和景明，波澜不惊，上下天光，一碧万顷，沙鸥翔集，锦鳞游泳，岸芷汀兰，郁郁青青。而或长烟一空，皓月千里，浮光跃金，静影沉璧，渔歌互答，此乐何极！登斯楼也，则有心旷神怡，宠辱偕忘，把酒临风，其喜洋洋者矣。嗟夫！予尝求古仁人之心，或异二者之为，何哉？不以物喜，不以己悲，居庙堂之高则忧其民，处江湖之远则忧其君。是进亦忧，退亦忧。然则何时而乐耶？其必曰“先天下之忧而忧，后天下之乐而乐”乎！噫！微斯人，吾谁与归？时六年九月十五日。 兰陵王·柳周邦彦 〔宋代〕 柳阴直，烟里丝丝弄碧。隋堤上、曾见几番，拂水飘绵送行色。登临望故国，谁识、京华倦客？长亭路，年去岁来，应折柔条过千尺。闲寻旧踪迹，又酒趁哀弦，灯照离席。梨花榆火催寒食。愁一箭风快，半篙波暖，回头迢递便数驿，望人在天北。凄恻，恨堆积！渐别浦萦回，津堠岑寂，斜阳冉冉春无极。念月榭携手，露桥闻笛。沉思前事，似梦里，泪暗滴。 苏幕遮·燎沉香周邦彦 〔宋代〕 燎沉香，消溽暑。鸟雀呼晴，侵晓窥檐语。叶上初阳干宿雨，水面清圆，一一风荷举。故乡遥，何日去？家住吴门，久作长安旅。五月渔郎相忆否？小楫轻舟，梦入芙蓉浦。 醉花阴·薄雾浓云愁永昼李清照 〔宋代〕 薄雾浓云愁永昼，瑞脑销金兽。佳节又重阳，玉枕纱厨，半夜凉初透。(厨 通：橱；销金兽 一作：消金兽)东篱把酒黄昏后，有暗香盈袖。莫道不销魂，帘卷西风，人比黄花瘦。(人比 一作：人似) 念奴娇·过洞庭张孝祥 〔宋代〕 洞庭青草，近中秋，更无一点风色。玉鉴琼田三万顷，着我扁舟一叶。素月分辉，明河共影，表里俱澄澈。悠然心会，妙处难与君说。(着 同：著；玉鉴 一作：玉界)应念岭海经年，孤光自照，肝肺皆冰雪。短发萧骚襟袖冷，稳泛沧浪空阔。尽挹西江，细斟北斗，万象为宾客。扣舷独啸，不知今夕何夕！(肝肺 一作：肝胆；沧浪 一作：沧冥；尽挹 一作：尽吸；岭海 一作：岭表) 初入淮河四绝句·其三杨万里 〔宋代〕 两岸舟船各背驰，波痕交涉亦难为。只余鸥鹭无拘管，北去南来自在飞。 闲居初夏午睡起·其一杨万里 〔宋代〕 梅子留酸软齿牙，芭蕉分绿与窗纱。日长睡起无情思，闲看儿童捉柳花。 卜算子·咏梅陆游 〔宋代〕 驿外断桥边，寂寞开无主。已是黄昏独自愁，更着风和雨。(着 同：著)无意苦争春，一任群芳妒。零落成泥碾作尘，只有香如故。 青玉案·凌波不过横塘路贺铸 〔宋代〕 凌波不过横塘路，但目送、芳尘去。锦瑟华年谁与度？月桥花院，琐窗朱户，只有春知处。飞云冉冉蘅皋暮，彩笔新题断肠句。试问闲情都几许？一川烟草，满城风絮，梅子黄时雨。 永遇乐·京口北固亭怀古作者：辛弃疾 千古江山，英雄无觅、孙仲谋处。舞榭歌台，风流总被、雨打风吹去。斜阳草树，寻常巷陌，人道寄奴曾住。想当年，金戈铁马，气吞万里如虎。元嘉草草，封狼居胥，赢得仓皇北顾。四十三年，望中犹记，烽火扬州路。可堪回首，佛狸祠下，一片神鸦社鼓。凭谁问：廉颇老矣，尚能饭否？ 青玉案·元夕辛弃疾 〔宋代〕 东风夜放花千树，更吹落、星如雨。宝马雕车香满路。凤箫声动，玉壶光转，一夜鱼龙舞。蛾儿雪柳黄金缕，笑语盈盈暗香去。众里寻他千百度，蓦然回首，那人却在，灯火阑珊处。 清平乐·村居辛弃疾 〔宋代〕 茅檐低小，溪上青青草。醉里吴音相媚好，白发谁家翁媪？大儿锄豆溪东，中儿正织鸡笼。最喜小儿亡赖，溪头卧剥莲蓬。 西江月·夜行黄沙道中作者：辛弃疾 明月别枝惊鹊，清风半夜鸣蝉。稻花香里说丰年，听取蛙声一片。七八个星天外，两三点雨山前。旧时茅店社林边，路转溪桥忽见。 点绛唇·丁未冬过吴松作姜夔 〔宋代〕 燕雁无心，太湖西畔随云去。数峰清苦。商略黄昏雨。第四桥边，拟共天随住。今何许。凭阑怀古。残柳参差舞。 约客赵师秀 〔宋代〕 黄梅时节家家雨，青草池塘处处蛙。有约不来过夜半，闲敲棋子落灯花。 踏莎行·候馆梅残欧阳修 〔宋代〕 候馆梅残，溪桥柳细。草薰风暖摇征辔。离愁渐远渐无穷，迢迢不断如春水。寸寸柔肠，盈盈粉泪。楼高莫近危阑倚。平芜尽处是春山，行人更在春山外。 眉妩·新月作者：王沂孙 渐新痕悬柳，淡彩穿花，依约破初暝。便有团圆意，深深拜，相逢谁在香径。画眉未稳，料素娥、犹带离恨。最堪爱、一曲银钩小，宝帘挂秋冷。千古盈亏休问。叹慢磨玉斧，难补金镜。太液池犹在，凄凉处、何人重赋清景。故山夜永。试待他、窥户端正。看云外山河，还老尽、桂花影。 观潮作者：周密 浙江之潮，天下之伟观也。自既望以至十八日最盛。方其远出海门，仅如银线；既而渐近，则玉城雪岭际天而来，大声如雷霆，震撼激射，吞天沃日，势极雄豪。杨诚斋诗云“海涌银为郭，江横玉系腰”者是也。每岁京尹出浙江亭教阅水军，艨艟数百，分列两岸；既而尽奔腾分合五阵之势，并有乘骑弄旗标枪舞刀于水面者，如履平地。倏尔黄烟四起，人物略不相睹，水爆轰震，声如崩山。烟消波静，则一舸无迹，仅有“敌船”为火所焚，随波而逝。吴儿善泅者数百，皆披发文身，手持十幅大彩旗，争先鼓勇，溯迎而上，出没于鲸波万仞中，腾身百变，而旗尾略不沾湿，以此夸能。江干上下十余里间，珠翠罗绮溢目，车马塞途，饮食百物皆倍穹常时，而僦赁看幕，虽席地不容间也。 己亥杂诗龚自珍 〔清代〕 九州生气恃风雷，万马齐喑究可哀。我劝天公重抖擞，不拘一格降人才 己亥杂诗·其五龚自珍 〔清代〕 浩荡离愁白日斜，吟鞭东指即天涯。落红不是无情物，化作春泥更护花 浣溪沙·谁念西风独自凉纳兰性德 〔清代〕 谁念西风独自凉，萧萧黄叶闭疏窗，沉思往事立残阳。被酒莫惊春睡重，赌书消得泼茶香，当时只道是寻常。 生于忧患，死于安乐作者：《孟子》 舜发于畎亩之中，傅说举于版筑之间，胶鬲举于鱼盐之中，管夷吾举于士，孙叔敖举于海，百里奚举于市。故天将降大任于是人也，必先苦其心志，劳其筋骨，饿其体肤，空乏其身，行拂乱其所为，所以动心忍性，曾益其所不能。人恒过，然后能改；困于心，衡于虑，而后作；征于色，发于声，而后喻。入则无法家拂士，出则无敌国外患者，国恒亡。然后知生于忧患而死于安乐也。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户运营]]></title>
    <url>%2F2021%2F11%2F19%2F%E7%94%A8%E6%88%B7%E8%BF%90%E8%90%A5%2F</url>
    <content type="text"><![CDATA[[Updating] 本文持续更新用户运营相关笔记… 用户运营运营增长框架增长制定一个北极星指标，通过科学的方式分析解决问题以达到该指标数值的增长。 运营和营销区别营销：重声量，偏拉新 运营：重销量，偏维护 AARRR/海盗模型？逻辑：降低直接变现的难度，分阶段完成流量 -&gt; 用户 -&gt; 客户的逐步转换。 acquire activate retention revenu referal 问： 字节为何做社交和电商？ 流量大，留存与创收。 淘宝为什么做社交和内容？ 商品多，经营用户。获客、创收。 微信为什么要做朋友圈和视频号、微店？ 用户多，活跃与留存。 为什么 revenue 在第 4 位？降低直接变现的难度，分阶段地梯度经营用户。 北极星指标？ 描述当前增长阶段和业务经营状况的指标 可有多个，一般为 1+N 的组合。 北极星指标是注册用户，如何指定增长方案？ 业务模式 经营策略 渠道和获客什么是获客渠道？场景获客渠道？Def：能触达用户的渠道，是用户的来源。 场景： 线上：应用市场、广告投放、流量交换、社交传播 线下：地推、线下商家合作 何为异业合作？其核心逻辑？Def：不同领域通过流量交换完成获客。 核心逻辑：用户客群一致。 问： 知乎二维码投放在车展上？ 汽车之家网站为何会有金融相关业务？ 答：用户客群一致。 合作途径：通过用户画像展现一致客群以说服异业合作。 具体形式： 内嵌广告（另行搜索/直接跳转） 后台唤醒（请求打开） 何为渠道转化率？如何评估？Def：考察渠道质量（获客效率）的指标之一。 评估：渠道覆盖客群数 - 引流客群数 - 新增客群数 - 达成业务目标客群数（活跃、留存、其他业务目标） 渠道客群质量？如何评估？ 获客成本 LTV ROI 用户活跃DAU 的定义Def：DAU — 考察线上产品真实使用用户的指标，属于产品质量类指标。 A — Activity：活跃状态（启动、打开、后台执行、执行指定操作、达成特定业务目标） U — User：用户标识（账号、设备、session、IP） 不同场景不同厂商，DAU 计算方法不同： 有账号记账号，无账号记 IP web 端记 Agent app 看设备标识符 DAU 发生波动，如何分析？ 判断波动是否异常及程度 不要上来就分析！任何分析先探究清楚是否有分析的必要。 若异常明显： 拆分新老客户 拆分页面模块，哪个环节流失多 按时段拆，控制时间粒度 按用户金字塔，观察哪个群体流失用户较多 趋势预测，推断未来走向 通知业务，探讨对策 DAU 业务含义和价值？评估产品规模和质量。 $\frac{DAU}{MAU}$ 的定义，用途？Def：考察用户黏性的指标，Facebook 提出 公式：$\frac{DAU{t-1}}{MAU{last 30}}$，即“过去一天的 DAU / 过去 30 天的 MAU” 用途： 适用于日常使用频率高的产品，如：游戏、即时通讯、日常消费 标准（视产品类别而定，微信≈1）：| 一般 | 20% | 高 | 50% | 很高 | $月平均使用天数 = \frac{\sum{i=1}^{30}DAU{t-i}}{MAU_{last30}}$ 常见促活场景？内容/游戏类产品的成长体系、任务体系、在线交流。 例如：工具类产品，为提升活跃，打造社区。 提升 DAU 的思路？任何一个指标的背后由两个东西组成： 业务模式 运营策略 已知 3 月前 15 天的 DAU 和 MAU（14天去重），请预估 7 月 完整的 MAU 最大/最小？ 最大：未知 最小：当前累计 MAU 预估：$MAU = 区段 MAU / 区段天数 * 整月天数$ 用户留存留存定义和价值？Def：核心价值，用户认可程度。 提升留存的思路？ 业务模式 运营策略 对于电商，留存会改为什么指标？复购和复购率。 五类用户的业务含义？ 留存用户：次日、三日、七日、半月内有过使用的用户 流失用户：较长时间周期没有使用或打开产品的用户 跳出用户：使用又很快跳出（没有使用核心功能）的用户 沉默用户：相对固定周期内有过使用，更短周期没有使用的用户 流失用户的运营策略？方式：流失用户大多已卸载或不再使用服务，需要通过其他渠道（短信、社交传媒）触达唤醒。 时机：一般选取大型活动/大版本更新。 沉默用户的运营策略？方式：大概率保留原先触达途径，只是没有使用。通过端口本身的营销活动和触达方式进行召回。 时机：日常推送。 什么是流量？流量 = 活跃 + 留存 营收相关评估用户贡献的方法 内容 内容消费能力 内容产生质量 社交 使用时长 电商 客单价 消费情况（频次、价位） LTV ROI 电商的常见创收方式/生态？ 内容：淘宝的直播、买家秀 社交：拼多多社交关系链、评测 垂直：得物专卖鞋 社交的变现逻辑？逻辑：基于用户间的信任关系（社交关系链）。 内容电商的变现逻辑？逻辑：通过内容做营销（评测、导购）。 电商常见的促销策略？ 价值本身：量贩、套餐，即“控制价格和价值” 流量变现：团购、砍一刀、好友助力，即“流量补贴价格” 外围平台：短视频、评测、导购，即“成本转移，借助其他平台成熟的推广手段降低获客和用户经营成本” 分享传播什么是裂变？Def：典项的社交领域传播方式， 目的是通过裂变建立起人与人之间的社交关系链（建立用户池），不一定是为了变现 利用打通的线上关系为后续活动创收 为什么要做裂变？其本质？本质：建立起人与人之间的社交关系链（建立用户池）。 基于社交关系传播： 传播速度快 获客成本相对较低 用户洞察和画像用户画像是什么?业务价值和意义？一种特征工程：抽取用户特征，以分析用户群体的运营方法 营销：分析客群特征，跟综客群变化。 增长：根据客群变化与特征，制定增长策略。 用户画像组成部分？ 基础属性：性别、操作系统，相对固定静态，相对时间不发生明显变化 行为属性：喜欢浏览什么业务/模块/行为、使用停留时间 业务属性：根据特定产品特定核心 — 商品消费、视频观看时长 交易属性：LTV、客单价、获客成本、消费频次 活跃用户手机型号老旧，如何做活动?推出手机换新优惠活动并推送。e.g. ihpone 出新机型掌银会推送 24 期免息活动。 某用户每周五晚回家且有特定外卖历史，明天就是周五且天气预报可能暴雨，如何制定策略？60 分：下班前主动 push 用户，提示有暴雨可能延误，请提前订餐70 分：以上 + 结合历史订单喜好推荐“您可能想订 xx 餐厅 xx 套餐” 用户画像完整生产流程？ 80 分：需求-圈定用户-汇总指标-分析画像-输出策略-跟进效果。 90 分：建设用户画像模型，自动化定时监控用户画像指标变化。 100 分：将用户画像能力嵌入业务系统中，在业务系统中调用用户画像能力。如：push 的时候选择用户画像生成的标签（是否会点推送-通过推送下单率高）来圈定用户。 用户金字塔？根据多维度的指标将用户划分不同层次，以及每层的用户数量。主要用来分析复杂业务规则下的客群分布，并结合用户画像制定经营策略。 高净值客户/高价值客户/核心客户：高活跃 + 高付费 + 高传播 主流用户/活跃用户/潜在客户 僵尸用户 观察各层用户的流向及变化，及时调整策略。 用户分层和用户分群？基于用户金字塔： 分层（战略层面）：按照业务规则分类，各层有严格分界线，动态（上下流动），规则相对稳定不变（如：二八法则中的 20% 的用户为优质用户） 分群（营销层面）：按照营销规则分类，跨层存在，无严格分界线，静态，规则经常变动（如：不同营销策略圈定不同特点的用户群体） RFM核心逻辑：基于 RFM 三个指标对用户进行分层的模型。 本质：三因素两水平分类器。 类 RFM 拓展： X Y Z RFM 最近购买 购买频次 消费金额 短视频 视频分类 完播率 点击率 资讯 阅读人数 点击率 阅读时长 B 端服务 使用人数 续费率 客单价 过程： 按需选择核心指标（a、b、c） 按需排列核心指标优先顺序（a|b|c） 按需（确定 1/0 的基线标准，常见中位数、均值）设定核心指标的值（分别 j、k、l 种，共 j·k·l 个分类） 按分类客群特征打标签，并统计数量 研究各类客群的用户画像，制定营销策略 问： rfm 只能是 rfm 吗？其他指标？ 可以任选，根据分类目标自由定制。 rfm 只能是 3 个指标吗？3 个以上可以吗？ 可以超过 3 个，但一般不。原因： 抓住核心：影响最终决策的因素往往不超过 3 个，选取三个较为合理。 维度合适：超过 3 个指标，分类指数级增长（分类爆炸），增加分析复杂度，降低营销效率、操作实用性。 rfm 只能是 1/0 吗？可以是多个值吗？ 可以是多个值，视分类目标而定。]]></content>
  </entry>
  <entry>
    <title><![CDATA[电商笔记]]></title>
    <url>%2F2021%2F10%2F07%2F%E7%94%B5%E5%95%86%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[[Updating] 本文持续更新电商相关笔记… 电商相关笔记电商模式 模式 定义 e.g. B2B（Business to Business） 企业间的 EC（Electronic Commerce） 可口可乐与麦当劳的供应 B2C（Business to Consumer） 企业对个人用户的 EC 京东自营 C2C（Consumer to Consumer） 个人对个人的 EC 淘宝，客户之间把东西放上网去卖 C2B（Consumer to Business） 个人对商家的 EC 客户指定内容和价格，由商家决定是否接单 M2C（Manufacturers to Consumer） 厂家对个人的 EC 厂家直销，减环节降成本 I2C（Info to Consumer） 给消费者提供信息的 EC 美团、大众提供打折团购、优惠信息 某天用户急剧下降流程梳理 异常确认 确认数据准确性： 自身： 根据纵向历史数据和横向同行数据，判断是否异常 现状≠问题，现状+标准=问题 逻辑判断：占比合计100%；正负零；计算公式 外界： 向业务确认统计口径，对比方式（同环比？）和程度（数量差距和百分比？） 向技术确认取数逻辑是否存在谬误 根据历史数据，计算同环比，确认是否周期性/季节性变化/节假日/产品服务生命周期 维度拆分：产品线、业务场景、版本、端口类型、用户属性 以用户为例： 新用户： 流失新用户的获取渠道状况 末次归因确定渠道问题 更新各渠道的ROI，留存效率，下次注意 老用户： 根据既有内部的用户分层，查看是哪一层的用户大量流失 提取流失用户的基本信息，通过聚类确定流失的是哪部分群体 产品改动：入口改动、流程改变、引入 BUG 新版本内容劝退用户？e.g. 活动复杂、副本太难 更新引入 BUG 或体验不佳？e.g. 无响应、延迟 运营活动：策略反效果、用户逆反 近期是否开展活动，用户反感？e.g. B 站南京夏日祭 羊毛党扎堆？e.g. 每个新号可以领取一项优惠 环境变化：内 + 外 宏观环境分析：PEST P：近期是否存在不利于产品的相关政策。e.g. 《电子烟管理办法》 E：产品相关产业缺乏经济支撑。e.g. 经济下行，高附加值产品销量惨淡 S：社会因素。e.g. 疫情爆发 - 线下减少，线上增加 T：新技术淘汰落后技术。e.g. 变频空调替代定频空调 竞争环境分析：波特五力 供应商：供应紧张、质量问题。e.g. 疫情停工停产无货；食品安全问题曝光。 消费者：无法消费、放弃消费。e.g. 品牌公关危机如辱华、辱女、丑闻。 直接竞争者：新竞品吸引风头、推广抢客。e.g. lol 吸引 dota 客群；饿了么答题免单美团用户减少。 间接竞争者：功能可替代竞品。e.g. 吸尘器逐渐被扫拖一体机器人替代。 潜在进入者：宣传造势。e.g. 传闻特斯拉上海建厂。 其它 客户职业 学生/工作党：周末，工作学习类骤降，休闲娱乐类上升。 客户集体 2B：企业集体用户为主，弃用换平台易出现规模流失（原因探查 — 竞品分析、问卷调查）。 2C：用户较为零散、组织度低，难以出现大规模同时流失（刚上线时的第一批用户服务到期，不再续费）。 产品指标体系与活动设计（星巴克APP为例） 搭建指标体系 思路：围绕“人-货-场” 解： 用户 新增用户数 日成交用户数 用户分层（消费水平）、分群（使用习惯 — 只看不买/一看必买） 行为指标 浏览、加购、用券、购买 转化率 商品 日/周/月销量 库存指标 SKU 数 各 SKU 库存数 交易 营业收入 （优惠券）折扣金额 毛利润 = 营业收入 - 营业成本 毛利率 = 毛利润 / 营业收入 营业利润 = 营业收入 - 营业成本 - 各项期间费用 各项期间费用 = 营业税金及附加 - 销售费用 - 管理费用 - 财务费用 - 资产减值损失 + 公允价值变动损益（ - 公允价值变动损失）+ 投资收益（ - 投资损失） 净利润 = 营业利润 - 所得税 = 营业利润 *（1 - 25%） 净利率 = 净利润 / 营业收入 服务 派送指标 平均订单时长 平均接单时间 平均出单时间 平均送达时间 评价指标 催单率 好评率 差评率 为促进用户余额充值并鼓励余额消费，如何策划活动？ 思路：现状 - 目标 - 钩子 - 策划 - 预案 - MVP - 推广 - 评估 解： 梳理现状，明确活动目标。 根据目的“促进用户余额充值并鼓励余额消费”，制定具体量化目标如下： 新增充值用户 N 人 充值金额达 M 元 余额消费金额达 L 元 ROI （$\frac{人工 + 渠道 + 物流 + 钩子}{余额消费金额}$）达 0.5 确定钩子（奖励）。 只有保证“奖励价值 &gt; 用户预期付出成本”，用户才会参与。 奖励价值不仅限于金额与价格，戳中用户痛点即可。如：网红周边（冬奥会-冰墩墩、KFC-可达鸭、迪士尼-玲娜贝尔）、高级会员。 梳理活动路径。 具体环节准备。 渠道选择（微信？微博？APP？） 活动的包装设计（洗脑文案？吸睛大图？爆款视频？） 活动页面/程序 活动预案。 模拟用户体验活动整体环节，预想出所有可能出现的问题并作预案。 MVP 测试。 最小可行性产品 (Minimum Viable Product, MVP)：用最小成本验证核心策略是否能达成目标。 先对小部分用户投放活动 根据数据反馈，及时调整活动： 用户参与度如何？— 钩子是否有吸引力？ 环节转化率如何？— 活动流程是否需要优化？ 符合预期，逐渐增大投放规模 发布活动。 如何评估活动效果？ 与既定目标的比值： ≥1：完成目标，完成/超出比例，完成原因分析 ＜1：没完成目标，缺额多少，没完成的原因分析 同/环比增长 ROI 总结优缺点，经验归档。 指标相关复购/回购 复购：同一统计周期内重复购买 回购：第 t 个统计周期内购买，第 t+1 个统计周期内重复购买 TGITarget Group Index, 目标群体指数，又称“偏好度”。 $TGI指数 = \frac{目标群体中具有特征 A 的群体所占比例}{总体中具有特征 A 的群体所占比例} * 标准数100$ 反映目标群体在特定研究范围(如地理区域、人口统计领域、媒体受众、产品消费者)内的强势或弱势的指数 高于 100，代表该类用户对某类问题的关注程度高于整体水平 e.g. 中老年微信用户使用视频号的比例为 60%，整体微信用户使用视频号的比例为 30%，那么 TGI 指数为 200，说明“中老年微信用户是整体用户中更强势的视频号受众”或“中老年微信用户对视频号的关注高于整体水平”。 注册类指标和活跃类指标，要看哪个?没有固定答案，应该视当前阶段和场景而定（AARRR）。 北极星指标：KPI/OKR 中的 K（Key，关键），评估当前业务还有多少差距？是否产生偏差？决定未来产品的方向。 虚荣指标：一般是累计指标（只见长，不见跌）。如：注册类/新增用户（活跃与留存）。 一些思考如何看待平台偏向优惠补贴新用户而不是老用户？明确新老用户标准：累计时长？使用天数？消费频次？ 常识前提： 老用户价值高于新用户。原因如下： 经历过试水期，老用户消费频次、消费水平、累积贡献、平台忠诚度高于新用户。 新用户补贴后利润低于老用户。 一般情况下，增量市场才需要大力优惠补贴新用户而非老用户，存量市场更看重老用户。 应基于“整体利益最大化”原则，而非主观认为“应该对谁好” 从平台环境考虑： 内部环境：老用户有粘性，持续消费累积贡献，但单个获取利润总有上限，所以需要增加老用户数量，因此必然先要获取新用户并促使其转化为老用户 外部环境：用户市场份额固定，不抢占就会拱手让于竞争平台 若产品从每个用户中获取的价值是固定的，那么当老用户的价值已榨干殆尽，则偏向新用户是必然。 从用户考虑： 新用户： 对平台接受度不高 — 需要“勾子”促进使用 老用户： 相比新用户，对平台有一定认知 — 使用没有认知成本 相比新用户，产生粘性，有一定沉没成本 — 持续使用；不会换平台 实际上，平台不应过分偏向新用户，补贴应该动态均衡。假设某平台大量优惠新用户，轻视老用户，可能左右为难： 作为平台收入支柱老的用户使用意愿下降甚至离弃 — 收入降低，口碑下滑 新用户对成为老用户的预期不好，薅完羊毛就跑 — 拉新 ROI 低]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据分析思维]]></title>
    <url>%2F2021%2F09%2F30%2Factive-data%2F</url>
    <content type="text"><![CDATA[[Updated] 本文基于《活用数据》整理了数据分析思维的相关内容。 数据分析思维先行概念市场营销企业在现有营销环境下，根据目标消费者的需求，利用现有的资源和能力，比竞争对手更快捷、更有效地向目标消费者提供产品和服务，实现企业盈利以及可持续发展的生产和经营活动。 — 现代市场营销学之父 菲利普·科特勒 为更好地驱动企业营销业务，围绕市场营销的定义展开作数据分析。 案例 角度 企业战略分析 现有营销环境 用户偏好分析 目标消费者的需求 STP 目标消费者的需求 品牌建设分析 让消费者选择自己 营销组合分析 比竞争对手更快捷、更有效 数据分析定义：数据分析是实现研究目的与研究内容的闭环。 做法：首先将研究目的分解为内容，然后再通过研究内容实现研究目的。 具体流程： 明确分析思路 获取数据 处理数据 分析数据 解读数据 定性分析与定量分析 特点 定性分析 关注意义，能做梳理，不能作为选择 定量分析 关注频率，量化成具体数据作为指导 预测预测是推断的过程： 由已知推断未知 条件：已知和未知具有相似性（类推原则）或相关性（相关原则）。 由过去推断未来 条件：市场具有惯性（惯性原则）。 预测的类型： 定性预测：基于经验和判断推断，主观性强。 定量预测：基于数据和模型推断，客观性强。基于同样的数据和模型，结论一定相同。 推断思路： 类推原则： 相关原则 惯性原则 解决什么问题 - 明确分析思维 识别机会 规避风险 问题诊断 营销效果评价 量化管理 分析哪些内容 - 开启分析思路学会提问 发散思维，头脑风暴，罗列出关键问题 MECE 原则（mutually exclusive collectively exhausted） 分解出的各项内容相互独立，交集为空集；汇总在一起完全穷尽，并集是全集。 核心：不重不漏 方法：二分法…等 使用模型或结构化思维对罗列的关键问题进行归纳划分 熟悉模型 模型是经过长期检验的成熟的分析思路，灵活运用可以事半功倍。 熟悉常用模型及其适用场景： 适用场景 模型 顾客满意度 RATER 产品属性优先度 KANO 战略选择 SWOT 战略选择 IEF 内外因素评价矩阵 用户行为 用户行为五阶段、5W2H 品牌形象 品牌知觉图 品牌知名度 Graveyard 定价决策 PSM 营销组合 4P营销理论 用户转化 AARRR 环境分析 PEST 市场分析 波特五力 结构化思维时间与结构思维 “四方上下曰宇，往古来今曰宙。” 时间：事务的过程，即发展阶段 结构：事务的方面，即构成要素 演绎思维 演绎而不是归纳。 原因 演绎：由共性原理（或假设）推出个性结论 归纳：由个性推出共性 标准式演绎 — 三段式 大前提：已知共性原理（或假设），该原理（或假设）具有一般性和普遍性。 小前提：关于对所研究对象的个性情况的描述。小前提应与大前提有关。 结论：从共性原理（或假设）推出对所研究对象的具体判断。 例如：（大前提）18 岁以上为成年人，（小前提）叶某今年 20 岁，（结论）那么叶某是成年人。 常见式演绎 — 4W What’s going on? 描述现象 Why did thits happen? 分析原因 What lies ahead? 判断趋势 Which course of action should i take? 提出对策 重要性思维 将资源花在关键改进点上。 分析到什么程度 - 打开分析视角对比视角对比类型 参照物 与自身纵向对比：过去和现在对比 与其他横向对比：与同类或相似的其他对象对比 对比指标性质 频数统计：分类型数据 均值分析：数值型数据 对比可信度 时间可比性：刨除特殊时段/时间粒度和长度一致。例如：夏天卖棉袄和冬天卖棉袄。 空间可比性：刨除特殊环境。例如：北极卖棉袄和赤道卖棉袄。 数量可比性：同量纲 统一单位，使数据可比。例如：身高（cm）和体重(kg)。 方法：变异系数法 — $v = \frac{\sigma}{\bar x}$，刻画单位平均水平下的差异。 剔除数量量级差异，使变量统一可比。例如：手掌长度与身高（接近 10 倍）。 方法：变量标准化 — $Z = \frac{x - \mu}{\sigma}$，各变量均值和标准差都化为 0 和 1。 分类视角意义同类共性，异类区别，针对性营销。 方法以客户分类为例： 分类维度处理：标准化、因子分析 客户细分检验：聚类分析、方差分析 目标市场分析：矩阵分析 目标客户定位：方差分析、交叉分析、比较分析、对应分析 相关视角相关定义A 与 B 相关有以下情况： 因果：A -&gt; B 或 B -&gt; A； 共存：C -&gt; A 且 C -&gt; B。 注：要判断因果，先证相关性。 相关判定 数值型：相关系数 — $r(X, Y) = \frac{Cov(X ,Y)}{\sqrt{Var|X| Var|Y|}} = \frac{E|XY| - E|X|E|Y|}{\sqrt{Var|X| Var|Y|}}$ 分类型：方差分析 — $SST（总体信息） = SSR（组间） + SSE（组内）$ SSR 大 =&gt; 分组属性引起差异，相关 SSE 大 =&gt; 分组属性以外的属性引起差异，无关 综上，SSR 小，SSE 大 =&gt; 无关，不存在显著差异 若 P(SSR = 0) &lt; 0.05，拒绝假设“SSR = 0”，即拒绝“无关”，则表明“相关”，即“存在差异” 应用 规模预测 例如：道格拉斯生产函数 $y = A K^\alpha L^\beta \mu，A-技术，K-资本，L-劳动力，\alpha 、\beta-弹性系数，\mu-干扰因子$ 精准预测 例如：方差分析（验证相关） + 交叉分析（偏好差异），对不同用户群体进行精准营销。 描述视角整体趋势 集中趋势：加权平均分 $\bar x = \frac{\sum x_i f_i}{i}$ 离中趋势：变异系数 $v = \frac{\sigma}{x}$ 个体波动 研究单独个体 离群、异常发现 研究个体间差异 具体业务中的视角选择不同层次组合应用。 模型RATER客户满意度模型： 硬实力 外在：Tangible 有形度 有形的服务设施、环境、服务人员的仪表以及对客户的帮助和关怀的有形表现。 内在：Assurances 专业度 企业的服务人员所具备的专业知识、技能和职业素质。 软实力 外在：Responsiveness 反应度 服务人员对于客户的需求给予及时反应并能迅速提供服务的愿望。 内在： Reliability 信赖度 企业是否能够始终如一地履行自己对客户所做出的承诺，获得良好的口碑与客户的信赖。 Empathy 同理度 服务人员能够随时设身处地地为客户着想，真正地同情理解客户的处境，了解客户的需求。 KANO属性分类： 属性 含义 用户情绪 重要性 示例 必备属性 M（Must be） 产品或服务的最核心的属性 有则无感，无则厌恶 1 + 手机能打电话 一维属性 O（One-dimensional） 与用户态度线性正相关的属性 有则好感，无则厌恶 2 \ 手机电池快充 魅力属性 A（Attractive） 用户期望的属性 有则喜，无无感 3 \ 手机高刷新率 可有可无属性 I（Inessential） 无论是否具备该属性，用户都无所谓 有无皆无感，无关紧要 4 - 手机编程 厌恶属性 R（Repugnant） 具备了反而让用户不满 有则厌恶，无则无感 × 不该有 手机上传隐私数据 两大原则确定新产品的属性开发顺序： 优先原则（M &gt; O &gt; A &gt; I） 产品研发人员的优先研发顺序：必备属性 &gt; 一维属性 &gt; 魅力属性 &gt; 可有可无属性。 组合原则(M + O + A) 有竞争力的产品应尤三部分组成：必须满足用户的必备属性 + 领先竞争对手的一维属性 + 差异化的魅力属性。 注意事项： 用户的差异性 对于同一个属性，不同用户的态度是不同的。 在已知产品细分市场的前提下，针对不同的细分市场进行 KANO 分析 在不清楚细分市场的前提下，根据 KANO 分析结果对用户需求进行市场细分，以便对不同细分市场提供不同功能的产品或服务 用户需求的发展性 对于同一个属性，其属性不可能恒定不变。即使是创新属性也可能随业界发展变成通用标准，变成一维属性或必备属性。因此，产品或服务的设计者需要进行连续性的 KANO 调查，以把握用户需求的发展和变化。 SWOT 内部\外部 Strength（优势） Weakness（弱势） Opportunity（机会） SO WO Threaten（威胁） ST WT IEF内外因素评价矩阵IEF 内外因素评价矩阵用来对于 SWOT 分析中的数据进行量化： 分别计算割割因素的评分和权重 分别计算机会、威胁、优势、劣势的加权平均数 用加权平均数的大小判断市场吸引力和企业竞争力，并给出相应的战略建议 评分数据来源: 专家访谈：专家适合对具体研究的企业外部因素（PEST）进行评分。 市场调研：消费者对产品或服务有切身体验，适合对企业内部因素评分。 权重确定法 主观赋权法 客观赋权法 思路与优缺点 专家经验主观判断；定性；简单 历史数据研究评价；定量；复杂 常用方法 层次分析法 主成分分析法（或因子分析法） 其他方法 环比评分法、最小平方法 变异系数法、回归分析 权重计算： 计算平均分：$\bar{x} = \frac{\sum x_i}{n}$ 计算频率：$p_i = \frac{n_i}{n}$ 计算方差：$\sigma_i = \frac{\sum(x_i - \bar{x})^2 p_i}{n}$ 计算变异系数：$v_i = \frac{v_i}{\sigma_i}$ 计算权重：$\omega_i = \frac{v_i}{\sum v_i}$ 内部\外部 Strength（优势） Weakness（弱势） Opportunity（机会） SO WO Threaten（威胁） ST WT 用户行为五阶段、5W2H用户行为五阶段： 产生需求 信息收集 方案比选 购买决策 购后行为 5W2H： When 时间 Where 地点 Who 对象 What 事件 Why 原因 How 方式 How much 程度 品牌知觉图品牌知觉图用于品牌形象分析，是指基于品牌形象数据用距离远近反映品牌与精神价值相关程度的图形。距离越近，表示相关程度越大。 解读方法： 圆心定理 - 最符合品牌的形象 以各品牌为圆心，最先圈进去的指标就是最符合该品牌定位的精神价值。 向量分析 - 品牌具有某形象的程度 从原点向任一指标画一条射线，构成一个向量； 然后将所有品牌对这个向量做垂线； 垂点越靠近向量箭头指向的指标，表明该产品越具有该指标描述的形象。 余弦定理 - 品牌定位相似性/竞争性 从原点向任意两品牌分别画一条射线，构成两个向量。向量夹角越小，则夹角余弦越大，表明两个品牌相关性越强，定位相似性高，具有竞争关系。 原点定理 - 品牌差异性 越远离原点的品牌，消费者越容易识别，说明品牌的特征越明显； 越靠近原点的品牌，消费者越不容易识别，说明品牌没有显著特征，越缺乏差异化认知。 GraveyardGraveyard 模型能够反映提示前知名度和提示后知名度之间的内在关系。 提示后知名度为横坐标，提示前知名度为纵坐标。 回归拟合曲线体现了品牌变化发展的总体趋势和平均发展水平，分布在该回归拟合线周围的品牌则体现了各品牌相对于平均发展水平的波动和差异。 正常品牌：位于回归曲线周围，品牌知名度与市场上的平均水平比较一致。 衰退品牌：位于回归曲线右下方，其提示前知名度明显低于提示后知名度，显现出该品牌被消费者淡忘的趋势。 利基品牌：位于回归曲线左上方，其提示前知名度高于提示后知名度，虽然品牌认知率相对不高，但是品牌回忆率较高，消费者对其忠诚度较高。 强势品牌：位于回归曲线右上方，其提示前知名度和提示后知名度都很高，消费者对其忠诚度很高，这些大都是市场上的强势品牌。 制作方法: 先计算各品牌提示前知名度和提示后知名度； 以提示后知名度为 X 轴、提示前知名度为 Y 轴绘制散点图，每个点代表一个品牌； 对散点做回归拟合线。 PSMPSM（Price Sensitivity Measurement）模型即价格敏感度测试模型。利用 PSM 模型测试价格，不需要预先给出价格，而是让受访者自己表示他们可接受的价格范围。 模型搭建 出示新品 demo 或概念后，询问 4 个问题： 哪个价格让你开始觉得便宜？ 哪个价格让你开始觉得贵？ 哪个价格让你开始觉得太贵而不买？ 哪个价格让你觉得太便宜，不相信它的质量而不买? 根据受访者回答，统计出每个价格在上述 4 个问题上的累计人数百分比 根据每个价格（X 轴）的累计人数百分比（Y 轴）绘图，得到四条相交曲线 由图可确定 两条相交曲线与其交点所构成的上方区域表示接受该交点对应的价格水平的市场规模 最优价格点 两条曲线与交点构成的上方区域面积最大的交点价格即最优价格点。 可接受价格点 “开始觉得便宜”曲线和“开始觉得贵”曲线的交点。 可接受价格区间 曲线所谓区域的左右端点价格。 证明： 最优价格点和可接受价格点都在该区域内； 对于端点，“觉得太贵而不买”和“觉得太便宜而不买”的人数增幅大于“开始觉得便宜”和“开始觉得贵”，导致整体市场规模减小； 因此端点价格即可接受价格区间。 不同市场的规模 设： A = 开始觉得便宜的累计人数百分比 B = 觉得太便宜而不买的累计人数百分比 C = 开始觉得贵的累计人数百分比 D = 觉得太贵而不买的累计人数百分比 有： $可接受者 = 1 - A - C$：对于该价格不觉得贵也不觉得便宜的人数比例 $保留接收者 = A - B + C - D$：对于该价格觉得贵但是不太贵或者觉得便宜但是不太便宜的人数比例。 $不接受者 = B + D$：觉得该价格太贵或太便宜而不买的人数比例。 4P 营销理论产品、价格、渠道、促销合成营销组合。通过营销组合，企业引导商品或服务从生产者到达消费者的决策活动成为营销决策。做出科学的营销决策就需要平衡好做好产品、定好价格、铺好渠道、打好促销这四个方面。 Product 产品 规模预测分析：季节分解法 如何决定生产规模？ 该生产多少？ 在市场经济环境下，该生产多少由市场需求决定，需要预测市场规模。 能生产多少？ 在资源约束环境下，能生产多少是由企业所拥有的生产要素决定的，需要预测产出规模。 市场规模预测和产出规模预测统称“规模预测”。 产品属性分析：KANO 模型 根据市场所需要的产品属性及其需求程度，挖掘消费者对产品的核心需求与偏好，明确产品属性开发优先级。 Price 定价 定价既是产品优劣的反映，也是顾客眼中的产品价值，是产品的竞争性定位和销售力的体现。 定佳决策分析：PSM Place 渠道 渠道价值分析 - “渠道为王”，是影响企业能否赢得市场的一个重要竞争力。在多渠道共同发生作用时，只有准确地对每个渠道的价值进行评价，才能做好渠道资源分配，实现效益最大化。 Promotion 促销 营销者向消费者传递有关本企业及产品的各种消息，说服或吸引消费者购买其产品，以达到扩大销售量的目的。为了达到促销目的，通常会在约束条件下搭配媒体组合使传播效果最优，常用线性规划解决。 资源配置三要素： 目标函数 约束条件 决策变量 资源配置方法：线性规划 AARRR用户生存周期可归纳为 AARRR 漏斗模型。 Acquisition 获取关键：降低用户的使用门槛，结合不同阶段用户群体的特征，制定最适合的拉新策略，同时时刻关注各个核心数据指标；而不是考虑各种渠道推广引流。 产品角度 简化注册登录流程 账号强相关产品可采取手机+短信验证码快捷注册登陆；非账号强相关产品可在用户体验核心功能时才要求注册登录。 滞后权限授权 体验相关功能时才进行授权；安装后第一时间授权影响体验。 用户初次进行产品介绍/操作指引 适量（&lt;=4）的页面/流程介绍核心/优质内容，复杂功能进行操作指引降低使用门槛；忌繁琐冗长。 运营角度 产品不同阶段中用户群体的特征不同，可从冷启动期/增长期/稳定期/衰退期四个阶段看： 冷启动期 现状：只有第一批少量用户。 措施：关注用户质量而非数量，打造用户交流社区，评估核心用户的反馈，对产品进行打磨。 关注指标: DAU、留存、活跃时长。 增长期 现状：产品经过冷启动期的打磨得到核心用户认证，开始进入飞速增长阶段。 措施：结合目标群体的普遍特征，寻找合适的渠道进行推广引流。 关注指标：ROI — 以以尽可能少的成本，获取高质量的用户。 稳定期 现状：由于产品用户群体的不同，产品的用户规模达到增长瓶颈。 措施：寻找产品是否有延展方向，通过需求的延展获取更多用户。 关注指标: 用户留存。 衰退期 现状：产品早晚会步入衰退期，用户慢慢转移至其他替代产品，用户数量逐渐降低。流失用户的召回存在于每个时期，在此阶段则尤为重要，往往需要面临较大的困难与成本守住用户流量。 措施：召回流失用户，减缓产品的衰退。可从四个角度召回： 利益驱动召回：优惠券、礼品、抽奖等活动； 社交属性召回：社交联动（关注、评论、邀请）； 产品核心需求召回：周期性收益产品查看/获取收益； 新功能刺激召回：版本重大更新、新功能。 关注指标: 召回率，流失指标。 Activation 激活注：激活 != 用户注册。对于账号强相关的产品，可以将注册作为用户激活的一个参考依据。从整体上来讲，用户激活更应该考虑的是用户对于产品核心功能的使用情况。比如：微博用户活跃情况、抖音用户短视频观看情况。 关键： 找到自己产品用户激活的标准。 根据产品的类型特点，做对应的产品模块设计，同时辅以运营活动，创造用户可触达的需求场景，进而达成目标，激活用户。 关注核心指标，而非一味提高用户注册量。 不同类型的产品用户群体不同，用户激活的方法也不尽不同，整体上可分为三种类型进行讨论： 单用户产品 单用户产品：服务于个人用户，没有过多的不同用户之间的接触以及不同角色的产品。大致可分为两类： 工具类 核心：便捷使用，舒适体验。 工具产品需要拿来即用，所以必须简化流程，让用户速上手。 工具产品设计应该符合生活中的逻辑，降低使用门槛，增强用户体验。 游戏 核心：提升代入感。 合适的玩家指引：无论单人还是网络游戏，玩家在游戏初期都存在认知的过程，应该尽快让玩家了解规则，体验游戏。 由浅入深的体验：游戏初期应该给予玩家适应的过程，初期可以降低游戏的难度和复杂度，先保证游戏体验，待其适应再逐步放开。 多用户产品 多用户产品：多用户接触的产品。根据用户关系，分为两个不同的类型： 熟人产品 核心：获取熟人的联系。 作为通讯类产品若不能及时联系到熟人，就没有沟通对象，则失去使用意义。例如：微信能够通过手机通讯录和 QQ 好友快速建立起与熟人之间的联系，开启新的通讯体验，所以很快获取了大量的用户。若新的通讯 APP 需要通过一套新的 ID 标识（与原本的联系方式无关联）去相互添加才能使用，这无疑增加了使用成本。 陌生人产品 核心：搭建起陌生人之间的联系，即用户品配。 水军：投放一定的虚拟用户，用于激活前期为用户创造需求场景。但随着用户体验的深入，水军会被用户察觉并反感，因此只适用于激活前期。 用户匹配：通过为用户匹配或者推荐用户，促成用户之间的联系，进一步沉浸于产品中，达成用户激活。常见的匹配策略：LBS 匹配/用户属性匹配/个性化推荐匹配。 多角色产品 多角色产品：多用户产品且用户存在不同角色。如：美团 — 既有商家也有买家；Uber — 既有司机也有乘客。 核心：激活各方用户，并达成多角色之间的平衡连接。以 Uber 为例：通过首次免单吸引乘客；通过算法实现多订单下司机与乘客的合理匹配，保证供需的稳定；通过司机补助、乘客优惠促成订单的产生。 可通过核心指标进行用户的激活与连接情况： 用户激活率：即用户使用核心功能的占比。各产品的激活定义规则不尽相同，每个产品都必须找到适合自己的激活定义； 用户激活花费时长：用用户从进入产品，到被激活所花费的时间。时间越短，证明激活效果越明显； DAU/MAU：用户日活跃/月活跃的比值。不同类型的产品的 DAU/MAU 存在一定的基准线，如移动游戏的基准线为20%，工具类APP为40%。比值越大，说明用户对于产品的粘性越强，激活效果越明显； DAOT：用户日均使用时长。使用时长越长，说明用户的粘性越强，但同时需结合其他指标，评估是否产品流程过长导致的时长增加。 Retention 留存用户留存在任何时期都是评估产品是否真正具有价值的重要因素。只有用户感到了价值，才会选择留下了。 关键：使用户持续使用产品，形成稳定的依赖。 有效地评估产品的留存水平的指标： 第 N 天计算法 次日留存：统计日新增用户次日仍然使用产品的用户数量占总新增用户数量的比例； 7 天留存：统计日新增用户第七天仍然使用产品的用户数量占总新增用户数量的比例； 30 天留存：统计日新增用户第七天仍然使用产品的用户数量占总新增用户数量的比例。 强调第N天，其反应的结果也就是随着时间的推移，留存用户逐渐减少，而行业上也存在着对应较为权威的基准 4-2-1 基准，也即40%/20%/10% 为此计算方法下，较为合理的一个水平。 N 天内计算法 次日留存：统计日新增用户次日仍然使用产品的用户数量占总新增用户数量的比例； 7 天留存：统计日新增用户7天内，再次使用产品的用户数量占总新增用户数量的比例； 30 天留存：统计日新增用户30天内，再次使用产品的用户数量占总新增用户数量的比例。 强调N天内，其反应的结果就是30天留存 &gt; 7天留存 &gt; 次日留存，更多的是表现一个产品的活跃水平。 改良后第N天计算法： 次日留存：统计日新增用户，再次使用产品的时间间隔小于24小时的用户所占比例（T+2出数据指标）； 7天留存：统计日新增用户，再次使用产品的时间间隔小于7个自然天然填的用户所占比例（T+8出数据指标）； 30天留存：统计日新增用户，再次使用产品的时间间隔小于30个自然天然填的用户所占比例（T+8出数据指标）。 改良后的第N天计算法，可以准确地反映新增用户的留存水平，避免特殊场景造成的数据指标影响。 改良后N天内计算法： 次日留存：统计日活跃用户中，次日再次使用产品的用户占比（T+2出数据指标）； 7天留存：统计日活跃用户中，往后7天内再次使用产品的用户占比（T+8出数据指标）； 30天留存：统计日活跃用户中，往后30天内再次使用产品的用户占比（T+8出数据指标）。 针对N天内计算法，由于计算方式反映的更多是产品的活跃水平，并不单纯针对新增用户，产品的迭代过程中，往往会有很多核心功能/用户体验等的改变，想要真实观察产品迭代过程中的留存活跃情况，应当将用户群体进行扩展。 改良后的N天内计算法，更能清晰/完整地表现所有用户的活跃留存情况。 留存标准 不应该盲目套用，而是应该结合自身产品所处的行业、产品形态以及自身产品定位进行考虑。 提升留存的方向: 产品方向 提升自己产品的竞争力，不断满足用户的需求，并优化用户体验。 提升用户对于产品的粘性。 日常活跃功能：培养用户习惯。如：日常签到及其奖励体系。 用户激励体系：鼓励用户行为，并给予认可的反馈，激励用户，成正向循环。如：用户头衔、用户等级。 强化用户投入：沉淀用户在产品上的行为，使用户对产品产生依赖。即使后来出现竞品，用户也不会轻易放弃。如： 时间投入：通过阶段性的任务，量化并强调用户的时间投入，使用户不甘放弃； 金钱投入：通过年度会员/月度会员等，强化用户的金钱投入，增加用户离平台的损失； 内容投入：通过 UGC 内容的沉淀，强化用户的内容投入，比如微博/朋友圈/笔记类产品等，用户在使用过程中沉淀了大量的内容，不会轻易放弃； 情感投入：通过引导用户投入情感，产生精神寄托； 社交投入：通过形成稳定的圈子关系，强化用户的社交投入，比如微信等社交平台。社交关系越牢靠，用户对于产品的依赖性也越强。 运营方向 用户挽留：减少用户的流失 明确产品定位及核心竞争力 从流失用户和活跃用户的特征入手，分析用户流失原因 结合二者制定优化方案 用户召回：召回已流失用户 召回的两个契机： 让用户看到 数据关联：通过输入法/浏览记录等，获取并及时为用户推送感兴趣的产品或内容 关注内容发生变动：当用户关注/收藏的内容发生更新变化时，及时推送用户 特定时间段/时间点：固定时间点（节日/生日等），向用户推送相关内容 用户场景变化：当用户所处场景（城市/天气等）发生切换时推送相关内容 让用户想起 要达到让用户主动回想，需要结合产品的调性，长期的诱导或宣传，促使用户的生活记忆与产品产生联系并不断加深认知。 Revenue 变现 用户群体区分 基本特征 合适的变现方式 免费用户 固有思维“互联网免费”，基本不进行消费行为 流量变现；观念引导成为付费用户 普通用户 有一定的消费行为及消费意识，挖掘潜力大 诱导持续消费行为，提高消费额度和频次，养成消费习惯 优质用户 消费金额远大于实际价值，注重精神感受 维护消费后的用户体验，给予特殊待遇 变现方式： 产品及服务变现 通过与用户直接建立传统买卖关系或为用户提供付费服务获得盈利。 关键： 提高产品的核心竞争力（前提） 找到免费与付费的平衡 平衡在于满足免费用户基本需求的同时，而使付费用户有所收获。在明确用户需求的基础上，对预期需求进行免费，在预期之外的需求（特色服务、体验升级）进行收费。 付费行为引导培养 上瘾模型及激励策略可以培养用户付费行为，诱导用户提升消费额度和频次，提高产品及服务变现的效率。 上瘾模型：使用户的某些行为发展为习惯。 触发 - 为用户创造一个场景，使其产生我们想要培养的行为。如：产品和服务的试用。 行动 - 用户产生培养行为。如：通过一元体验活动开启会员服务。 激励 - 在用户产生培养行为给予正向反馈。如：给予荣誉成就（精神）或红包（物质）等奖励。 投入 - 引导用户进行付费后的产品体验，沉浸用户的行为成本，增加用户产生持续付费的可能。如：购物礼品卡（单次抵消额度有限），定时提醒余额和限期。 流量变现 广告变现：基于用户，通过广告定向投放进行变现。 现代化广告三要点: 用户精准 - 精准找到广告的目标用户群体，提升触达有效率； 时间精准 - 相同的广告，在用户休闲的情况下，往往能够促成转化；在用户忙碌的情况下，更有可能的则是造成骚扰，引起反感； 场景精准 - 适当的场景下，往往更能促成广告的转化。如：购买手机时推荐耳机。 数据变现：基于用户行为活动，通过用户行为脱敏，抽取过滤加工，形成有效、可利用的数据，通过数据的运用或商业转让，获取盈利。并非卖数据。 构建用户画像，掌握用户的付费意愿及付费倾向，精准把控用户需求，提供推荐或服务，从而获取盈利。还可以根据相似性，进行关联推荐。 相关指标： LTV：客户终生价值， 公司从用户生命周期中所得到的全部经济收益的总和，即从用户上手到离开产品所获取的总收益。 用户付费率：付费用户群体在活跃用户群体中所占规模比例。 一旦用户产生付费行为，便成为付费用户。该指标往往用来衡量产品的付费模块是否能够真正触达用户需求。 二次付费率：付费用户群体中，产生过二次及以上付费行为的用户所占规模比例。 通过观察二次付费率指标，可以评估产品付费模块是否对用户产生正向价值？付费体验是否良好？ ARPU 及 ARRPU：（通常以月份为维度进行统计）ARPU是指平均每用户收入，ARRPU是指平均每活跃用户收入。 用以以评估不用渠道的用户质量，不同时期的用户付费情况。 Referral 传播自传播：无需借助过多外力，产品自身激发用户间的自发传播。 自传播优势： 指数级增长（一传十，十传百） 用户获取成本低（用户自行推荐节省了渠道成本） 用户获取质量高（自传播在相似的用户群体中进行） 口碑效应（用户间讨论形成话题） 自传播步骤： 传播基础 条件： 产品能满足用户需求 传播手段便捷 纵向传播：内容形式转换 横向传播：跨平台分享 自主传播 关键：激发用户传播的欲望。 出发点： 制造话题：话题可能会引起讨论并在人群中迅速传播。如：热搜讨论。 从众心理：个人受到外界人群行为的影响，而在自己的知觉、判断、认识上表现出符合于公众舆论或多数人的行为方式。如：转发锦鲤。 情绪引导：通过情绪上的引导（产生波动、感同身受），能够促使用户进行分享传播。如：水滴筹。 引导参与：通过沉淀用户的行为，促使用户分享自身的成果。如：短视频平台发布视频。 超预期场景：在超预期的场景下，用户分享传播将会变得更加简单。如：买东西被告知中奖。 传播转换 发生自主传播后需要进行有效传播转换，才能达到用户增长的目的。 出发点: 可读性：降低被传播用户的接受门槛，轻松获取信息。如：图表优于文字。 互动：使被传播者与传播者产生互动行为，有参与感或收获。如：帮别人砍价自己也能得到优惠。 注意力：足够吸引用户的注意力，不至于被忽视。如：标题党。 指标： K 因子 = 传播数量（每个用户向他的朋友们发出的邀请的数量）* 转化率（接收到邀请的人转化为新用户的转化率）K 因子直接体现自传播结果水平： K 值大于1时，将激发自传播巨大的力量，K值越大，力量越强 K值小于1，那么传播水平会逐步减弱，直至消失。 PEST宏观环境是指影响市场的宏观因素，可归纳为 PEST。 环境 关键指标 Politics 政治环境 政治/经济体制、财政/税收/产业/投资/补助政策、国际/地区关系 Economics 经济环境 GDP 及增长率、利率汇率、居民可支配收入、产业结构 Social 社会文化环境 人口规模、性别比例、年龄构成、价值观、生活方式、教育状况、消费观念、宗教信仰、风俗习惯 Technology 技术环境 国家重点支持、技术更新与传播速度、商品化速度、技术保护情况 波特五力企业竞争环境可归纳为影响企业生存状态的波特五力。 对象 与企业的关系 供应商 原材料 讨价还价 购买者 产品 讨价还价 直接竞争对手 同类竞品 抢占市场份额，直接竞争 间接竞争对手 替代类竞品 替代品削弱需求，间接竞争 潜在进入者 有可能进入该领域的大企业 抢占市场的潜在威胁 归因分析常见归因分析模型 模型 定义 最后交互模型（Last Model） 认定最后一个渠道的贡献为 100%，因此把转化归功于最后一个渠道 第一次交互模型（First Model） 认定第一个渠道的贡献为 100%，因此把转化归功于第一个渠道 平均模型（Average Model） 认为所有的渠道的贡献相等，因此将权重均摊到参与转化的所有渠道中 时间衰减模型（Time Decay Model） 认为贡献程度随时间而衰减 自定义模型（Customized Model） 以上四种权重分配都比较武断，无法直接指导投放的优化，因此有针对性的产生了生存分析、通径分析、马尔科夫链、夏普利值等模型 夏普利值原则：在合作博弈中，所得与贡献相等。 适用夏普利值的三个特点： 夏普利值的有效性：联盟 S 具有完整性，不存在具有贡献却未纳入联盟里的参与者 夏普利值的对称性：参与者价值 V{a, b} = V{b, a} 夏普利值的可加性：联盟具有独立性，任意两个联盟合并的值等于两个联盟的值的合计，即 V{a, b} = V{a} + V{b} = V(a) + V(b) 设 |S| 表示与该参与者相关的某联盟 S 中成员的数量，n 表示在合作博弈中所有参与者的数量，则 与该参与者相关的每个联盟 S 的加权因子为：$\gamma_n (S) = \frac{(|S| - 1)! × (n - |S|)!}{n!}$ 该参与者价值的夏普利值为：$\varphi(v) = \sum \gamma_n(S) × (V(S) - V(S - {I}))$ RFM 用户画像是了解用户的重要手段，其包含多个方面：用户属性、用户消费特征、用户关联、用户非消费行为…等。其中，用户消费特征是用户画像中最核心、与业绩最直接相关的指标。RFM 模型就是根据消费特征对用户进行分层。 DefRFM 模型是根据最近消费时间（Recency）、消费频率（Frequency）、消费金额（Monetary）三个指标构建的用户分层模型。 R（Recency）：用户最近一次消费时间间隔 即，用户最后一次下单时间距今天多长时间。R 的值越小，用户价值越高。 F（Frequency）：用户消费频率 即，用户在固定的时间段内消费了几次。 该指标反映了用户的消费活跃度。F 的值越大，用户价值越高。 M（Monetary）：用户消费金额 即，用户在固定的周期内在平台上花了多少钱。 该指标直接反映了用户对公司贡献的价值。M 的值越大，用户价值越高。 用户分层以下三种方法可以根据不同需求选用，一般第一种方法最常用，因为可以直观反映用户价值和重要性，然后根据用户价值和重要性采取不同策略，可操作性强。 等级变量划分法 根据R、F、M三个指标数据，将其转化为等级变量，如高、低，具体划分标准可依据指标数据分布确定（可取中位数或平均数作为分界线），再根据三个指标等级划分用户等级。 如果每个指标划分为高、低两种等级，则用户可出现2^3=8种，但可根据实际需要将用户划分为3种等级，如下表所示： |R-Recency|F-Frequency|M-Monetary|用户等级|划分群体类型||-|-|-|-|-||高|高|高|A|重要价值客户||高|低|高|A|重要发展客户||低|高|高|B|重要保持客户||低|低|高|B|重要挽留客户||高|高|低|B|一般价值客户||高|低|低|B|一般发展客户||低|高|低|C|一般保持客户||低|低|低|C|一般挽留客户| 加权得分法 将每一个指标归一化，$x_1 = \frac{x - min}{max - min}$，将每个指标都转化到0~1的区间内。需要注意的是R的取值需要进行转化，取 $x_2 = 1 - x_1$； 赋予指标权重； 计算用户的加权得分，根据得分对用户进行分层。 将指标取值转化为顺序变量，再计算加权得分 根据分位数（如四分位数）将各项指标转化为1,2,3,4顺序变量 赋予指标权重； 计算用户的加权得分，根据得分对用户进行分层。 方法多维度拆解 辛普森悖论：考察数据的整体，和考察数据的部分会得出相反的结论。 Def： 通过不同维度观察同一组数据，发掘数据波动真正的原因。 拆分角度： 指标构成：根据单一指标的构成进行拆解分析。如：买家城市分为一线、二线、三线。 业务流程：根据业务流程进行拆解分析。如：推荐页购物分为浏览、收藏、加购、购买。 示例： 为何推荐页购买率低？ 从指标构成拆解：对于不同城市，买家对本身没有强烈购买意愿的推荐物品的购买决策取决于当地经济水平和收入剩余，所以一线城市购买率可能高于二三线，但二三线用户基数大，所以拉低整体购买率。可以根据城市经济水平推荐价格更合适的物品。 从业务流程拆解：通过各流程发现，浏览/收藏/加购多，而最终购买少，表明用户是感兴趣的但无法下定决心，猜测可能是不符合心里的预期价格导致推迟或放弃购买。可以通过组合优惠或购买赠送优惠券的方式增大购买的吸引力。 逻辑树分析Def： 逻辑树分析法通过将问题的各个要素以逻辑树的形式体现出来，同时从广度和深度两方面找出问题所在。逻辑树结构能够帮助理清思路及层次，避免混乱、重复、无关的思考，从而有针对性地制定策略。 步骤： 找出核心问题，将其放在逻辑树最上层； 思考并罗列出所有能够影响核心问题的因素或思路，将其罗列在第二层； 思考并罗列出所有能够实现各影响因素或思路的方法，将其罗列在第三层； 针对第三次层方法，思考并罗列相应的解决对策； 查漏补缺。 示例： 增加利润 增加销售额 增加销售量 -&gt; 促销活动 提高单价 -&gt; 组合套装 降低成本 降低原料成本 -&gt; 改变供应商 降低人力成本 -&gt; 智能化设备 假设检验Def： 假设结果：对总体参数提出一个假设值 验证假设：利用样本信息判断这一假设是否成立 显著性水平： 群组分析Def： 同期群分析（Cohort Analysis），又称群组分析，根据初始行为的发生时间将用户划分为不同的群组，观察相似群组用户的行为特征表现。 示例： （注：百分比为留存率 = 基期新增用户某月使用数 / 基期新增用户数） 月份 新增用户 +1 月 +2 月 +3 月 +4 月 1 月 106 62% 51% 43% 34% 2 月 122 60% 48% 36% 3 月 279 49% 27% 4 月 302 35% 横向对比：每个月份的新增用户的在几个月内的留存变化情况 纵向对比：比较不同月份的新增用户的新增数量和留存变化 可以观察到： 1、2 月份新增用户数少于 3、4 月份 — 因为 3、4 月份有拉新活动，新用户明显增多； 3、4 月份的留存率低于同期 1、2 月份的留存率 — 3、4 月份拉新得来的新用户并未有效转化，可能新用户羊毛党居多或者不是目标用户。 如果将观察指标从“留存率”换为“日均使用时长”，则可以从另一个角度了解这段时间的变化情况。 因子分析Def： 同类型因素间的相关性会造成重叠信息的扩大化，增加分类偏差。因子分析是数据消减的常用方法，通过数据聚合，用少数不相关的因子反映多个具有相关性的原始信息，起到剔除相关性和数据降维的作用。 步骤： 适用性检验：原始维度具有相关性才能进行因子分析。 因子提取：提取主要因子。常见方法有主成分分析。 因子旋转：（类似旋转坐标轴）有效区隔各因子的维度特征，使之差异化。 因子载荷：表示因子对维度信息的解释程度。 因子命名：根据维度特征的特点对因子进行命名。 计算因子得分：某因子得分越高表明越具有该因子的特征。 聚类分析Def： 用于非监督分类 — 事先不知该分成几类，探索样本数据的内在规律进行归类，使各类别之间具有显著性差异并描述各类别特征。 分类： 层次聚类（系统聚类） 特点：事先不需要知道分几类，树状图会显示出所有的聚类方案。 步骤： 根据样本距离，将距离最近的样本合为一类； 然后计算所形成的类别与其他样本的距离，对距离最近样本再做合并； 依此类推，直到所有样本聚成一类，形成树状图。 迭代聚类 特点：根据指定类别数进行分类。 步骤： 选择初始类中心点； 将每个点按最近距离进行归类，并重新计算形成的新类的中心点； 不断迭代，直至归类正确（每个点到所归类的中心点最近）。 层次聚类与迭代聚类比较： 层次聚类 迭代聚类 思路 逐层合并 不断迭代，以确定类别中心点和类别构成 类别数 事先未知。树状图会显示所有聚类方案，可以从中选择最优方案 事先已知并需要指定。若聚类效果不好，则需要重新设定类别数，重新聚类 计算速度 由于反复计算距离，当样本量太大或者变量比较多时计算速度比较慢 计算量小，内存占用低，运行速度快。常用于处理多变量、大样本的数据 聚类对象 记录与变量均可 只能对记录聚类 数据类型 连续变量和分类变量均可 只可用连续变量 回归分析Def： 探索两种或两种以上变量间相互依赖的定量关系（方程）。 基本概念（以道格拉斯生产函数为例：$y = A K^\alpha L^\beta \mu$）： 自变量与因变量 自变量是因，常用 x 表示；因变量是果，常用 y 表示。 在道格拉斯生产函数中： y（生产规模）是因变量 K（资本） 和 L（劳动力） 为自变量，受企业影响 A 是外生变量，不受企业影响，是一个常数。 一元与多元 元，指变量的个数。 在道格拉斯生产函数中： 存在 K 和 L 两个变量，因此为二元分析。 线性与非线性 如果回归模型中的所有自变量都是一次幂，则是线性回归；否则，为非线性回归。 在道格拉斯生产函数中： 自变量 K 和 L 分别为 $\alpha ∈ (0, 1)$ 次幂和 $\beta ∈ (0, 1)$ 次幂，因此为非线性回归 为了使用线性回归，可以将非线性函数线性化（通过取 ln 对数化），将 K 和 L 转化为一次幂： $lny = lnA + \alpha lnK + \beta lnB + ln\mu$，设 $y_1 = lny$，$K_1 = lnK$，$B_1 = lnB$，则 $K_1$ 和 $B_1$ 为一次幂，与 $y_1$ 存在线性关系。 回归分析预测步骤： 整理数据源与线性化 调用回归分析 确定常量 A 和系数$\alpha$，$\beta$，求出回归方程 假设检验 原因：经验模型不一定适用于所有场景；根据散点图判断回归模型具有主观性。 检验方法: $T$ 检验 对回归系数的检验，思路：若 X 与 Y 相关，则回归系数≠0。检验标准是 $T_{统计量}$ 的伴随概率 $P &lt; \alpha（显著性水平，默认为 0.05）$。 若检验不通过，以 L（劳动力）为例，表明 L 的回归系数等于 0 不是小概率事件，即 L 与 y 的相关性不强，因此提出 L 这个自变量，重新回归；对重新回归后的系数再次检验，知道所有剩余的自变量都通过检验为止。若用剔除法仍有自变量没有通过检验，则表明回归模型不恰当，需要重新建立回归模型。 $F$ 检验 对回归系数的检验，思路：若回归方程有效，则回归方程对样本数据的信息解释量要高于误差项 $\mu$ 对样本数据信息的解释量。检验标准是 $F_{统计值}$ 的伴随概率 $Significance F &lt; \alpha（显著性水平，默认为 0.05）$。 若检验不通过，则表明从样本数据来看，L（劳动力）和 K（资本）并不能充分解释生产规模的变动，即可能还有其他重要的影响因素没有纳入模型中，需要重新建立模型。 回归预测]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>collection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 规范]]></title>
    <url>%2F2021%2F08%2F18%2Fmysql-norm%2F</url>
    <content type="text"><![CDATA[[Updating] 记录 MySQL 使用规范。持续更新… MySQL 规范规范的目的 提高数据库系统的处理效率 避免数据库不必要的死锁及资源浪费 提高应用系统的数据库升级方便 约定 简单应用 MySQL，只做存储，尽量不做运算（计算由程序端 CPU 处理） e.g. 避免 md5（）/ order by rand() 控制表/库数据量 表字段数少而精：IO高效，全表遍历，表修复快，提高并发，alter table 快 单表字段数上限控制在 20-50 个 单表1g体积，500w行评估 顺序读1g文件需n秒 单行不超200Byte 单表不超50个纯INT字段 单表不超20个CHAR(10)字段 合理分表不超载：USERID/DATE/AREA 建议单库不超过 300-400 个表 单库容量： SAS盘Raid 10 控制在500G左右 PCI-E卡，可以控制在卡的大小80%左右 日志或是历史库除外 平衡范式与冗余 效率优先、提升性能 适当时牺牲范式、加入冗余 拒绝3B — 大SQL、大事务、大批量 必须有主键：AUTO_INCREMENT/全局 ID 建议使用 INT/BIGINT 并且自增做为主键，顺序 insert 效率更高，表空间碎片率更低 主键避免采用字符型，如VARCHAR/CHAR/UUID，会导致原本可以顺序写入的请求变成随机写入，效率更低 索引Def 二级索引：非主键索引（聚簇索引）/ 普通索引，只含索引字段+主键，需要回表才能取到完整记录数据 覆盖索引：查询所需数据就在普通索引/非聚簇索引树上，无需回表。 索引（条件）下推（ICP, index condition pushdown）：MySQL5.6 引入，失效索引（索引功能由存储引擎提供）也能在引擎层中进行条件判断（无需回表到 server 层进行判断），减少了回表进而提高查询效率。 最左匹配 ：在联合索引中，只有左边的字段被用到，右边的才能够被使用到。 索引代价时间： B+树维护 键索引的B+树的每一个节点内的记录都是按照主键值由小到大的顺序，采用单向链表的方式进行连接。 增删数据时可能引起主键页内重排序，甚至是数据页的分类与回收。 生成执行计划 优化器会计算每个索引的搜索成本比较生成最优执行计划，索引过多增加该过程耗时，降低 sql 执行效率。 空间：一个索引 == 一颗B+树，B+树的每个节点都是一个数据页，每个数据页默认会占用16KB的磁盘空间，因此，B+树节点越多 -&gt; 数据页越多 -&gt; 磁盘占用越大。 回表代价原则： 能不回就不回 必须回就减少次数 索引创建原则 索引能不加则不加（单表索引数 &lt;= 5） 优先考虑联合索引（可用于覆盖索引） 小表不使用（全表扫描性能差距不大，节约空间） 区分度低（重复值多）的字段不使用 随机无序字段不使用（e.g. 身份证、UUID） 常见优先场景： ID 类字段 搜索/排序/分组，即WHERE / JOIN / GROUP BY / ORDER BY 常用字段 充分设置联合索引，且区分度大的字段靠前 区分度：唯一值个数 / 表长。 降低索引树的高度，查询时减少磁盘 IO 次数，提高效率。 频繁更新字段不加索引 重建索引树的成本高。 字符字段必须建前缀索引，且禁作主键 EXPLAIN 查看 SQL 执行计划的索引使用情况 索引命名： 非唯一索引按照“i字段名称字段名称[_字段名]”索引命名 唯一索引按照“u字段名称字段名称[_字段名]” 索引使用原则 充分利用唯一索引 只需找到一条目标记录即返回 若由 ≤3 个字段组成，且字段都是整数型，使用唯一键作为主键。 索引字段作为单纯左值 索引字段使用运算表达式或函数会导致索引失效 避免隐式类型转换 字段操作类型不匹配会引起隐式类型转换，导致索引失效 数据库隐式类型转换方式检测 select &#39;10&#39; &gt; 9;： 1 — 字符串转数字 0 — 数字转字符串 避免反向查询 反向查询：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT EXISTS、NOT IN、NOT LIKE 等 — 索引失效 遵循 最左匹配 原则 当联合索引中的某字段遇到范围查询 BWTWEEN时，就停止某字段右边索引的匹配，右边部分字段索引失效 e.g. 如联合索引 KEY idx_abc(a, b, c) 遇上条件 WHERE a BWTWEEN (1, 10) AND b = 1 AND c = 2，实际使用到的索引的字段只有 a。可通过 EXPLAIN 中的 key_length 确认。 or 条件列中存在非索引列，or 条件中的索引列也会索引失效 SQL 多条简单 SQL &gt;&gt; 单条大 SQL 减少锁表时间，特别是 MyISAM 多条多 CPU &gt; 单条单 CPU：充分使用硬件，速度快，降低阻塞 简单 SQL 缓存命中率更高 保持事务(连接)短小 事务/连接使用原则：即开即用，用完即关 带锁 SQL 尽量放事务后期，减少占用时间 与事务无关操作放到事务外面，减少锁资源的占用 不破坏一致性前提下，使用多个短事务代替长事务 线上 OLTP 尽可能避免使用 SP/TRIG/FUNC/UDF，由客户端处理 避免 SELECT *，降低 CPU、内存、IO、网络带宽的消耗 改写 OR O(n) 同一字段 — IN，效率 O(n) → O(Log n)，但控制 IN 集合＜200 不同字段 — 分字段查询再 UNION 避免反向查询、前缀模糊查询 反向查询：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT EXISTS、NOT IN、NOT LIKE 等 — 无法使用索引 LIKE% 前缀模糊 — 导致全表扫描 减少 COUNT(*) 开销大，尽量不用/少用/空闲用 计数统计代替 实时统计：用 memcache，双向更新，凌晨跑基准 非实时统计：尽量用单独统计表，定期重算 LIMIT 利用主键索引高效分页 123456789101112-- 传统分页，偏移量越大则越慢SELECT * FROM table limit 10000,10;-- （推荐）获取每页前的最大 id，然后在下一页查询中作为条件传入SELECT * FROM table WHERE id&gt;=&lt;page_id&gt; limit 10;SELECT * FROM table WHERE id&gt;=&lt;page_id&gt;+11 limit 10;-- 取目标页第一条记录的 id，再利用 id 索引取剩余数据SELECT * FROM table WHERE id &gt;= (SELECT id FROM table limit 10000,1) limit 10;-- 取出目标页的所有 id，与主表 join 再强制索引SELECT * FROM table INNER JOIN (SELECT id FROM table LIMIT 10000,10) USING (id);-- 取出目标页的所有 id，再主表 WHERE id INSELECT id FROM table limit 10000,10;SELECT * FROM table WHERE id IN (123,456…) ; 用 UNION ALL 而非 UNION（存在去重开销） 分解 JOIN联接保证高并发 高并发 DB 不建议进行 &gt; 2个表的 JOIN 适当分解 JOIN 联接保证高并发 先缓存高频 join 数据表 分步多次查询再 ID IN 123456789SELECT * FROM tagJOIN tag_post ON tag_post.tag_id=tag.idJOIN post ON tag_post.post_id=post.idWHERE tag.tag=‘二手玩具’; -- 分解为SELECT tag_id FROM tag WHERE tag=‘二手玩具’; -- 取得 tag_idSELECT post_id FROM tag_post WHERE tag_id=1321; -- 取得 post_idSELECT * FROM post WHERE post_id IN (123,456,314,141) 去重用 GROUP BY 有序：GROUP BY 无序：GROUP BY &lt;group_key&gt; ORDER BY NULL 字段间的比较尽量保持数据类型相同 避免隐式类型转换 — 不同数值类型比较，“小往大看齐”，即小类型转为大类型再比较 在non-strict mode下，MySQL会自动帮你把字符串转换成整形，但是如果数值超出了范围，转换就会失败，所以MySQL就按照字符串来处理，因此不能使用索引。而从explain的结果上，并没有表现出这样的差别。 隐式类型转换无法使用索引，相当于应用了函数 减少排序 充分利用索引和 GROUP BY 避免 UNION ALL 和 DISTINCT WHERE中区分度大的条件靠前，具体看执行计划 去掉无意义的连接用条件：1=1，2&gt;1，1&lt;2 等 MyBatis 或 ORM 对象关系映射 替代 动态添加 SQL 标准化。 使用 LOAD DATA 导数据 成批装载比单行装载更快，不需要每次刷新缓存 无索引时装载比索引装载更快 INSERT values ,values，values 减少索引刷新 LOAD DATA 比 INSERT 快约20倍 尽量不用 INSERT ... SELECT：1. 延迟；2. 同步出错 UPDATE 时禁止将“,”写成“and” 12345-- 正确示例update Table set uid=uid+1000,gid=gid+1000 where id &lt;=2 ;-- 错误示例：此时“uid+1000 and gid=gid+1000”将作为值赋给uid，并且无Warning！！！update Table set uid=uid+1000 and gid=gid+1000 where id &lt;=2 ; MySQL 不支持对一张表同时进行UPDATE 和 SELECT（在同一 sql 中对同一张表先SELECT后UPDATE） 常见场景： 123update tb set tb_col1 = 123 where id in (select id from tb where tb_col2 = 789); 解决核心：使 sql 不对同一张表进行操作/另命名 select from 嵌套一层子查询 查询内容另作一张表再 select出来。 123update tb set tb_col1 = 123 where id in (select tmp.id from (select id from tb where tb_col2 = 789) as tmp); inner join 表另名 1234update tb as a join tb as b on b.tb_col2 = 789 and a.id = b.id set a.tb_col1 = 123; 尽量使用 主键 或 WHERE 字段加索引 进行 UPDATE和 DELETE UPDATE和 DELETE是表锁，影响性能。 避免不必要的查询和数据库的交互 123INSERT … ON DUPLICATE KEY UPDATE ……;REPLACE INTO / INSERT IGNORE / INSERT INTO VALUES(), (), ();UPDATE … WHERE ID IN (10,20,50,…); 打散大批量更新 避开高峰期，大批量更新凌晨操作 白天设置上限默认为 N 条/秒 1234update post set tag=1 WHERE id in (1,2,3);sleep 0.01; -- update post set tag=1 WHERE id in (4,5,6);sleep 0.01; 其余约定 命名规范 库名/表名统一小写 MySQL库表大小写敏感，字段名的大小写不敏感。 库名用缩写（2~7个字母） e.g. students course — sc 避免用保留字命名 统一字符集为 UTF8 校对规则：utf8_general_ci 乱码：SET NAMES UTF8 存储emoji字符：utf8mb4 每张表三个必加字段： aid（int/bigint unsigned类型，自增长列，并且作为主键） create_time（timestamp或int unsigned） update_time（和create_time 相同） 不在程序端显式加锁 外部锁对数据库不可控，高并发时是灾难，极难调试和排查 采用事务解决并发更新等一致性问题 相对值修改在 Commit 前二次较验冲突 尽量避免使用子查询 子查询大部分情况优化较差，最好使用 DBA 确认的子查询 用 JOIN 代替 WHERE IN 的子查询 敏感数据加密再存储 严禁明文存储用户密码、身份证、信用卡号（信用卡PIN码）等核心机密数据，务必先行加密，避免明文存储 隔离线上线下 线上连线上，线下连线下。 实时数据用 real 库，模拟环境用 sim 库，测试用 qa 库，开发用 dev 库。 防后悔 操作 delete / update 先 where……select 确认无误后再执行 备份数据 加 limit 降低一次误删的代价 — 通过 binlog 快速回滚 可能提高效率 — 待删除记录 &lt;10 条，limit 10 删完即返回，不 limit 继续扫描直至不符合条件 一次删除数据量大，可能导致大量纪录被锁或 CPU 打满 变更 SQL 应有明细步骤和回滚方案，测试、review再上产线 SQL 命令行修改数据养成 begin……end; 的习惯，方便回滚]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>collection</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行研研究笔记]]></title>
    <url>%2F2020%2F12%2F23%2Findustrial_research%2F</url>
    <content type="text"><![CDATA[[Updating] 本文持续记录与行研研究相关的笔记。 行研研究笔记速通 行业报告 艾瑞、199IT 搜索： &lt;行业报告&gt; filetype:pdf &lt;年份&gt; &lt;行业报告&gt; 信息筛选 国家政策 - 风向命脉 产业链&amp;行业细分 - 升维，了解全局 行业标杆&amp;竞争对手 - 商业模式，最新动作 目标用户 - 普遍痛点，行为习惯，消费模式 介绍过程结合自身优点与相关能力 行业研究 行业概况：判断行业是否正处风口，位于生命周期的哪个阶段 市场分析：市场大小直接决定盈利天花板 产品研究：产品是否具有发展潜力 竞争格局 ：了解行业蛋糕是如何被割分的 监管政策：一切行业都跟着国家政策走 公司研究研究框架 主营业务分析 提供产品 / 服务 商业模式 净利润驱动分析 量 需求提升 市场占有率提升 价 供给收缩 行业进入壁垒 产能供给限制 产品力提升 成本 原材料价格下降 技术升级 管理层分析 控股股东背景 股权激励 B端和C端 区别 B端 C端 目标客群 企业 个体用户 产品价值 企业业务“降本增效控风险”为主 围绕用户情绪价值和实用价值 付费决策对象 企业领导、采购部门（常不直接使用） 用户自身 迭代逻辑 追踪痛点优化更新 盈利方式 多以saas解决方案、项目招投标进行单一采购进行单次或者持续性按年收费 多样 运营方式 商务、市场、销售进行意向客户挖掘，建立联系进行反复营销，预约演示、试用，从而促成订单 以促销、裂变、活动进行拉新，以社群、私域进行持续性经营，通过流量分发进行变现]]></content>
  </entry>
  <entry>
    <title><![CDATA[systemctl 与 service]]></title>
    <url>%2F2020%2F11%2F21%2Fsystemctl%E4%B8%8Eservice%2F</url>
    <content type="text"><![CDATA[[Updated] 本文整理了 systemctl 与 service 的不同。 systemctl 与 serviceserviceservice 解析通过命令查看 service 命令的脚本内容： cat /usr/sbin/service # 通过 “whereis service” 定位脚本位置 可知，其大致功能是： 通过 service 命令，传入服务程序名和服务程序所支持的命令（最起码支持 start stop）； service 脚本在 /etc/init.d 下找到相应的程序脚本； 执行相应命令参数的程序脚本。 service 用法通过 man 函数查找 service 命令： man service 命令用法： service &lt; option &gt; | --status-all | [ service_name [ command | --full-restart ] ] 常用命令 含义 service &lt;服务名&gt; start 启动服务 service &lt;服务名&gt; stop 停止服务 service &lt;服务名&gt; restart 重启服务 systemctlsystemctl 解析CentOS 7 之后，CentOS 使用 systemd 服务管理系统启动和系统服务，具体通过 systemctl 命令来控制。 systemctl 用法用法： systemctl [OPTIONS...] COMMAND [NAME...] 常用命令 含义 systemctl start &lt;服务名&gt; 启动服务 systemctl stop &lt;服务名&gt; 停止服务 systemctl restart &lt;服务名&gt; 重启服务 systemctl status &lt;服务名&gt; 查看服务运行状态 systemctl reload &lt;服务名&gt; 重载配置文件]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>systemctl</tag>
        <tag>service</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 博客从 github 迁至阿里云服务器]]></title>
    <url>%2F2020%2F11%2F20%2Fhexo-migration%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 hexo blog 从 github 迁移到阿里云服务器的过程。 起因国内访问 github 速度较慢且时有抽风，恰逢双十一大促购入阿里云 ESC，尝试迁移至云上。 以下是云服务器和客户端本机的配置过程。 云服务器配置过程云服务器基本信息 选择系统： CentOS 7 安全组配置： 阿里云默认不开放 HTTP（80）端口，故需要在阿里云服务器控制台手动添加安全组规则，授权 HTTP（80）端口的访问，否则将无法访问服务器的 web 应用。 git使用 git 配置云服务器代码仓库，创建 git 用户统一管理。 安装yum install git 配置 创建 git 用户 12345678# 1. 创建 git 用户，并设置密码sudo adduser gitsudo passwd git# 2. 为 git 用户设置权限chmod 740 /etc/sudoers # 先临时给 sudoers 文件添加写权限，修改完毕后及时收回vi /etc/sudoers # 添加 “git ALL=(ALL) ALL”chmod 400 /etc/sudoers 配置 ssh 123456789101112# 1. 生成 ssh 秘钥mkdir ~/.sshcd ~/.sshssh-keygen # .ssh 文件夹下会生成 id_rsa 和 id_rsa.pub 文件# 2. 设置密钥认证，使客户端可免密连接vi authorized_keys&lt;! 文件内输入本地客户端的 ssh 公钥，即本地客户端 ～/.ssh/id_rsa.pub 文件的内容&gt;# 3. 设置只有 git 用户对 .ssh 文件有读写权限chmod 600 ~/.ssh/authorized_keys chmod 700 ~/.ssh 创建 git 仓库 1234cd ~mkdir repo &amp; cd repo git init --bare hexoBlog.gitsudo chown -R git:git hexoBlog.git # 递归修改文件用户组 设置 hooks 1vi ~/repo/hexoBlog.git/hooks/post-receive 编辑内容为： 1git --work-tree=/home/www/blog --git-dir=/home/git/repo/hexoBlog.git checkout -f 保存退出后，给 post-receive 添加执行权限： 1sudo chmod u+x ~/repo/hexoBlog.git/hooks/post-receive 测试本地客户端通过 ssh 连接能够免密登录： ssh git@&lt;域名/服务器公网 ip 地址&gt; NginxNginx 安装 安装 epel-release，以自动配置 yum 的软件仓库 sudo yum install -y epel-release 安装 sudo yum install -y Nginx Nginx 测试 云服务器中，设置开机启动 Nginx： sudo systemctl enable nginx 使用以下命令启动 Nginx： systemctl nginx start 查看运行状态： sudo systemctl status nginx 可以看到进程处于活跃状态和其他相关信息。 在任意浏览器中，访问云服务器公网 ip，若出现 Nginx 欢迎界面，说明 Nginx 安装成功。若没有出现 Nginx 欢迎界面，首先应排查是否安全组未配置，其次观察是否为端口问题（占用或防火墙），再者可观察安装版本，换源/换版本重装。 Nginx 配置注意：因为 yum 源和安装方式不一， Nginx 版本或有不同，配置文件的位置可能会不一样，注意找到对应的文件进行修改。 查看 nginx.conf 下包含的配置文件路径 找到配置文件文件夹在 “conf.d”（即 include 的目录）。 在 “conf.d” 下创建新配置文件 “hexoBlog.conf”，添加以下内容 123456789server &#123; listen 80; # 监听端口 server_name &lt;域名/服务器公网 ip 地址&gt;; location / &#123; root /home/git/www/hexo; # hexo blog 根目录 index index.html; # 索引页，与 hexo 生成的静态索引页应同名 &#125;&#125; 重新加载服务，加载更新后的配置文件： systemctl nginx reload 此处使用 include 方式增加配置文件原因有二： 不破坏原有默认配置文件； 便于以后增删网站配置文件。 本机配置修改hexo修改 hexo 目录下的 _config.yml 文件中的 deploy 模块即可： 1234deploy: type: git repository: git@&lt;域名/服务器公网 ip 地址&gt;:/home/git/repo/hexoBlog.git # git 仓库地址 branch: master 测试发布文章 新建文章 hexo new &quot;test&quot; 生成静态文件 hexo clean # 清除缓存文件和已生成的静态文件 hexo g -d # 生成静态文件并部署，“hexo generate --deploy”的简写 任意客户端通过域名和 ip 进入可看见博客首页即成功。 Issue list 无法访问资源 403 Forbidden. 查看 /etc/Nginx/Nginx.conf 文件中的 “user”，是否为博客所在目录所属用户，修正即可。 若安装 Nginx 时使用 sudo，user 会默认为 root。 定位不到博客文件 查看 conf 文件的 root 目录是否设置正确； 查看 conf 文件的 index 索引页是否设置正确。 博客页面由 hexo 静态生成，对照博客目录下静态生成的页面，查看 index 是否设置错误。]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>云服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析报告]]></title>
    <url>%2F2020%2F10%2F06%2Freport%2F</url>
    <content type="text"><![CDATA[[Updating] 怎样写出一份高可读性的报告呢？ 数据分析报告逻辑走向：结论 - 论点 - 数据支撑 - 图表展现 结构：总-分-总 总：背景介绍，结论先行（数据现状 + 参考标准/使用方法 + 结论） 分：分论点，支撑结论 总：总结发现，提出建议与措施 STAR 原则 S：情景，背景介绍（基于什么背景、有什么目的、做了什么分析） T：任务，采取方法、策略 A：具体过程（采用方法） R：结果（提升/达到目标、发现问题及其措施） 不要有猜测，拿数据验证说话 不说正确的废话（显而易见的结论） 不给显然的、本就该做的措施，应该提出基于分析结论的可落地的建议（如：基于分析结论 xxx，在保持原本该做的 A 策略的基础之上，还应增加采取 B 策略） 不要把过程/手段当作结果表述 不是介绍技术、模型，而是分析结果，并以业务为导向 分析思路过程无需展示（数据分析标准都差不多） 提出问题应该放在背景介绍中告知 无需搭建指标体系（一个公司只有一套，是一个公共基础设施，是所有人都需要共同遵循的规范） 描述要简洁干练、忌流水账（如：人、货、场三个角度分别总结并建议）]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Excel 快捷与函数]]></title>
    <url>%2F2020%2F09%2F21%2FExcel%E5%BF%AB%E6%8D%B7%E4%B8%8E%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 EXCEL 的高频快捷用法和常用函数 Excel 快捷与函数1 快捷1.1 常用快捷操作 快捷键 功能 Ctrl+方向键 跳转至选中方向边缘的数据 Ctrl+Shift+方向键 框选当前位置到选中方向边缘的所有数据 Ctrl+空格键 框选当前列 Shift+空格键 框选当前行 Ctrl+f 查找 Ctrl+h 替换 Ctrl+Alt+v 选择性粘贴 Ctrl+t 隔行上色 1.2 自动填充自动填充数据和函数： 选中单元格，瞄准右下角时鼠标变为十字准星，向某一方向框选区域进行自动填充 选中单元格，瞄准右下角时鼠标变为十字准星，双击自动填充至边缘 框选区域，输入填充内容，快捷键 Ctrl+Enter 自动填充 智能填充： 在第一个单元格手工输入目标值； 框选填充区域； 快捷键 Ctrl+E，自动识别填充规则并进行填充。 1.3 锁定：$锁定行或列，常配合自动填充使用。 2 函数2.1 假设函数2.1.2 IF根据条件进行真假判断，返回真假对应结果： IF(判断条件, 条件为“真”的返回值， 条件为“假”的返回值) 判断条件可以为： 单个条件 AND(条件 1， 条件 2，……) OR(条件 1， 条件 2，……) 示例 说明 =IF(AND(A1 &gt; 1, A2 &gt; 1), 1, 0) A1、A2 都大于 1 则返回 1，否则返回0 2.1.3 COUNT计算包含数字的单元格个数以及参数列表中数字的个数: COUNT(value1, [value2], …) 示例 说明 =COUNT(A1:A5) 返回单元格区域 A1 到 A5 中包含数字的单元格的个数 2.1.4 COUNTIFS统计满足所有条件的次数： COUNTIFS(criteria_range1, criteria1, [criteria_range2, criteria2],…) 示例 说明 =COUNTIFS(A2:A17, “=1”, B2:B17, “&gt;1/1/2010”, C2:C17, “&lt;” &amp; D2) 返回满足“A=1，B 日期晚于 2010-1-1，C &lt; 单元格 D2 数字”条件的单元格的个数 2.1.5 SUMIF对范围中符合指定条件的值求和： SUMIF(range, criteria, [sum_range]) 示例 说明 =SUMIF(A1:A5, “&gt;100”) 返回 A1:A5 区域大于 100 的数值之和 =SUMIF(A1:A5, “=” &amp; C1, B1:B5) 返回 A1:A5 中等于 C1 对应的 Bi 之和 2.1.6 SUMIFS计算满足多个条件的全部参数的总量： SUMIFS(sum_range, criteria_range1, criteria1, [criteria_range2, criteria2], …) 示例 说明 =SUMIFS(A1:B5, B1:B5, “=A*”, C1:C5, “B”) 返回 A1:B5 区域中满足条件“Bi 为 ‘Axx..’ 且 Ci 为 ‘B’” 的 Ai 之和 2.2 数值函数2.2.1 SUM对选中区域数值求和： SUM(number1, [number2], …) 示例 说明 =SUM(A1:B5, D1:F5) 返回 A1:B5 和 D1:F5 区域值之和 快捷键：Alt+= 2.2.2 SUMPRODUCT返回对应的区域或数组的乘积（默认乘法，还可通过公式指定加、减、除）之和（执行完所有操作，最终操作总为求和）： SUMPRODUCT(array1, [array2], [array3], …) 示例 说明 =SUMPRODUCT(A1:A5, B1:B5) 对 A1:A5, B1:B5 对应乘积求和（常见：加权平均） 2.2.3 ROUND（UP/DOWN）对数字按指定位数（向上/向下）四舍五入： ROUND(number, num_digits) num_digits 大于 0（零），则将数字四舍五入到指定的小数位数。 num_digits 等于 0，则将数字四舍五入到最接近的整数。 num_digits 小于 0，则将数字四舍五入到小数点左边的相应位数。 示例 说明 =ROUND(3.14, 1) 四舍五入到一个小数位，返回 3.1 =ROUND(13.14, -1) 四舍五入到小数点左侧一位，返回 10 2.2.4 SUBTOTAL返回列表或数据库中的分类汇总： SUBTOTAL(function_num, ref1, [ref2],…) 其中，function_num 与函数对应表如下： Function_num 对应函数 1 101 AVERAGE 2 102 COUNT 3 103 COUNTA 4 104 MAX 5 105 MIN 6 106 PRODUCT 7 107 STDEV 8 108 STDEVP 9 109 SUM 10 110 VAR 11 111 VARP 示例 说明 =SUBTOTAL(9/109, A1:A5) 对 A1:A5 执行 9/109 代表的函数（求和） 2.2.5 LARGE返回数据集中第 k 个最大值： LARGE(array, k) 示例 说明 =LARGE(A1:B5, 5) 返回 A1:B5 区域第 5 大的值 2.3 定位查找函数2.3.1 VLOOKUP按行查找项目： VLOOKUP （查阅值、查阅值所在的区域、区域中包含返回值的列号、匹配模式） 查阅值：要查找的值 查阅值所在的区域：包含查阅值。查阅值应该始终位于所在区域的第一列 区域中包含返回值的列号：以 A2:C5 区域为例，A 为第一列，B 为第二列进行计数，依此类推 匹配模式：近似匹配（TRUE）或完全匹配（FALSE） 示例 说明 =VLOOKUP(A1, B1:D5, 2, FALSE) 在 Bi 列中找到精确等于 A1 的行，返回对应的 Ci（以 B 为首列，C 为第 2 列） =IF(VLOOKUP(A1, B1:D5, 2, FALSE)=’C’, ‘找到’, ‘未找到’) 在 Bi 列中找到精确等于 A1 的行，取出对应的 Ci，若 Ci 为 ‘C’，返回’找到’, 否则返回’未找到’。 2.3.2 LOOKUP在单行区域或单列区域（称为“向量”）中查找值，然后返回第二个单行区域或单列区域中相同位置的值： LOOKUP(lookup_value, lookup_vector, [result_vector]) lookup_value：LOOKUP 在第一个向量中搜索的值，可以是数字、文本、逻辑值、名称或对值的引用 lookup_vector：只包含一行或一列的区域 注： lookup_vector 中的值必须按升序排列：…, -2, -1, 0, 1, 2, …, A-Z, FALSE, TRUE；否则，LOOKUP 可能无法返回正确的值。 文本不区分大小写。 如果 LOOKUP 函数找不到 lookup_value，则该函数会与 lookup_vector 中小于或等于 lookup_value 的最大值进行匹配。 如果 lookup_value 小于 lookup_vector 中的最小值，则 LOOKUP 会返回 #N/A 错误值 result_vector：只包含一行或一列的区域，与 lookup_vector 大小必须相同 示例 说明 =LOOKUP(C1, A1:A5, B1:B5) 返回 A1:A5 中等于 C1 的行对应的 Bi 2.3.3 MATCH在选中单元格中搜索特定的项，然后返回该项在此区域中的相对位置： MATCH(lookup_value, lookup_array, [match_type]) Match_type 说明 1 或缺省 查找小于或等于 lookup_value 的最大值。 lookup_array 参数中的值必须以升序排序 0 查找完全等于 lookup_value 的第一个值。 lookup_array 参数中的值可按任何顺序排列 -1 查找大于或等于 lookup_value 的最小值。 lookup_array 参数中的值必须按降序排列 示例 说明 =MATCH(“b”, {“a”,”b”,”c”}, 0) 返回 “b” 在 {“a”,”b”,”c”} 中的相对位置 2.3.4 INDEX返回表格或区域中的值或值的引用： INDEX(array, row_num, [column_num]) 数组只包含一行或一列：相应的 row_num 或 column_num 参数是可选的 数组具有多行和多列, 并且仅使用 row_num 或 column_num：返回数组中整个行或列的数组 同时使用 row_num 和 column_num 参数：返回 row_num 和 column_num 交叉处的单元格中的值。该值必须在数组中，否则将返回 #REF! 错误。 示例 说明 =INDEX(A1:B5, 2, 3) 返回 A1:B5 区域第 2 行第 3 列交叉处的值 2.4 格式函数2.4.1 IS检验指定值并根据结果返回 TRUE 或 FALSE： IS**(value) 函数 说明 ISBLANK 值为空白单元格 ISERR 值为任意错误值（除去 #N/A） ISERROR 值为任意错误值（#N/A、#VALUE!、#REF!、#DIV/0!、#NUM!、#NAME? 或 #NULL!） ISLOGICAL 值为逻辑值 ISNA 值为错误值 #N/A（值不存在） ISNONTEXT 值为不是文本的任意项。 （请注意，此函数在值为空单元格时返回 TRUE） ISNUMBER 值为数字 ISREF 值为引用 ISTEXT 值为文本 2.4.2 LEFT/RIGHT/MID文本字符串[从左起 | 从右起 | 字符串中的指定位置]第一个字符开始返回指定个数的字符： LEFT/RIGHT(text, [num_chars])MID(text, start_num, num_chars) 示例 说明 =LEFT/RIGHT(A1, 3) A1 左/右边第 1 个字符起，返回 3 个字符 =MID(A1, 3, 5) A1 第 3 个字符起，返回 5 个字符]]></content>
      <categories>
        <category>Excel</category>
      </categories>
      <tags>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 实战问题]]></title>
    <url>%2F2020%2F06%2F20%2Fmysql-practise%2F</url>
    <content type="text"><![CDATA[[Updated] 本文收集了一些常见 MySQL 实践知识点。（基于《MySQL 实战 45 讲》） MySQL 实战问题普通索引和唯一索引的选择普通索引唯一索引查询过程的索引区别过程 对于普通索引来说，查找到满足条件的第一个记录后，需要继续查找下一个记录，直至碰到第一个不满足条件的记录； 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 性能差距大部分情况下，微乎其微。 InnoDB 以“数据页”为单位来读写数据。当需要读一条记录的时候，会将记录所在的页整体（InnoDB 中数据页的默认大小是 16KB）从磁盘读出来放至内存。连续记录大概率会在同一个数据页内，而在内存中寻找下一条记录的操作只是：一次指针寻址 + 一次计算，这在 CPU 的操作成本微乎其微。当连续记录分别在两个数据页的页尾和页头时，操作复杂度较高。 更新过程的索引区别change buffer当更新一个数据页时，若该数据页还没有在内存中，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入该数据页。直到需要查询该数据页的时，才将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 change buffer 使用的是 buffer pool 里的内存，因此不能无限增大。change buffer 可以通过参数 innodb_change_buffer_max_size 来动态地设置大小，如：当参数设置为 50，表示 change buffer 的大小最多只能占用 buffer pool 的 50%。 change buffer 优点： 减少读磁盘（随机 IO 的访问），提升 SQL 语句的执行速度 避免占用内存，提高内存利用率（数据读入内存会占用内存为 RDBMS 开辟的 buffer pool） 使用场景 一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。因此，对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。这种业务模型常见的就是账单类、日志类的系统。 反之，在写少读多的场景下，每次更新数据都先记录在 change buffer，之后很快就要读数据页并触发 merge，既不能减少随机 IO 访问，又增加了 change buffer 维护成本 merge将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。 触发 merge 的条件： 访问数据页时，change buffer 中有与这个页有关的操作 系统的后台线程定期 merge 在数据库正常关闭（shutdown）的过程中执行 merge 操作。 过程 对于唯一索引： 无法使用 change buffer。 对于唯一索引，所有的更新操作都要先判断这个操作是否违反唯一性约束（即更新后的结果是否唯一），而该过程需要将数据页读入内存，故直接在内存中修改更快，不需要 change buffer。 对于普通索引： 事实上，只有普通索引能用 change buffer。 记录要更新的目标页在内存中，找到目标记录进行更新 记录要更新的目标页不在内存中，将更新记录在 change buffer，语句执行即结束 持久化change buffer 是可以持久化的数据，在内存中有拷贝，也会被写入到磁盘上。 change buffer 有一部分在内存有一部分在 ibdata。 merge 操作会把 change buffer 里相应的数据持久化到 ibdata； redo log 里记录了数据页的修改以及 change buffer 新写入的信息。 如果掉电，持久化的 change buffer 数据已经 merge，不用恢复。主要分析没有持久化的数据情况又分为以下几种： change buffer 写入，redo log 虽然做了 fsync 但未 commit，binlog 未 fsync 到磁盘，这部分数据丢失； change buffer 写入，redo log 写入但没有 commit，binlog 已经 fsync 到磁盘，先从 binlog 恢复 redo log，再从 redo log 恢复 change buffer； change buffer 写入，redo log 和 binlog 都已经 fsync，那么直接从 redo log 里恢复。 注： fsync，同步内存中所有已修改的文件数据到磁盘/储存设备。 索引选择以上可见，类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。建议尽量选择普通索引 + change buffer。 实用场景 在使用机械硬盘时，change buffer 的收效非常显著。如果有一个类似“历史数据”的库，并且出于成本考虑用的是机械硬盘，那么应该尽量使用普通索引，并把 change buffer 尽量开大，以确保这个“历史数据”表的数据写入速度。 特殊场景 如果所有更新后面，往往都伴随着对该数据的查询，那么建议关闭 change buffer。 change buffer 和 redo log两者关系假设更新过程有两个待更新数据页 A、B，A 在内存中，B 在磁盘中。该过程涉及四个部分：内存（buffer pool）、redo log、 数据表空间、系统表空间： 对于在内存中的 A，直接更新内存中的数据表；（写一次内存） 对于不在内存中的 B，在内存的 change buffer 区域，记录下更新信息；（写一次内存） 上述两个动作都记入 redo log 中。（写一次磁盘，顺序写） 若更新后，随即读取这些数据： 对于在内存中的 A，直接从内存读取数据； 对于不在内存中的 B，将 B 从磁盘读入内存中，然后应用 change buffer 操作日志，生成正确的版本再返回。 提升性能上的区别 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写）； change buffer 主要节省的则是随机读磁盘的 IO 消耗。 MySQL 索引选择异常和处理排查方法 explain 命令查看语句的执行情况； 查看优化器选择的索引是否符合预期； 若 key 选择不符合预期，通过慢查询日志（slow log）查看具体的执行情况； 12set long_query_time=0;&lt;查询语句&gt;; 查看扫描行数，是否进行全表扫描/索引扫描。 优化器逻辑优化器通过选择索引，找到最优的执行方案，并用最小的代价去执行语句。优化器主要会结合以下因素进行综合判断： 扫描行数 是否使用临时表 是否排序 扫描行数扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的 CPU 资源越少。 索引统计 统计信息 索引的“区分度”，一个索引上不同的值越多，这个索引的区分度就越好。 索引基数 一个索引上不同的值的个数称为“基数”（cardinality），显然，基数越大，索引的区分度越好。查看一个索引的基数： 1show index from &lt;表名&gt;; 但这个统计结果并不一定准确。 MySQL 获取索引基数 MySQL 通过采样统计得到索引的基数。因为取整张表逐行统计虽然结果精确，但代价太高。 采样统计的方法： InnoDB 默认会选择 N 个数据页，统计这些页面上的不同值； 对 N 个数据页的不同值计算平均值； 用所得的 N 页平均值，乘以索引的页面数，得到该索引基数； 此后，当变更的数据行数超过 1/M 的时候，会自动触发重新做一次索引统计。 存储索引统计的两种方式 通过设置参数 innodb_stats_persistent 的值来选择： 设置为 on 表示：统计信息会持久化存储。此时，默认的 N 是 20，M 是 10； 设置为 off 表示：统计信息只存储在内存中。此时，默认的 N 是 8，M 是 16。 显然，不管 N 取 20 还是 8，采样统计的基数都很容易不准。 优化器对扫描行数的判断 执行 SQL 前，根据统计信息来估算扫描记录数； 针对“直接主键扫描”和“索引扫描+主键回表”两种方案，分别估算扫描行数，选择代价更小的方案。 解决方法扫描行数的估计值不准确的情况 统计信息的修正 explain 的结果预估的 rows 值跟实际情况差距比较大的情况下，可使用命令修正统计信息： 1analyze table &lt;表名&gt;; 再执行则可以正确使用索引。 选错索引的情况 采用 force index 强行选择一个索引 MySQL 会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行。如果 force index 指定的索引在候选索引列表中，就直接选择这个索引，不再评估其他索引的执行代价。 缺点： 写法不佳； 修改了索引名，语句要进行相应改动； 迁库后可能存在兼容性问题。 修改语句，引导 MySQL 使用我们期望的索引 在语义逻辑不变的情况下，修改 SQL 语句，引导其用上我们期待的索引。 如：order by b limit 1 和 order by b,a limit 1，都返回按 b 排序第一行，但后者会使用上 a 的索引。 新建更合适的索引供优化器做选择，或删掉误用的索引 新建更合适的索引的情况比较少见。尤其经过 DBA 索引优化过的库，找到更合适的索引一般比较难。 根据实际情况，若检查发现优化器错误选择的索引其实根本没有必要存在，应予以删除。 字符串字段加索引前缀索引原则使用合适长度的前缀索引，就可以做到既节省空间（减少每个索引长度），又不用额外增加太多的查询成本（回主键索引取数据的次数）。 寻找合适的前缀索引 核心：索引区分度越高越好，那么重复的键值越少。 首先，统计索引上有多少个不同的值 N； 然后，依次截取不同长度的前缀串，查看各前缀串不同的值 M； 最后：选取区分效率最高（M 趋于 N）的截取长度，取该截取长度作前缀索引。 前缀索引对覆盖索引的影响以如下方 SQL 为例 1select id, email from User where email='******@xxx.com'; 若使用对 email 的完整索引，可以了利用覆盖索引，查到结果直接返回，不需要回表再查一遍； 若使用对 email 的前缀索引，即使是 email(14)，在查到结果后，为获取完整信息，都必须回表查找。因为 RDBMS 并不确定前缀索引的定义是否截断了完整信息，此时便用不上覆盖索引对查询性能的优化。 其他索引方式倒序存储 方法 先对字符串数据进行倒序存储，再创建前缀索引。 优点 绕过字符串本身前缀的区分度不够的问题。如：身份证号码。 缺点 不支持范围扫描； 从数据库读写数据时，需要额外进行翻转操作。 hash 字段存储 方法 在表上再创建一个整数字段，保存字符串的校验码，同时在这个字段上创建索引。MySQL 常用校验码 hash 函数：crc32()、crc64()。 优点 控制索引的长度为 4 个字节。 缺点 不支持范围扫描； 校验码可能存在冲突， 查询语句 where 部分要额外判断 id_card 的值是否精确相同。 倒序存储与 hash 字段存储异同 同 都不支持范围查询。 异 额外占用空间：≈ 倒序存储方式在主键索引上，不会消耗额外的存储空间；hash 字段方法需要增加一个字段。实际上，倒序存储使用的前缀长度为了区分度往往不止 4 个字节，二者总体的空间消耗可能相差无几。 CPU 消耗：倒序小 倒序方式每次读写数据时，都需要额外调用一次 reverse 函数；hash 字段的方式需要额外调用一次 hash 函数。从函数的计算复杂度来看，二者 CPU 消耗：reverse 函数 &lt; hash 函数。 查询效率：hash 高 倒序存储 + 前缀索引的方式，可能会增加扫描行数；hash 字段方式的查询性能相对更稳定一些，虽然 hash 有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近 1。 索引方式选择结合业务需求和设备条件，进行合理选择。 MySQL 突然变慢（数据库 flush）脏页当内存数据页跟磁盘数据页内容不一致时，称该内存页为“脏页”。将内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 引发 flush 的场景InnoDB 的 redo log 写满 场景描述 此时系统会停止所有更新操作，把 checkpoint 往前推进，推进区间的日志所对应的脏页全都 flush 到磁盘上，为 redo log 留出继续写的空间。 性能影响 此时整个系统不能再接受更新，所有更新操作将被阻塞，应该尽力避免。 系统内存不足 场景描述 当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给新的数据页使用。如果淘汰的是“脏页”，就要先将脏页 flush 到磁盘。 “刷脏页一定会写盘”的机制保证了每个数据页只会有两种状态： 内存里存在，内存里就肯定是正确的结果，直接返回； 内存里没有，就可以肯定数据文件上是正确的结果，读入内存后返回。 相比“从内存直接淘汰，等到下次请求从磁盘读入数据页，再拿 redo log 应用”的做法，效率更高。 性能影响 InnoDB 用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态： 未使用的； 使用了的“干净页”； 使用了的“脏页”。 而 InnoDB 的策略是尽量使用内存，因此对于长时间运行的库来说，未被使用的页面很少。当待读入的数据页在内存中不存在，而内存又不足时，缓冲池中将淘汰最久未使用数据页： 若淘汰页是“干净页”，则直接释放用来复用； 若淘汰页是“脏页”，则要先 flush 到磁盘，变成干净页后才能释放复用。 MySQL 认为系统“空闲”的时候进行 flush 场景描述 可以灵活设置 MySQL 定期 flush 的时间，以提高空闲或繁忙时 MySQL 刷“脏页”的效率。 性能影响 MySQL 空闲时的操作，对系统压力不大。 MySQL 正常关闭 场景描述 MySQL 需要正常关闭时，会把内存的脏页都 flush 到磁盘上，这样下次 MySQL 启动的时候，就可以直接从磁盘上读数据，启动速度会很快。 性能影响 MySQL 关闭前的操作，对性能无影响。 性能影响总结根据上述的第一、二个 flush 场景，对应易引发两种明显影响性能情况： 日志写满，更新全部堵住，写性能跌为 0。这种情况对敏感业务来说，是不能接受的； 一次查询要淘汰的“脏页”个数太多，导致查询响应时间明显变长。 因此，InnoDB 需要控制脏页比例的机制，以避免上述这两种情况。 InnoDB 控制脏页比例的机制 正确地设置 innodb_io_capacity 参数 正确地设置 innodb_io_capacity 参数能够告知 InnoDB 所在主机的 IO 能力，在需要全力刷脏页的时候尽最快的速度。 测试磁盘随机读写的命令： 1fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest innodb_io_capacity 的设置不当可能导致较好的硬件设备也产生脏页累计（刷脏页的速度比脏页生成还慢），影响查询和更新性能。 控制 redo log 写盘速度 参数 innodb_max_dirty_pages_pct 是脏页比例上限（默认值是 75%），InnoDB 会根据当前的脏页比例 M，算出一个范围在 0 到 100 之间的数字，计算公式记为 $F_1(N)$。 InnoDB 每次写入的日志都有一个序号，根据当前写入的序号跟 checkpoint 对应的序号之间的差值 N，算出一个范围在 0 到 100 之间的数字，计算公式记为 $F_2(N)$。$F_2(N)$ 算法比较复杂，其特点为：N 越大，$F_2(N)$ 的值越大。 根据上述算得的 $F_1(N)$ 和 $F_2(N)$ 两个值，取 R = max{$F_1(N)$, $F_2(N)$}，之后引擎就按照 v = innodb_io_capacity × R% 来控制刷脏页的速度。 控制脏页比例 平时应多关注脏页比例，不要让它经常接近 75%。 查看脏页比例： 123456-- 查看 Innodb_buffer_pool_pages_dirtyselect VARIABLE_VALUE into @a from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_dirty';-- 查看 Innodb_buffer_pool_pages_totalselect VARIABLE_VALUE into @b from global_status where VARIABLE_NAME = 'Innodb_buffer_pool_pages_total';-- 脏页比例 = Innodb_buffer_pool_pages_dirty / Innodb_buffer_pool_pages_totalselect @a/@b; MySQL 刷脏页的“连坐”机制描述对于每个数据页，如果跟它相邻的数据页和它一样也是脏页，就会被放到一起 flush，因此，一个脏页的 flush 可能肯能导致相邻脏页的连锁 flush，使查询更慢。 设置在 InnoDB 中，通过设置 innodb_flush_neighbors 参数来控制这个行为： 值为 1 时，开启“连坐”机制 适合用于使用机械硬盘的情况。 机械硬盘的随机 IOPS（Input/Output Operations Per Second，每秒的读写次数） 一般只有几百，“连坐”机制可以减少随机 IO，大幅提高系统性能。 值为 0 时，仅刷选中脏页自身 适合用于使用 IOPS 比较高的设备的情况，比如 SSD。 该场景下 IOPS 往往不是瓶颈，仅刷选中脏页能够更快地执行完必要的刷脏页操作，减少 SQL 语句响应时间。 数据表删掉一半，但表文件大小不变skip。 COUNT(*) 慢count(*) 的实现方式在没有使用 where 时： MyISAM 引擎：将表的总行数存在磁盘上，执行 count(*) 时会直接返回 InnoDB 引擎：把数据逐行读出，然后累积计数。 此时，InnoDB 使用 COUNT(*) 慢于 MyISAM。而使用 where 时，二者都需要逐行读出数据并进行条件过滤。 InnoDB 不存储表行数的原因即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的。 TABLE_ROWSshow table status 命令中的 TABLE_ROWS 是根据采样估计得来的，误差可能达到 40% 到 50%，并不准确，所以不能直接使用。 快速获取记录总数的方案用缓存系统保存计数用一个 Redis 服务来保存这个表的总行数。这个表每被插入一行 Redis 计数就加 1，每被删除一行 Redis 计数就减 1。读写效率快，但要注意以下两个问题： 丢失更新 可能情况：数据库更新了一条数据，Redis 在内存中（未永久化）计数 +1 后，发生了异常重启，+1 操作丢失。 解决方案：异常重启后，到数据库中单独执行依次 COUNT(*) 再写入 Redis。鉴于异常重启是小概率事件，该操作成本可以接受。 逻辑不精确 可能情况： 假设有会话 A 和会话 B。A 中的操作有：① Redis 计数 +1;② 插入一行数据。B中的操作有：③ 读 Redis 计数，取最近 100 条记录。当执行顺序为 ①③② 或 ①③② 时，操作是数据不一致的。 在并发系统里面，我们无法精确控制不同线程的执行时刻，因为此可能存在以上数据不一致的操作序列。所以，即使 Redis 正常工作，这个计数值还是逻辑上不精确的。 解决方案：无。 在数据库保存计数将计数直接放到数据库里单独的一张计数表 C 中。 丢失更新 由于 InnoDB 支持崩溃恢复，所以可以解决丢失更新的问题。 逻辑不精确 利用事务特性，实现一致性读。 不同 count 用法count() 的语义count() 是一个聚合函数，对于返回的结果集，一行行地判断，如果 count 函数的参数不是 NULL，累计值就加 1，否则不加。最后返回累计值。故 count(*)、count(主键 id) 和 count(1)，表示返回满足条件的结果集的总行数； count(字段)，表示返回字段不为 NULL 的结果集的总行数 性能差异原则： server 层要什么就给什么； InnoDB 只给必要的值； 优化器只优化了 count(*) 的语义为“取行数”，其他“显而易见”的优化并没有做。 各用法差别如下： count(主键 id)：取 id。 InnoDB 引擎会遍历整张表，把每一行的 id 值都取出来，返回给 server 层。server 层拿到 id 后，判断不可能为空（主键不能为空），按行累加。 count(1)：不取值，只置“1”。 InnoDB 引擎遍历整张表，但不取值。server 层对于返回的每一行，放一个数字“1”进去，判断不可能为空，按行累加。 与 count(主键 id) 相比，count(1) 执行得要比 count(主键 id) 快。因为从引擎返回 id 会涉及到解析数据行，以及拷贝字段值的操作。 count(字段)：取值，若类型定义为 not null 直接累加，否则逐行判段不为 null 才累加。 如果这个“字段”是定义为 not null 的话，一行行地从记录里面读出这个字段，判断不可能为 null，按行累加； 如果这个“字段”定义允许为 null，那么执行的时候，判断到有可能是 null，还要把值取出来再判断一下，不是 null 才累加。即第一条原则，server 层要什么字段，InnoDB 就返回什么字段。 count(*)：不取值，直接累加。 count(*) 不会把全部字段取出来，而是专门做了优化，不取值。count(*) 肯定不是 null，按行累加。 按效率排序，count(字段) &lt; count(主键 id) &lt; count(1) ≈ count(*)，因此建议尽量使用 count(*)。 order by 工作原理全字段排序全字段排序执行流程以下方 SQL 为例，其中，id 是主键，a 是普通索引 1select a, b, c from T where a=1 order by b limit N; 初始化 sort_buffer，确定放入 a、b、c 这三个字段； 从索引 a 找到第一个满足 a=1 条件的主键 id； 到主键 id 索引取出整行，取 a、b、c 三个字段的值，存入 sort_buffer 中； 从索引 a 取下一个记录的主键 id； 重复步骤 3、4 直到 a 的值不满足查询条件为止，对应的主键 id； 对 sort_buffer 中的数据按照字段 b 做快速排序； 按照排序结果取前 N 行返回给客户端。 sort_bufferMySQL 会给每个线程分配一块内存用于排序，称为 sort_buffer。sort_buffer 中做快速排序，可能在内存中完成，也可能需要使用外部排序，这取决于排序所需的内存和参数 sort_buffer_size。 sort_buffer_size sort_buffer_size 是 MySQL 为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量小于 sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序。 查看一个排序语句是否使用了临时文件的方法如下： 12345678910111213141516171819/* 打开optimizer_trace，只对本线程有效 */SET optimizer_trace='enabled=on'; /* @a保存Innodb_rows_read的初始值 */select VARIABLE_VALUE into @a from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 执行排序语句 */select a, b, c from T where a=1 order by b limit N;/* 查看 OPTIMIZER_TRACE 输出 */SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`\G/* @b保存Innodb_rows_read的当前值 */select VARIABLE_VALUE into @b from performance_schema.session_status where variable_name = 'Innodb_rows_read';/* 计算Innodb_rows_read差值 */select @b-@a;-- 通过查看 OPTIMIZER_TRACE 的结果中的 number_of_tmp_files，可以判断是否使用了临时文件 number_of_tmp_files number_of_tmp_files = N（N &gt; 0），表示排序过程中使用的临时文件数为 N。外部排序一般使用归并排序算法，将待排数据分成 N 份做 N 路归并排序。sort_buffer_size 越小，需要分成的份数越多，number_of_tmp_files 的值就越大。 number_of_tmp_files = 0，表示 sort_buffer_size 大于待排数据的大小，排序可以直接在内存中完成。 缺点全字段排序只需读取一遍原表数据，剩下的操作都是在 sort_buffer 和临时文件中执行的。在查询需要返回很多个字段时，sort_buffer 中要放的字段数太多，会导致内存里能够同时放下的行数会很少，需要分成很多个临时文件，排序的性能会很差。 rowid 排序max_length_for_sort_datamax_length_for_sort_data，是 MySQL 中专门控制用于排序的行数据的长度的一个参数。若单行的长度超过这个值，MySQL 就认为单行太大，会从全字段排序算法切换成rowid 排序算法。 采用 rowid 排序时，放入 sort_buffer 的字段只有需要排序的列和主键 id。 rowid 排序执行流程以下方 SQL 为例，其中，id 是主键，a 是普通索引 1select a, b, c from T where a=1 order by b limit N; 初始化 sort_buffer，确定放入两个字段，即 b 和 id； 从索引 a 找到第一个满足 a=1 条件的主键 id； 到主键 id 索引取出整行，取 b、id 这两个字段，存入 sort_buffer 中； 从索引 a 取下一个记录的主键 id； 重复步骤 3、4 直到不满足 a=1 条件为止； 对 sort_buffer 中的数据按照字段 b 进行排序； 遍历排序结果，取前 N 行，并按照 id 的值回到原表中取出 a、b 和 c 三个字段返回给客户端。 与全字段排序区别rowid 排序相比全自担排序，在取最后结果集时多出一步：访问主键索引，以取出其他字段数据。 MySQL 排序算法的选择思路MySQL 的一个设计思想：如果内存够，就要多利用内存，尽量减少磁盘访问。故对于 InnoDB 表来说，rowid 排序会要求回表多造成磁盘读，因此不会被优先选择。 对排序算法的优化并非所有的 order by 语句都需要排序操作的，若从表中取出的数据行天然有序，则无需排序。 联合索引以下方 SQL 为例，其中，id 是主键 1select a, b, c from T where a=1 order by b limit N; 创建 (a, b) 的联合索引，由于 (a, b) 这个0联合索引本身有序，那么查询过程中，只要 a=1，b 的值一定有序。 此时，排序执行过程如下： 从索引 (a, b) 找到第一个满足 a=1 条件的主键 id； 到主键 id 索引取出整行，取 a、b、c 三个字段的值，作为结果集的一部分直接返回； 从索引 (a, b) 取下一个记录主键 id； 重复步骤 2、3，直到查到第 N 条记录，或者是不满足 a=1 条件时循环结束。 该查询过程不需要临时表，也不需要排序。 联合索引 + 覆盖索引以下方 SQL 为例，其中，id 是主键 1select a, b, c from T where a=1 order by b limit N; 创建 (a, b， c) 的联合索引，由于 (a, b， c) 这个联合索引本身有序，且索引树上已具备查询所需的所有字段。 此时，排序执行过程如下： 从索引 (a, b， c) 找到第一个满足 a=1 条件的记录，取出其中的 a, b， c 这三个字段的值，作为结果集的一部分直接返回； 从索引 (a, b， c) 取下一个记录，同样取出这三个字段的值，作为结果集的一部分直接返回； 重复执行步骤 2，直到查到第 N 条记录，或者是不满足 a=1 条件时循环结束。 该查询过程不需要临时表，不需要排序，也不需要回主表取出其它字段数据，直接从索引 (a, b， c) 上取出数据行作结果集。 优点： 联合索引：本身有序。 覆盖索引：索引上的信息足够满足查询请求，则不需要再回到主键索引上去取数据。 缺点： 索引维护有代价，使用需适当。 随机消息的显示skip。 SQL 语句逻辑相同，而性能差异巨大条件字段函数操作的影响关键对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃走树搜索功能。 示例隐式类型转换的影响隐式类型转换的规则 判断方法 1select “10” &gt; 9； 若结果为 1，表明做的是数字比较，规则为“将字符串转成数字” 若结果为 0，表明做的是字符串比较，规则为“将数字转成字符串” 通过验证得知，MySQL 的转换规则为“字符串转换成数字” 数据类型转换导致全索引扫描的原因隐式数据类型转换，本质上相当于对索引字段使用了数据类型转换函数，影响与条件字段函数操作的影响相同。 隐式字符编码转换自动类型转换原则在程序设计语言里面，做自动类型转换的时候，为了避免数据在转换过程中由于截断导致数据错误，也都是“按数据长度增加的方向”进行转换的。 隐式字符编码转换导致全索引扫描的原因隐式字符编码转换，相当于对索引字段使用了字符编码转换函数，影响与条件字段函数操作的影响相同。 优化思路 修改字段的类型/字符集，避免类型/字符集转换； 修改 SQL 语句，将转换函数加在输入参数（右值）上，避免破坏字段有序性; SQL 语句中，尽量不要在字段上做任何操作，以免破坏索引。避免类似写法： 12345SELECT * FROM T WHERE id + 1 = 1000;-- 该 where 条件的写法，将导致无法使用 id 的索引查找，MySQL 也不会主动重写这个语句SELECT * FROM T WHERE id = 1000 - 1;-- 正确做法 只查一行，执行慢查询长时间不返回执行查询语句后，长时间等待没有返回结果，大概率是表被锁住了，可执行 show processlist 命令，查看当前语句处于什么状态。 等 MDL 锁 现象 若执行 show processlist 命令， State 为 “Waiting for table metadata lock”，表示有一个线程正在表上请求或者持有 MDL 写锁，把查询语句堵住了。 处理方法 找到持有 MDL 锁的进程，将其 kill 掉 MySQL 启动时设置 performance_schema=on（相比于设置为 off 会有 10% 左右的性能损失） 查询 sys.schema_table_lock_waits 这张表，找到阻塞进程 1SELECT blocking_id FROM sys.schema_table_lock_waits; kill 阻塞进程 等 flush MySQL 里面对表做 flush 操作的用法:1234-- 用法一：指定表 t 的话，表示只关闭表 tflush tables t with read lock;-- 用法二：没有指定具体的表名，表示关闭 MySQL 里所有打开的表flush tables with read lock; 正常情况下，两种用法执行起来都很快，除非它们被别的线程堵住了。 现象 1select * from information_schema.processlist where id=&lt;pid&gt;; 线程的状态是 “Waiting for table flush”。 处理方法 show processlist 找到阻塞 flush 线程的线程； kill 等行锁 处理方法 通过 sys.innodb_lock_waits 表找谁占着写锁（blocking_pid） 1select * from t sys.innodb_lock_waits where locked_table=`'test'.'t'`\G KILL ，直接断开这个连接。 连接被断开的时候，会自动回滚这个连接里面正在执行的线程，也就释放了行锁。 若使用 KILL QUERY ，只是停止 blocking_pid 线程当前正在执行的语句，而占有行锁的是 update 语句，这个语句已经是之前执行完成了的，此时执行 KILL QUERY 无法让这个事务去掉行锁。 查询慢坏查询不一定是慢查询。可能在数据量大起来之后，执行时间才开始快速上涨。 一致性读事务执行过程中，查询某行数据遇锁（其他事物频繁执行更新操作，生产大量 redo log），当锁释放后，为保持一致性读，进行大量计算找到对应数据版本。 幻读只改一行，锁很多“饮鸩止渴” 提高性能的方法MySQL 保持数据不丢binlog 的写入机制过程 事务执行过程中，先把日志写到 binlog cache 事务提交的时候，再把 binlog cache 写到 binlog 文件中，并清空 binlog cache redo log 的写入机制总结WAL 机制主要得益于 redo log 和 binlog 都是顺序写，磁盘的顺序写比随机写速度要快 组提交机制，可以大幅度降低磁盘的 IOPS 消耗 MySQL 出现 IO 性能瓶颈的提升方法 设置 binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，减少 binlog 的写盘次数。 风险：此法基于“额外的故意等待”实现，可能会增加语句的响应时间，但没有丢失数据的风险。 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）。 风险：主机掉电时会丢 binlog 日志。 将 innodb_flush_log_at_trx_commit 设置为 2。 风险：主机掉电的时候会丢数据。 不建议 innodb_flush_log_at_trx_commit 设置成 0。因为把这个参数设置成 0，表示 redo log 只保存在内存中，这样的话 MySQL 本身异常重启也会丢数据，风险太大。而 redo log 写到文件系统的 page cache 的速度也是很快的，所以将这个参数设置成 2 跟设置成 0 其实性能差不多，但这样做 MySQL 异常重启时就不会丢数据了，相比之下风险会更小。 数据库的 crash-safe 如果客户端收到事务成功的消息，事务就一定持久化了 如果客户端收到事务失败（比如主键冲突、回滚等）的消息，事务就一定失败了 如果客户端收到“执行异常”的消息，应用需要重连后通过查询当前状态来继续后续的逻辑。此时数据库只需要保证内部（数据和日志之间，主库和备库之间）一致就可以了 MySQL 保持主备一致主备切换的基本原理假设 A 为主库，B 为备库。 未切换前，客户端的读写都直接访问 A，而备库 B 只是将 A 的更新操作都同步到本地并执行，保证了 A 和 B 的数据相同。 切换后，客户端读写直接访问 B，此时 A 做备库并同步 B 的操作在本地执行。 [注]建议将备库设置为 readonly 模式，即使备库并没有被直接访问。因为： 备库上可能会执行一些运营类的查询语句，设置只读可避免误操作 防止切换逻辑 bug，比如切换过程中出现双写，导致主备不一致 以是否 readonly 状态判断数据库的主/从角色 主备流程M-S 结构：备库 B 跟主库 A 之间维持了一个长连接。主库 A 内部有一个线程，专门用于服务备库 B 的这个长连接。 一个事务日志同步的完整过程是这样的： 在备库 B 上通过 change master 命令，设置主库 A 的 IP、端口、用户名、密码，以及要从哪个位置开始请求 binlog，这个位置包含文件名和日志偏移量； 在备库 B 上执行 start slave 命令，这时候备库会启动两个线程： io_thread（与主库建立连接） 和 sql_thread； 主库 A 校验完用户名、密码后，开始按照备库 B 传过来的位置，从本地读取 binlog 发送给 B； 备库 B 拿到 binlog 后，写到本地文件，称为中转日志（relay log）； sql_thread 读取中转日志，解析出日志里的命令，并执行。 binlog 三种格式分类 statement 当 binlog_format=’statement‘ 时，binlog 里面记录的就是 SQL 语句的原文，包括注释。 由于 statement 格式下，记录到 binlog 里的是语句原文，因此可能会出现这样一种情况：在主库执行这条 SQL 语句的时候，用的是索引 a；而在备库执行这条 SQL 语句的时候，却使用了索引 b。因此，MySQL 认为这样写是有风险的。 row 当 binlog_format=‘row’ 时，binlog 不会 SQL 原文而是记录 event（包含：记录更新表的 Table_map event、记录更新事务的 event）。 更新事务的 event 中记录了主库中真实更新的主键 id，所以备库不会有选错索引导致主备更新不同行的问题。 mixed（statement 和 row 的混合） 存在原因： 有些 statement 格式的 binlog 可能会导致主备不一致，所以要使用 row 格式； row 格式很占空间； 比如你用一个 delete 语句删掉 10 万行数据，用 statement 的话就是一个 SQL 语句被记录到 binlog 中，占用几十个字节的空间。但如果用 row 格式的 binlog，就要把这 10 万条记录都写到 binlog 中。一方面，占用了更大的空间；另一方面，写 binlog 需要耗费大量 IO 资源，影响执行速度。 mixed 格式是一种折中方案：MySQL 自行对 SQL 语句进行判断，若可能引起主备不一致，就用 row 格式；否则，就用 statement 格式。 故线上 MySQL 设置的 binlog 格式至少应该为 ‘mixed’，既避免了 ‘statement’ 数据不一致的风险，又在不需要 ‘row’ 模式时提高空间和时间效率。 row 格式越来越常见的原因便于数据恢复。例如： 执行 delete 语句 row 格式的 binlog 会把被删掉的行的整行信息保存起来。若发现误删，直接将 binlog 中记录的 delete 语句转成 insert，把被错删的数据插入回去即可恢复。 执行 insert 语句 row 格式的 binlog 会记录所有插入的字段信息。若发现误插入，直接将 binlog 中记录的 insert 语句转成 delete，把插入的数据删除即可。 执行 update 语句 row 格式的 binlog 会记录修改前整行的数据和修改后的整行数据。若误更新，只需要将更新 event 中前后的两行数据对调一下，再去数据库里面执行即可恢复。 binlog 恢复数据的标准做法用 mysqlbinlog 工具解析出来，然后把解析结果整个发给 MySQL 执行。 12-- 将 &lt;binlog 文件&gt; 里面从 &lt;binlog 起始位置&gt; 字节到 &lt;binlog 结束位置&gt; 字节中间的内容解析出来，放到 MySQL 去执行mysqlbinlog --start-position=&lt;binlog 起始位置&gt; --stop-position=&lt;binlog 结束位置&gt; | mysql -h127.0.0.1 -P13000 -u$user -p$pwd; 循环复制实际生产上使用比较多的是双 M 结构：A 和 B 互为主备关系，切换主备时就不用再修改主备关系。但互为主备关系，则可能发生双方互相发送更新生成 binlog 的循环复制问题。 解决循环复制 规定每个库的 server id 必须不同，若相同则不能设定为主备关系； 备库接到 binlog 并在重放的过程中，生成与原 binlog 的 server id 相同的新的 binlog； 每个库在收到从自己的主库发过来的日志后，先判断 server id，如果跟自己的相同，表示这个日志是自己生成的，就直接丢弃这个日志。 日志的执行流程 A 的 binlog 中更新事务记录标注着 A 的 server id； A 的 binlog 传到 B 执行一次后，B 生成的 binlog 的 server id 是 A 的 server id，再传回 A； A 判断收到的 binlog 的 server id 与 自身相同，弃之。 MySQL 保持高可用skip。 备库延迟skip。 主库出错，备库如何操作skip。 读写分离的坑skip。 判断数据库是否出问题skip。 数据误删误删类型使用 delete 语句误删数据行Flashback：通过闪回恢复数据的工具 原理 修改 binlog 的内容，拿回原库重放。 使用前提 确保 binlog_format=row 和 binlog_row_image=FULL。 恢复操作 对于单个事务： 对于 insert 语句，对应的 binlog event 类型是 Write_rows event，把它改成 Delete_rows event 即可； 对于 delete 语句，也是将 Delete_rows event 改为 Write_rows event； 对于 Update_rows，binlog 里面记录了数据行修改前和修改后的值，对调这两行的位置即可。 对于多个事务： Flashback 解析 binlog 后，需要将事务的顺序逆转过来再执行。 注意事项 不建议直接在主库上执行恢复操作。 恢复数据比较安全的做法： 恢复出一个备份，确认过再恢复回主库； 找一个从库作为临时库，在这个临时库上执行这些操作，然后再将确认过的临时库的数据，恢复回主库。 原因：一个在执行线上逻辑的主库，数据状态的变更往往是有关联的。可能由于发现数据问题的时间晚了一点儿，就导致已经在之前误操作的基础上，业务代码逻辑又继续修改了其他数据。所以，如果这时候单独恢复这几行数据，而又未经确认的话，就可能会出现对数据的二次破坏。 事前预防误删的措施： 把 sql_safe_updates 参数设置为 on。 此时，如果忘记在 delete / update 语句中写 where 条件，或者 where 条件里面没有包含索引字段，这条语句的执行就会报错； 该设置下进行全表删除： 在 delete 语句中加上 where 条件，比如 where 1=1； 注：delete 全表效率很低，需要生成回滚日志、写 redo、写 binlog。 从性能角度考虑，全表删除应优先考虑使用 truncate table 或者 drop table 命令，但无法通过 Flashback 来恢复。 因为，即使配置了 binlog_format=row，执行 truncate/drop 时，记录的 binlog 还是 statement 格式。binlog 里面就只有一个 truncate/drop 语句，这些信息是恢复不出数据的。 代码上线前，必须经过 SQL 审计。 使用 drop table 或者 truncate table 语句误删数据表/数据库前提条件： 线上有定期的全量备份 实时备份 binlog 假如有人中午 12 点误删了一个库，恢复数据的流程如下： 取最近一次全量备份，假设这个库是一天一备，上次备份是当天 0 点； 用备份恢复出一个临时库； 从日志备份里面，取出凌晨 0 点之后的日志； 把这些日志，除了误删除数据的语句外，全部应用到临时库。 注意： 为了加速数据恢复，如果这个临时库上有多个数据库，可以在使用 mysqlbinlog 命令时，加上“–database”参数，用来指定误删表所在的库，以避免在恢复数据时还要应用其他库日志的情况。 在应用日志的时候，需要跳过 12 点误操作的那个语句的 binlog： 如果原实例没有使用 GTID 模式 只能在应用到包含 12 点的 binlog 文件的时候，先用“–stop-position”参数执行到误操作之前的日志，然后再用“–start-position”从误操作之后的日志继续执行； 如果实例使用了 GTID 模式 假设误操作命令的 GTID 是 gtid1，那么只需要执行 set gtid_next=gtid1;begin;commit; 先把这个 GTID 加到临时实例的 GTID 集合，之后按顺序执行 binlog 的时候，就会自动跳过误操作的语句。 mysqlbinlog 方法恢复数据不够快的主要原因有两个： 如果是误删表，最好就是只恢复出这张表，也就是只重放这张表的操作，但是 mysqlbinlog 工具并不能指定只解析一个表的日志； 用 mysqlbinlog 解析出日志应用，应用日志的过程就只能是单线程。 加速恢复数据： 并行复制 在用备份恢复出临时实例之后，将这个临时实例设置成线上备库的从库，这样： 在 slave``` 之前，先通过执行 ```change replication filter replicate_do_table 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195 2. 这样做也可以用上并行复制技术，来加速整个数据恢复过程。2. 延迟复制备库 一般的主备复制结构存在的问题是，如果主库上有个表被误删了，这个命令很快也会被发给所有从库，进而导致所有从库的数据表也都一起被误删了。 延迟复制的备库是一种特殊的备库，通过 ```CHANGE MASTER TO MASTER_DELAY = N``` 命令，可以指定这个备库持续保持跟主库有 N 秒的延迟。 比如你把 N 设置为 3600，这就代表了如果主库上有数据被误删了，并且在 1 小时内发现了这个误操作命令，这个命令就还没有在这个延迟复制的备库执行。这时候到这个备库上执行 ```stop slave```，再通过之前介绍的方法，跳过误操作命令，就可以恢复出需要的数据。这样的话，你就随时可以得到一个，只需要最多再追 1 小时，就可以恢复出数据的临时实例，也就缩短了整个数据恢复需要的时间。预防误删库 / 表的方法：1. 账号分离，避免写错命令 - 只给业务开发同学 DML 权限，而不给 truncate/drop 权限。在有 DDL 需求的时候，通过开发管理系统得到支持； - DBA 团队成员，日常也都规定只使用只读账号，必要的时候才使用有更新权限的账号。2. 制定操作规范，避免写错要删除的表名。 - 在删除数据表之前，必须先对表做改名操作。然后，观察一段时间，确保对业务无影响以后再删除这张表。 - 改表名的时候，要求给表名加固定的后缀（比如加 _to_be_deleted），然后删除表的动作必须通过管理系统执行。并且，管理系删除表的时候，只能删除固定后缀的表。#### 使用 rm 命令误删整个 MySQL 实例1. 对于一个有高可用机制的 MySQL 集群来说只要不是恶意地把整个集群删除，而只是删掉了其中某一个节点的数据的话，HA 系统（High Available，高可用性集群）就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。2. 避免 MySQL 集群挂掉：尽量将备份跨机房，最好是跨城市保存。## kill 不掉的语句### 两种 kill 命令- kill query + 线程 id 终止这个线程中正在执行的语句。- kill (connection) + 线程 id 断开这个线程的连接，当然如果这个线程有语句正在执行，也是要先停止正在执行的语句的。### kill 执行过程以 kill query thread_id_B 为例：1. 把 session B 的运行状态改成 THD::KILL_QUERY(将变量 killed 赋值为 THD::KILL_QUERY)；2. 给 session B 的执行线程发一个信号。 发送信号是因为：假设 session B 处于锁等待状态，如果只是把 session B 的线程状态设置 THD::KILL_QUERY，线程 B 无法知道这个状态变化，还是会继续等待。所以发送信号通知 session B 退出等待，来处理这个 THD::KILL_QUERY 状态。 表明： 1. 一个语句执行过程中有多处“埋点”，在这些“埋点”的地方判断线程状态，如果发现线程状态是 THD::KILL_QUERY，才开始进入语句终止逻辑； 2. 如果处于等待状态，必须是一个可以被唤醒的等待，否则根本不会执行到“埋点”处； 3. 语句从开始进入终止逻辑，到终止逻辑完全完成，是有一个过程的。### kill 失效#### kill 失效现象使用了 kill 命令，却没能断开这个连接。再执行 show processlist 命令，看到这条语句的 Command 列显示的是 Killed。show processlist 的特别逻辑：如果一个线程的状态是KILL_CONNECTION，就把Command列显示成Killed。#### 情况一：线程没有执行到判断线程状态的逻辑#### 情况二：终止逻辑耗时较长常见场景：1. 超大事务执行期间被 kill 此时，回滚操作需要对事务执行期间生成的所有新数据版本做回收操作，耗时很长。2. 大查询回滚 如果查询过程中生成了比较大的临时文件，加上此时文件系统压力大，删除临时文件可能需要等待 IO 资源，导致耗时较长。3. DDL 命令执行到最后阶段被 kill 此时，系统需要删除中间过程的临时文件，也可能受 IO 资源影响耗时较久。### 关于客户端的误解#### “Ctrl+C” 可以直接终止线程 -- X正解：“Ctrl+C” 不能直接终止线程。原因：1. 客户端的操作只能操作到客户端的线程，客户端和服务端只能通过网络交互，是不可能直接操作服务端线程的。2. 由于 MySQL 是停等协议，所以当前线程执行的语句还没有返回的时候，再往这个连接里面继续发命令也是没有用的。实际上，执行 “Ctrl+C” 时 MySQL 客户端会另外启动一个连接，然后发送一个 kill query 命令。#### 库里面的表越多，连接越慢 -- X正解：本地客户端执行操作慢，而非连接慢或服务端慢。原因：1. 前提 客户端与服务端建立连接的涉及操作只有： TCP 握手、用户校验、获取权限。与库里的表个数无关。2. 根源 当使用**默认参数连接**的时候，MySQL 客户端会提供一个本地库名和表名补全的功能（Tab 键自动补全表名或者显示提示）。为了实现这个功能，客户端在连接成功后，需要多做一些操作： 1. 执行 show databases； 2. 切到 db1 库，执行 show tables； 3. 把这两个命令的结果用于构建一个本地的哈希表。（此过程耗时较长，尤其在表数很多的时候） 自动补全关闭： 1. 在连接命令中加上 “-A”，就可以关掉这个自动补全的功能，然后客户端就可以快速返回了； 2. 在连接命令中加上 “–quick”(或 “-q”) 参数，也可以跳过这个阶段。#### “–quick” 是一个让服务端加速的参数 -- X正解：“–quick” 只能加快客户端响应，反而可能降低服务端的性能。原因：使用 “–quick” 参数，客户端会使用不缓存的方式。此时，如果客户端本地处理缓慢，就会导致服务端的发送结果被阻塞，让服务端变慢。“–quick” 效果：1. 跳过表名自动补全功能；2. mysql_store_result 需要申请本地内存来缓存查询结果，如果查询结果太大，会耗费较多的本地内存，可能会影响客户端本地机器的性能；3. 不会把执行命令记录到本地的命令历史文件。可见，“–quick” 是让客户端变得更快。MySQL 客户端接收服务端返回结果的方式有两种：1. 本地缓存 在本地开一片内存，先把结果存起来。如果你用 API 开发，对应的就是 mysql_store_result 方法。2. 不缓存 读一个处理一个。如果你用 API 开发，对应的就是 mysql_use_result 方法。## 查大量数据的内存问题### 全表扫描对 server 层的影响#### MySQL S/C 收发数据流程MySQL 是**边读边发的**，而非服务端保存一个完整的结果再集一次性发送。如果客户端接收得慢，会导致 MySQL 服务端由于结果发不出去，使事务的执行时间变长。取数据和发数据的流程：1. 获取一行，写到 net_buffer 中。这块内存的大小是由参数 net_buffer_length 定义的，默认是 16k；2. 重复获取行，直到 net_buffer 写满，调用网络接口发出去；3. 如果发送成功，就清空 net_buffer，然后继续取下一行，并写入 net_buffer；4. 如果发送函数返回 EAGAIN 或 WSAEWOULDBLOCK，就表示本地网络栈（socket send buffer）写满了，进入等待。直到网络栈重新可写，再继续发送。可见：1. 一个查询在发送过程中，占用的 MySQL 内部的内存最大就是 net_buffer_length 这么大，并不会达到 200G；2. socket send buffer 也不可能达到 200G（默认定义 /proc/sys/net/core/wmem_default），如果 socket send buffer 被写满，就会暂停读数据的流程。#### 线程 &quot;sending&quot; 状态线程状态：- &quot;Sending to client&quot; 服务器端的网络栈已写满，线程处于“等待客户端接收结果”的状态。 如果客户端使用–quick 参数，会使用 mysql_use_result 方法：读一行处理一行。假设客户端在遇上逻辑比较复杂的业务处理较慢，就容易产生该状态。- “Sending data” 线程“正在执行”。并不一定是指“正在发送数据”，而可能是处于执行器过程中的任意阶段。 查询语句的状态变化： 1. MySQL 查询语句进入执行阶段后，首先把状态设置成“Sending data”； 2. 然后，发送执行结果的列相关的信息（meta data) 给客户端； 3. 再继续执行语句的流程； 4. 执行完成后，把状态设置成空字符串。故对于正常的线上业务来说，当一个查询的返回结果：- 不会很多 建议使用 mysql_store_result 这个接口，直接把查询结果保存到本地内存。- 很多 建议使用 mysql_use_result 接口。同时，优化查询结果，并评估这么多的返回结果是否合理。### 全表扫描对 InnoDB 的影响InnoDB 引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于 InnoDB 对 LRU 算法做了改进，冷数据的全表扫描，对 Buffer Pool 的影响也能做到可控。#### Buffer Pool作用：- 保存更新结果，配合 redo log，避免随机写盘- 缓存数据页，加速查询内存命中率：- Buffer Pool 对查询的加速效果的指标- 查看 ```SQL show engine innodb status; -- Buffer pool hit rate 即当前的命中率 innodb_buffer_pool_size： 设置 Buffer Pool 的大小 建议大小：物理内存的 60%~80%（内存用尽原则） InnoDB 内存管理 最近最少使用 (Least Recently Used, LRU) 算法：淘汰最久未使用的数据 链表大小固定，表头是最近刚刚被访问过的数据页； 最新请求的数据页会被移至表头； 请求数据页不在链表中，向 BP 申请新数据页移至表头。此时若链表已满，表尾会被情况并放入新数据页内容，再移至表头。 若一次性读取一个较大且久未访问的数据表 当前 BP 中的数据会被全部淘汰掉，导致 BP 的内存命中率急剧下降，磁盘压力增加，SQL 语句响应变慢，对正在做业务服务的库效率影响很大。 为避免该情况，InnoDB 对 LRU 算法做了改进。 InnoDB 改进后的 LRU 算法： 按照 5:3 的比例把整个 LRU 链表分成了 young 区域和 old 区域，LRU_old 指向 old 区域的第一个位置，即整个链表的 5/8 处。 执行过程： 请求访问数据页在 young 区域，将数据页移至 young 区域头部，即表头； 请求的访问数据页不在当前整个链表中，淘汰表尾数据页，新数据页放至 LRU_old 处； old 区域中的每个数据页，每次被访问的时候都要做下面这个判断： 若其在 LRU 链表中存在的时间 &gt; 1 秒，将其移动到链表头部； 如其在 LRU 链表中存在的时间 &lt; 1 秒，位置保持不变。 1 秒这个时间，由参数 innodb_old_blocks_time 控制，其默认值是 1000，单位毫秒。 改进后的 LRU 算法，既使用了 BP，又使 young 区域中的常用数据页不受大规模历史数据查询的影响，保证了 BP 响应正常业务的查询命中率。 查询大量数据的常用做法一次性取 好处：对服务端只全表，只扫描一遍； 坏处：可能会出现大事务。 建议做法分批次取，然后每一批拿到最大的一个id（主键值），下一批查询的时候用 where Id &gt; N。 join 的使用和优化Index Nested-Loop JoinNLJ 执行过程以下方 SQL 为例，t1 和 t2 表结构相同，都有一个主键索引 id 和一个索引 a： 123select * from t1 straight_join t2 on (t1.a=t2.a);-- straight_join 让 MySQL 使用固定的连接方式执行查询，这样优化器只会按照我们指定的方式去 join：在该语句中，t1 是驱动表，t2 是被驱动表。-- 直接使用 join 语句，MySQL 优化器可能会选择表 t1 或 t2 作为驱动表 该 SQL 执行过程： 从表 t1 中读入一行数据 R； 从数据行 R 中，取出 a 字段到表 t2 里去（索引）查找； 取出表 t2 中满足条件的行，跟 R 组成一行，作为结果集的一部分； 重复执行步骤 1 到 3，直到表 t1 的末尾循环结束。 此过程与嵌套查询类似，并可以用上被驱动表的索引。故称 “Index Nested-Loop Join”，简称 NLJ。 NLJ 选择驱动表应该让小表来做驱动表。 假设被驱动表的行数是 M。每次在被驱动表查一行数据，要先搜索索引 a，再搜索主键索引。每次搜索一棵树近似复杂度是以 2 为底的 M 的对数，记为 $\log_2M$，所以在被驱动表上查一行的时间复杂度是 $2·\log_2M$。 假设驱动表的行数是 N，执行过程就要扫描驱动表 N 行，然后对于每一行，到被驱动表上匹配一次。 因此整个执行过程，近似复杂度是 $N + N·2·\log_2M$。可见，N 对扫描行数的影响更大，因此应该让小表来做驱动表。 Simple/Block Nested-Loop Join以下方 SQL 为例，t1 和 t2 表结构相同，都有一个主键索引 id 和一个索引 a，字段 b 上无索引： 1select * from t1 straight_join t2 on (t1.a=t2.b); 由于表 t2 的字段 b 上没有索引，因此再用图 2 的执行流程时，每次到 t2 去匹配的时候，就要做一次全表扫描。该算法被称作 “Simple Nested-Loop Join”。假设 t1 行数为 M，t2 行数为 N，那么总共需要扫描 M*N 行。 当 Simple Nested-Loop Join 中两个表的行数都非常大，称作 “Block Nested-Loop Join” 的算法，简称 BNL。 BNL 执行过程由于被驱动表上没有可用的索引： 把表 t1 的数据读入线程内存 join_buffer 中，由于语句中写的是 select ，因此是*把整个表 t1 放入了内存； 扫描表 t2，把表 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回。 与 Simple Nested-Loop Join 相比，二者时间复杂度相同，但 BNL 中的判断 M*N 次判断操作在内存中进行，速度会快很多。 join_bufferjoin_buffer 是一个无序数组，其大小是由参数 join_buffer_size 设定的，默认值是 256k。 若表 t1 很大，join_buffer 一次性放不下 t1 的所有数据，会将数据分段放。此时执行过程变成： 扫描表 t1，顺序读取数据行放入 join_buffer 中，待 join_buffer 满了，继续第 2 步； 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回； 清空 join_buffer； 继续扫描表 t1，顺序读取最后的 12 行数据放入 join_buffer 中，继续执行第 2 步。 BNL 选择驱动表假设驱动表的行数是 N，被驱动表的行数是 M N 小于 join_buffer 的大小： 两个表都做一次全表扫描，所以总的扫描行数是 M+N； 内存中的判断次数是 M*N。 此时，选择大表还是小表做驱动表，执行耗时是一样的。 N 大于 join_buffer 的大小，驱动表被分成 k 段。因为 N 越大 k 越大，所以 k 可表示为 μ * N，μ ∈ (0,1)： 大表扫描行数为 N，小表扫描行数为 μ N M，扫描总行数为 N + μ N M； 内存中判断的次数为 N * M。 可见： N 越小越好，故应选择小表做驱动表。 k（分段数）越小越好，所以 join_buffer_size 越大越好。 总结是否用 joinexplain 查询语句，查看 Extra 字段： NLJ：可以使用。 BNL：尽量不使用。扫描行数过多，太占用系统资源。 join 驱动表选择总是用“小表”： 小表 两个表按照各自的条件过滤，过滤完成之后，计算参与 join 的各个字段的总数据量，数据量小的那个表。 小表并非数据行数较少的那个表。 临时表与内存表的区别 内存表 使用 Memory 引擎的表创键。 表的数据都保存在内存里，系统重启的时候会被清空，但是表结构还在。 临时表 可以使用各种引擎（包括 Memory 引擎）类型创建。 使用 InnoDB 引擎或者 MyISAM 引擎的临时表，写数据的时候是写到磁盘上的。 临时表的特征 建表语法：create temporary table …。 一个临时表只能被创建它的 session 访问，对其他线程不可见。在session 结束的时候，会自动删除。 临时表可以与普通表同名。 session A 内有同名的临时表和普通表的时候，show create 语句，以及增删改查语句访问的是临时表。 show tables 命令不显示临时表。 临时表的应用临时表一般用于处理比较复杂的计算逻辑。由于临时表是每个线程自己可见的，所以不需要考虑多个线程执行同一个处理逻辑时，临时表的重名问题。在线程退出的时候，临时表也能自动删除，省去了收尾和异常处理的工作。 join 优化 不同 session 的临时表是可重名。 如果有多个 session 同时执行 join 优化，不需要担心表名重复导致建表失败的问题。 不需要担心数据删除问题。 如果使用普通表，在流程执行过程中客户端发生了异常断开，或者数据库发生异常重启，还需要专门来清理中间过程中生成的数据表。 而临时表由于会自动回收，所以不需要这个额外的操作。 分库分表 一般场景 把一个逻辑上的大表分散到不同的数据库实例上。 分库分表系统一般都有一个中间层 proxy。在这个架构中，分区 key 的选择是以“减少跨库和跨表查询”为依据的。如果大部分的语句都会包含 f 的等值条件，那么就要用 f 做分区键。这样，在 proxy 这一层解析完 SQL 语句以后，就能确定将这条语句路由到哪个分表做查询。 查询语句没有用到分区字段： 在 proxy 层的进程代码中实现排序。 这种方式的优势是处理速度快，拿到分库的数据以后，直接在内存中参与计算。 不过，这个方案的缺点也比较明显： 需要的开发工作量比较大。如果涉及到复杂的操作，比如 group by，甚至 join 这样的操作，对中间层的开发能力要求比较高； 对 proxy 端的压力比较大，尤其是很容易出现内存不够用和 CPU 瓶颈的问题。 各分库操作再汇总操作。 把各个分库拿到的数据，汇总到一个 MySQL 实例的一个临时表中，然后在这个汇总实例上做逻辑操作。由于分库操作每个分库的计算量往往不饱和，所以会直接把临时表放到某一个分库上。 临时表可重名原因物理原因使用 InnoDB 引擎创键表时，会创建一个 frm 文件保存表结构定义，还要有地方保存表数据。 frm 文件 存放在临时文件目录下，文件名的后缀是.frm，前缀是“#sql{进程 id}{线程 id} 序列号”。可以使用 select @@tmpdir 命令，来显示实例的临时文件目录。 表中数据的存放方式 在 5.6 以及之前的版本里，MySQL 会在临时文件目录下创建一个相同前缀、以 .ibd 为后缀的文件，用来存放数据文件； 5.7 版本开始，MySQL 引入了一个临时文件表空间，专门用来存放临时文件的数据。因此，我们就不需要再创建 ibd 文件了。 可见，不同 session 创键名为 t 的同名临时表，在存储上两者磁盘文件名是不同的。 table_def_key 原因MySQL 维护数据表，除了物理上要有文件外，内存里面也有一套机制区别不同的表，每个表都对应一个 table_def_key： 普通表的 table_def_key 由“库名 + 表名”得到的，所以如果你要在同一个库下创建两个同名的普通表，创建第二个表的过程中就会发现 table_def_key 已经存在了。 临时表 table_def_key 在“库名 + 表名”基础上，又加入了“server_id+thread_id”。 可见，不同 session 创键名为 t 的同名临时表，两者的 table_def_key 是不同的。 总结不同 session 创键名为 t 的同名临时表，由于它们的 table_def_key 不同，磁盘文件名也不同，因此可以并存。 临时表与主备复制bing log 记录临时表操作在实现上，每个线程都维护了自己的临时表链表。每次 session 内操作表时： 先遍历链表，检查是否有这个名字的临时表，如果有就优先操作临时表； 如果没有临时表，操作普通表； 在 session 结束的时候，对链表里的每个临时表，执行 “DROP TEMPORARY TABLE + 表名”操作。 为了备库需要，binlog 中会记录 “DROP TEMPORARY TABLE” 这条命令。如果不记录临时表的操作，当主库普通表中存在部分由临时表得来的数据，备库在执行时将无法获取这部分数据。因此，主库创建临时表的语句会传到备库执行，备库的同步线程就会创建这个临时表。主库在线程退出的时候，会自动删除临时表，但是备库同步线程是持续在运行的。所以，此时需要在主库上再写一个 DROP TEMPORARY TABLE 传给备库执行。 由于 binlog_format=row 时，binlog 中直接记录操作相关的数据，所以此时跟临时表有关的语句，就不会记录到 binlog 里；而当 binlog_format=statment/mixed 时，binlog 中会记录临时表的操作。 drop table 的改写drop table 命令可以一次删除多个表。比如，在上面的例子中，设置 binlog_format=row，如果主库上执行 “drop table t_normal, temp_t”这个命令，那么 binlog 中就只能记录：DROP TABLE t_normal / generated by server /因为备库上并没有表 temp_t，将这个命令重写后再传到备库执行，才不会导致备库同步线程停止。所以，drop table 命令记录 binlog 的时候，就必须对语句做改写。“/ generated by server /”说明了这是一个被服务端改写过的命令。 主库上不同线程的同名的临时表是没关系的如何传到备库执行MySQL 在记录 binlog 的时候，会把主库执行这个语句的线程 id 写到 binlog 中。这样，在备库的应用线程就能够知道执行每个语句的主库线程 id，并利用这个线程 id 来构造临时表的 table_def_key。 如主库 M 上的两个 session 创建了同名的临时表 t1，这两个 create temporary table t1 语句都会被传到备库 S 上： session A 的临时表 t1，在备库的 table_def_key 就是：库名 +t1+“M 的 serverid”+“session A 的 thread_id”; session B 的临时表 t1，在备库的 table_def_key 就是 ：库名 +t1+“M 的 serverid”+“session B 的 thread_id”。 由于 table_def_key 不同，这两个表在备库的应用线程里面不会冲突。 MySQL 使用内存临时表的时机UNION执行过程 创建一个内存临时表，该临时表中的字段 = 第一个子查询中的查询字段，且为主键字段； 执行第一个子查询，结果存入临时表； 执行第二个子查询，逐行取出结果试图插入临时表： 若该值已存在于临时表，违反唯一性约束，则插入失败，跳过； 若该值临时表中不存在，则插入； 从临时表中按行取出数据，返回结果，并删除临时表。 内存临时表大小 内存临时表的大小是有限制的，通过参数 tmp_table_size 控制，默认是 16M。 当内存临时表大小到达了上限，系统会将内存临时表转成磁盘临时表，磁盘临时表默认使用的引擎是 InnoDB。 GROUP BY执行过程 创建一个内存临时表，临时表字段 = 待查询字段，聚合键作为主键； 通过表索引找到所需要的数据，插入临时表； 对临时表数据进行排序，返回结果，并删除临时表； 无需排序的情况下，可通过 ORDER BY NULL 取消排序这一步。 优化：索引不论是使用内存临时表还是磁盘临时表，GROUP BY 逻辑都需要构造一个带唯一索引的内存临时表，目的时为了排序。 如果可以确保输入的数据是有序的，就可以拿到 GROUP BY 的结果，不需要临时表，也不需要再额外排序。故可以： MySQL 5.6 及之前的版本 对聚合键、待查数据创建索引。 MySQL 5.7 以后版本 使用 generated column 机制实现列数据的关联更新：创建一个新列，然后在新列上创建一个索引。 优化：直接排序如果遇到数据量很大，又不适合创建索引的场景，此时排序无法避免： 在内存临时表足够大小的情况下，应优先使用内存临时表； 但在数据量远大于内存临时表大小的情况下，可以绕过存入内存临时表这一环，直接走磁盘临时表效率更高。 以下方 SQL 为例： 1SELECT SQL_BIG_RESULT &lt;列名&gt;, ... FROM t GROUP BY ... 由于磁盘临时表是 B+ 树存储，存储效率不如数组，MySQL 优化器从磁盘空间考虑直接用数组存数据。执行过程如下： 初始化 sort_buffer，放入整型字段 = 聚合键字段； 通过扫描表 t 的索引依次取出目标结果存入 sort_buffer 中； 扫描完成后，对 sort_buffer 的字段做排序（如果 sort_buffer 内存不够用，就会利用磁盘临时文件辅助排序）； 排序完成后，就得到了一个有序数组。 总结：内粗临时表使用场景 SQL 执行过程无法即读即得目标结果，需要额外的内存来保存中间结果； join_buffer 是无序数组，sort_buffer 是有序数组，内存临时表是二维表结构。如果 SQL 执行逻辑需要用到二维表特性，就会优先考虑使用临时表。如：UNION、GROUP BY。 InnoDB 和 Memory 引擎skip。 自增主键为何不连续skip。 insert 语句的锁insert … selectinsert 循环写入 insert 唯一键冲突insert into … on duplicate key update快速复制一张表为了避免对源表加读锁，稳妥的方案是先将数据写到外部文本文件，然后再写回目标表。 使用 mysqldump使用 mysqldump 命令将数据导出成一组 INSERT 语句，再执行插入。 导出 INSERT 语句通过以下 mysqldump 命令生成包含 INSERT 语句的 t.sql 文件。 1mysqldump -h$host -P$port -u$user --add-locks=0 --no-create-info --single-transaction --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --result-file=/client_tmp/t.sql 参数： –single-transaction 在导出数据的时候不需要对表 db1.t 加表锁，而是使用 START TRANSACTION WITH CONSISTENT SNAPSHOT 的方法； –add-locks 设置为 0 表示在输出的文件结果里，不增加” LOCK TABLES t WRITE;” ； –no-create-info 不需要导出表结构； –set-gtid-purged 设置为 off 表示不输出跟 GTID 相关的信息； –result-file 指定输出文件的路径，其中 client 表示生成的文件是在客户端机器上的。 执行 INSERT 语句通过下面这条命令，执行 t.sql 文件中的 INSERT 语句。 1mysql -h127.0.0.1 -P13000 -uroot db2 -e &quot;source /client_tmp/t.sql&quot; 执行流程： 打 t.sql 开文件，默认以分号为结尾读取逐条 SQL 语句； 将 SQL 语句发送到服务端执行。 使用 csv导出为 .csv 文件MySQL 提供了下面的 SQL 语法，用来将查询结果导出成.csv 文件到服务端本地目录： 1SELECT * FROM t WHERE ... into outfile '/server_tmp/t.csv'; 注意事项： 该语句会将结果保存在服务端。 如果你执行命令的客户端和 MySQL 服务端不在同一个机器上，客户端机器的临时目录下是不会生成 t.csv 文件的。 into outfile 指定了文件的生成位置（/server_tmp/），这个位置必须受参数 secure_file_priv 的限制。参数 secure_file_priv 的可选值是： 如果设置为 empty，表示不限制文件生成的位置，这是不安全的设置； 如果设置为一个表示路径的字符串，就要求生成的文件只能放在这个指定的目录，或者它的子目录； 如果设置为 NULL，就表示禁止在这个 MySQL 实例上执行 select … into outfile 操作。 该语句不会帮你覆盖文件。 你需要确保 /server_tmp/t.csv 这个文件不存在，否则执行语句时就会因为有同名文件的存在而报错。 该语句生成的文本文件中，原则上一个数据行对应文本文件的一行。 如果字段中包含换行符，在生成的文本中也会有换行符。不过类似换行符、制表符这类符号，前面都会跟上“\”这个转义符，这样就可以跟字段之间、数据行之间的分隔符区分开。 将 .csv 文件导入目标表使用 load data 命令： 1load data infile &apos;/server_tmp/t.csv&apos; into table &lt;目标表&gt;; 执行过程： 打开文件 /server_tmp/t.csv，以制表符 (\t) 作为字段间的分隔符，以换行符（\n）作为记录之间的分隔符，进行数据读取； 启动事务。 判断每一行的字段数与表 &lt;目标表&gt; 是否相同： 若不相同，则直接报错，事务回滚； 若相同，则构造成一行，调用 InnoDB 引擎接口，写入到表中。 重复步骤 3，直到 /server_tmp/t.csv 整个文件读入完成，提交事务。 如果 binlog_format=statement，由于 /server_tmp/t.csv 文件只保存在主库所在的主机上，如果只是把 load data 语句原文写到 binlog 中，在备库执行的时候，备库的本地机器上没有这个文件，就会导致主备同步停止。所以，完整流程如下： 主库执行完成后，将 /server_tmp/t.csv 文件的内容直接写到 binlog 文件中； 往 binlog 文件中写入语句 load data local infile ‘/tmp/SQL_LOAD_MB-1-0’ INTO TABLE t； 把这个 binlog 日志传到备库； 备库的 apply 线程在执行这个事务日志时： 先将 binlog 中 t.csv 文件的内容读出来，写入到本地临时目录 /tmp/SQL_LOAD_MB-1-0 中； 再执行 load data 语句，往备库的 db2.t 表中插入跟主库相同的数据。 备库执行的 load data 语句里多了一个“local”，意为“将执行这条命令的客户端所在机器的本地文件 /tmp/SQL_LOAD_MB-1-0 的内容，加载到目标表 t 中”。 故，load data 命令有两种用法： 不加“local” 读取服务端的文件，这个文件必须在 secure_file_priv 指定的目录或子目录下； 加上“local” 读取的是客户端的文件，只要 mysql 客户端有访问这个文件的权限即可。这时候，MySQL 客户端会先把本地文件传给服务端，然后执行上述的 load data 流程。 select … into outfile优点：该方法是最灵活的，支持所有的 SQL 写法。 缺点:每次只能导出一张表的数据，不会生成表结构文件, 所以导数据时还需要单独的命令得到表结构定义。 mysqldump 提供了一个–tab 参数，可以同时导出表结构定义文件和 csv 数据文件。使用方法如下： 1mysqldump -h$host -P$port -u$user ---single-transaction --set-gtid-purged=OFF db1 t --where=&quot;a&gt;900&quot; --tab=$secure_file_priv 该命令会在 $secure_file_priv 定义的目录下，创建一个 t.sql 文件保存建表语句，同时创建一个 t.txt 文件保存 CSV 数据。 物理拷贝直接拷贝 .frm 和 .ibd此法行不通。 一个 InnoDB 表，除了包含这两个物理文件外，还需要在数据字典中注册。直接拷贝这两个文件的话，因为数据字典中还没有注册这个表，系统是不会识别和接受它们的。 可传输表空间MySQL 5.6 及之后的版本引入了可传输表空间(transportable tablespace) 的方法，可以通过导出 + 导入表空间的方式，实现物理拷贝表的功能。 假设我们现在的目标是在 db1 库下，复制一个跟表 t 相同的表 r，具体的执行步骤如下： 执行 create table r like t;，创建一个相同表结构的空表； 执行 alter table r discard tablespace;，这时候 r.ibd 文件会被删除； 执行 flush table t for export;，这时候 db1 目录下会生成一个 t.cfg 文件； 在 db1 目录下执行 cp t.cfg r.cfg;，cp t.ibd r.ibd;；这两个命令（这里需要注意的是，拷贝得到的两个文件，MySQL 进程要有读写权限）； 执行 unlock tables;，这时候 t.cfg 文件会被删除； 执行 alter table r import tablespace;，将这个 r.ibd 文件作为表 r 的新的表空间，由于这个文件的数据内容和 t.ibd 是相同的，所以表 r 中就有了和表 t 相同的数据。 至此，拷贝完毕。 注意事项： 在第 3 步执行完 flsuh table 命令之后，db1.t 整个表处于只读状态，直到执行 unlock tables 命令后才释放读锁； 在执行 import tablespace 的时候，为了让文件里的表空间 id 和数据字典中的一致，会修改 r.ibd 的表空间 id。而这个表空间 id 存在于每一个数据页中。因此，如果是一个很大的文件（比如 TB 级别），每个数据页都需要修改，所以你会看到这个 import 语句的执行是需要一些时间的。当然，如果是相比于逻辑导入的方法，import 语句的耗时是非常短的。 grant 后 flush privilege 的问题skip。 分区表的使用skip。 自增 id 用尽skip。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>collection</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL基础速览]]></title>
    <url>%2F2020%2F06%2F02%2Fsql%2F</url>
    <content type="text"><![CDATA[[Updated] 本文梳理了 SQL 相关基础知识（基于《SQL基础教程》）. SQL 基础 SQL 概要标准 SQL国际标准化组织（ISO）为 SQL 制定的相应标准，适用于各种 RDBMS SQL 语句及其种类由关键字、表名、列名等组合而成一条 SQL 语句描述操作的内容 DDL（Data Definition Language，数据定义语言）用来创建或者删除存储数据用的数据库以及数据库中的表等对象。包含指令： CREATE： 创建数据库和表等对象 DROP： 删除数据库和表等对象 ALTER： 修改数据库和表等对象的结构 DML（Data Manipulation Language，数据操纵语言）用来查询或者变更表中的记录。包含指令： SELECT：查询表中的数据 INSERT：向表中插入新数据 UPDATE：更新表中的数据 DELETE：删除表中的数据 DCL（Data Control Language，数据控制语言）用来确认或者取消对数据库中的数据进行的变更。除此之外，还可以对 RDBMS 的用户是否有权限操作数据库中的对象（数据库表等）进行设定。包含指令： COMMIT： 确认对数据库中的数据进行的变更 ROLLBACK： 取消对数据库中的数据进行的变更 GRANT： 赋予用户操作权限 REVOKE： 取消用户的操作权限 SQL 的基本书写规则 以分号（；）结尾 SQL 不区分关键字大小写；表中数据区分大小写 SQL 一般书写原则： 关键字大写 表名首字母大写 其余（列名）小写 SQL 子句的顺序不能改变，也不能互相替换 表的创建数据库的创建（CREATE DATABASE语句）1CREATE DATABASE &lt;数据库名&gt;; 表的创建（CREATE TABLE语句）123456CREATE TABLE &lt;表名&gt;(&lt;列名1&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;, &lt;列名2&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;, …… &lt;列名n&gt; &lt;数据类型&gt; &lt;该列所需约束&gt;, &lt;该表约束1&gt;, &lt;该表约束2&gt;, ……); NOT NULL 约束只能以列为单位进行设置 列的数据类型必须指定 列的约束可以定义时设置，也可以语句末尾设置 命名规则 数据库/表/列名：半角英文字母、数字、下划线(_) 标准 SQL 中名称必须以半角英文字母开头 名称不能重复 数据类型的指定 INTEGER：整数 CHAR：定长字符串 括号中指定可存储字符串长度 超出部分无法输入到列中 不足部分由半角空格进行补足。如：char(10)存”123”，存储字符占10个字节。取数据的时候，char类型的要用trim()去掉多余的空格 比 VARCHAR 效率高，空间换时间 VARCHAR：可变长字符串 括号指定最大长度 不足部分不会使用半角空格进行补足，会自动裁剪。如：varchar(10)存”123”，存储字符占3个字节 比 CHAR 节省空间 DATE：日期 含年/月/日 Oracle 中 DATE 型还包含时分秒 约束的设置 键：指定特定数据时使用的列的组合 主键（primary key）：可以唯一确定一行数据的列（故无法重复） 表的删除和更新表的删除（DROP TABLE语句）1DROP TABLE &lt;表名&gt;; 删除表无法回复，只能重建 避免到需要恢复数据的场景 表定义的更新（ALTER TABLE语句）添加列 1ALTER TABLE &lt;表名&gt; ADD COLUMN &lt;列的定义&gt;; 删除列 1ALTER TABLE &lt;表名&gt; DROP COLUMN &lt;列名&gt;; 表定义变更（ALTER TABLE）之后无法恢复 不同数据库提供不同变更表名（RENAME）（非标准 SQL）指令。如：MySQL 中， 1RENAME TABLE &lt;原表名&gt; to &lt;新表名&gt;; 查询基础SELECT语句基础列的查询12SELECT &lt;列名&gt;，…… FROM &lt;表名&gt;； &lt; * &gt; 按表定义列的顺序列出所有列 不建议使用 &lt; * &gt;；建议列出所有列名，以提高 SQL 可读性 为列或表设定别名123SELECT 列名1 AS 别名1, 列名2 AS 别名2 FROM 表名 AS 表别名; 别名可使用中文，使用时将中文用双引号 &lt;” “&gt; 括起来 别名中空格建议用下划线取代。使用双引号可以设定包含空格的别名，但若忘记使用双引号可能导致错误 Oracle 的 FROM 子句中不能使用 AS（会发生错误），表别名直接跟在表名后即可（即不加 AS） 从结果中删除重复行使用 DISTINCT 实现删除由选择列出的列合成的数据中的重复行 12SELECT DISTINCT &lt;列名&gt;, …… FROM Product; 使用 DISTINCT 时， NULL 也被视为一类数据（显示为空白） DISTINCT 关键字只能用在第一个列名之前 通过 WHERE 子句来指定查询数据的条件123SELECT &lt;列名&gt;, …… FROM &lt;表名&gt; WHERE &lt;条件表达式&gt;; 书写顺序：WHERE 子句必须紧跟在 FROM 子句之后，否则会造成执行错误 执行顺序：首先通过 WHERE 子句查询出符合指定条件的记录，然后再选取出 SELECT 语句指定的列 注释的书写方法 英汉字皆可 单行注释：书写在 “—“ 之后 多行注释：书写在 “/“ 和 “/“ 之间 算术运算符和比较运算符算术运算符SELECT 子句中可以使用常数或者表达式：+ - * ÷ () 所有包含 NULL 的计算，结果肯定是 NULL（包括“NULL/0”的情况，不会报错） FROM 子句在 SELECT 语句中并不是必不可少的，只使用SELECT子句进行计算也是可以的，但使用场景很少。如： 1SELECT (1 + 2) * 3 AS calculation; 存在不允许省略 SELECT 语句中的 FROM 子句的 RDBMS。如：Oracle 比较运算符 =、&lt;、&gt;：等于、小于、大于 &lt;&gt;:不相等。“!=” 非标准 SQL，考虑可移植性和安全问题 , 不建议使用 &lt;=、&gt;=：不大于、不小于。必须不等号在左，等号在右 字符串类型的数据原则上按照字典顺序进行排序。该规则对定长字符串和可变长字符串都适用 SQL 用“IS NULL”和“IS NOT NULL”判断数据是否为NULL。因为SQL 不识别“= NULL”和“&lt;&gt; NULL”，所以不能对 NULL 使用比较运算符 逻辑运算符NOT运算符 NOT 不能单独使用，必须组合其他查询条件表“不是该条件” 为保持程序清晰可读，不滥用 NOT AND 运算符和 OR 运算符 AND 运算符在其两侧的查询条件都成立时整个查询条件才成立，其意思相当于“并且” OR 运算符在其两侧的查询条件有一个成立时整个查询条件都成立，其意思相当于“或者” AND 运算符的优先级高于 OR 运算符 建议使用括号强化优先级，使语句更清晰可读 真值 真值：值为真（TRUE） 或假（FALSE） 其中之一的值 AND 运算的结果与乘法运算（积）的结果一样，称逻辑积 OR 运算的结果与加法运算（和）的结果一样，称逻辑和 SQL 特有情况 — 三值逻辑：除真值外，还存在“不确定”（UNKNOWN）这样的值：P | Q |P AND Q|P OR Q|-|-|-|-真|不确定|不确定|真假|不确定|假|不确定不确定|真|不确定|真不确定|假|假|不确定不确定|不确定|不确定|不确定 建议尽量不使用 NULL，为列设置 NOT NULL 约束，以避免繁琐的条件判断 聚合与排序对表进行聚合查询聚合函数将多行输入汇总为一行输出 COUNT： 计算表中的记录数（行数） COUNT() 特性：不会排除 NULL。故 COUNT()会得到包含 NULL 的数据行数，而 COUNT(&lt;列名&gt;) 会得到 NULL 之外的数据行数 所有的聚合函数，如果以列名为参数，那么在计算之前会把 NULL 排除在外，与“等价为 0”并不相同 SUM： 计算表中 数值列 中数据的合计值 AVG： 计算表中 数值列 中数据的平均值 会事先删除 NULL 同时减少相应数据条数再计算。但也可以选择将 NULL 改变为 0 进行计算 MAX： 求出表中 任意列（如日期） 中数据的最大值 MIN： 求出表中 任意列（如日期）中 数据的最小值 使用聚合函数删除重复值（关键字DISTINCT）12SELECT COUNT(DISTINCT &lt;列名&gt;) FROM &lt;表名&gt;; DISTINCT 必须写在聚合函数参数（即括号）中，因为必须要在执行之前删除列中的重复数据 计算值的种类：在 COUNT() 的参数中使用 DISTINCT 对表进行分组GROUP BY1234SELECT &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, …… FROM &lt;表名&gt; WHERE GROUP BY &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, ……; 聚合键/分组列：GROUP BY 子句中指定的列 书写顺序：SELECT → FROM → WHERE → GROUP BY 执行顺序：FROM → WHERE → GROUP BY → SELECT 聚合键中包含 NULL 时，在结果中会以“不确定”行（空行）的形式表现出来 使用聚合函数和 GROUP BY 注意事项 使用聚合函数时， SELECT 子句中只能存在以下三种元素： 常数 聚合函数 GROUP BY 子句中指定的列名（也就是聚合键） 虽然 MySQL 支持使用GROUP BY子句时， SELECT 子句中出现聚合键之外的列名，但是 MySQL 以外的 DBMS 都不支持这样的语法，因此不建议使用 在 GROUP BY子 句中不能使用 SELECT 子句中定义的别名。根据执行顺序，执行 GROUP BY 子句时，DBMS 还不知道 SELECT 子句中定义的别名 GROUP BY子句结果的显示是无序的。可在 SELECT 语句中进行指定特定顺序 只有 SELECT 子句和 HAVING 子句（以及 ORDER BY 子句）中能够使用聚合函数 DISTINCT 和 GROUP BY：都是通过数据的内部排序处理实现的（执行速度相近），可删除重复数据。根据使用场景，选择能清晰表明语义的写法 为聚合结果指定条件HAVING 子句WHERE 子句只能指定记录（行）的条件，而不能用来指定组的条件。对集合指定条件使用 HAVING 子句： 1234SELECT &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, …… FROM &lt;表名&gt;GROUP BY &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, ……HAVING &lt;分组结果对应的条件&gt; 书写顺序：SELECT → FROM → WHERE → GROUP BY → HAVING 执行顺序：FROM → WHERE → GROUP BY → SELECT HAVING 注意事项 HAVING 子句中只能存在以下三种元素： 常数 聚合函数 GROUP BY 子句中指定的列名（即聚合键） 聚合键所对应的条件应该书写在 WHERE 子句当中，而不应该书写在 HAVING 子句当中。理由如下： HAVING 子句是用来指定“组”的条件的。因此，“行”所对应的条件还是应该写在 WHERE 子句当中，便于理解区分功能 通常情况下，为了得到相同的结果，将条件写在 WHERE 子句中要比写在 HAVING 子句中的处理速度更快，返回结果所需的时间更短 为了理解其中原因，就要从 DBMS 的内部运行机制来考虑。使用 COUNT 函数等对表中的数据进行聚合操作时，DBMS 内部就会进行排序处理。排序处理是会大大增加机器负担的高负荷的处理 A。因此，只有尽可能减少排序的行数，才能提高处理速度。通过 WHERE 子句指定条件时，由于排序之前就对数据进行了过滤，因此能够减少排序的数据量。但 HAVING 子句是在排序之后才对数据进行分的，因此与在 WHERE 子句中指定条件比起来，需要排序的数据量就会多得多。虽然 DBMS 的内部处理不尽相同，但是对于排序处理来说，基本上都是一样的。此外， WHERE 子句更具速度优势的另一个理由是，可以对 WHERE 子句指定条件所对应的列创建索引，这样也可以大幅提高处理速度。创建索引是一种非常普遍的提高 DBMS 性能的方法，效果也十分明显，这对 WHERE 子句来说也十分有利。 对查询结果进行排序ORDER BY子句123SELECT &lt;列名1&gt;, &lt;列名2&gt;, &lt;列名3&gt;, …… FROM &lt;表名&gt; ORDER BY &lt;排序基准列1&gt;, &lt;排序基准列2&gt;, ……; 书写顺序：SELECT → FROM → WHERE → GROUP BY → HAVING → ORDER BY ORDER BY 子句通常写在 SELECT 语句的末尾 未指定 ORDER BY子句中排列顺序时会默认使用升序进行排列；使用 DESC 关键字降序排列 ORDER BY 注意事项 多键排序规则：优先使用左侧的键，如果该列存在相同值的话，再接着参考右侧的键 排序键中包含 NULL 时，会在开头或末尾进行汇总（因为不能对 NULL 使用比较运算符） 在 ORDER BY 子句中可以使用 SELECT 子句中定义的别名。因为 SELECT 子句的执行顺序在 GROUP BY 子句之后， ORDER BY 子句之前 在 ORDER BY 子句中可以使用 SELECT 子句中未使用的列和聚合函数 不要使用列编号指定排序键： 可读性差 SQL-92A 中明确该功能将来会被删除 列编号 — SELECT 子句中的列按照从左到右的顺序进行排列时所对应的编号（1, 2, 3, …） 数据更新数据的插入（INSERT语句的使用方法）INSERT 语句1INSERT INTO &lt;表名&gt; (列1, 列2, 列3, ……) VALUES (值1, 值2, 值3, ……); 原则上，执行一次INSERT语句会插入一行数据,表名后面的列清单和 VALUES 子句中的值清单的列数必须保持一致 很多 RDBMS 都支持多行 INSERT： 1INSERT INTO &lt;表名&gt; (列1, 列2, 列3, ……) VALUES (值1, 值2, 值3, ……), (值1, 值2, 值3, ……) …… ; 列清单的省略 1INSERT INTO &lt;表名&gt; VALUES (值1, 值2, 值3, ……), (值1, 值2, 值3, ……) …… ; 插入NULL：插入 NOT NULL 约束的列会报错 INSERT， DELETE 和 UPDATE 等更新语句也一样，SQL 语句执行失败时都不会对表中数据造成影响 默认插入值在创建表的 CREATE TABLE 语句中设置 DEFAULT 约束来设定默认值： 1234CREATE TABLE ProductIns(&lt;列名&gt; CHAR(4) NOT NULL, &lt;列名&gt; INTEGER DEFAULT 0, -- 销售单价的默认值设定为0;……); 显式方法插入默认值：在 VALUES 中指定 DEFAULT 关键字 1INSERT INTO &lt;表名&gt; (&lt;列名1&gt;, &lt;列名2&gt;, ……) VALUES (DEFAULT, DEFAULT, ……); 隐式方法插入默认值：在列清单和 VALUES 中省略要设定默认值的列 1INSERT INTO &lt;表名&gt; (&lt;列名1&gt;, &lt;列名3&gt;, ……) VALUES (DEFAULT, DEFAULT, ……); -- 列2设定默认值 从其他表中复制数据 创建一张结构一样的表 旧表数据插入新表： 123INSERT INTO 旧表 (&lt;列名1&gt;, &lt;列名2&gt;, ……)SELECT &lt;列名1&gt;, &lt;列名2&gt;, ……FROM 新表; INSERT 语句的 SELECT 语句中，可以使用 WHERE 子句或者 GROUP BY 子句等何 SQL 语法（除 ORDER BY） 指定 ORDER BY 子句也没有任何意义，因为无法保证表内部记录的排列顺序 数据的删除（DELETE语句的使用方法）DELETE语句1234DELETE FROM &lt;表名&gt;; -- 保留数据表，清空表全部数据DELETE FROM &lt;表名&gt; -- 删除表中指定条件数据 WHERE &lt;条件&gt;; 注意事项 DELETE 语句中只能使用 WHERE，而不能使用 GROUP BY、HAVING 和 ORDER BY。因为： GROUP BY 和 HAVING 是从表中选取数据时用来改变抽取数据形式的 ORDER BY 是用来指定取得结果显示顺序的 TRUNCATE：删除表中全部数据1TRUNCATE &lt;表名&gt;; 非标准SQL；Oracle、SQL Server、PostgreSQL、MySQL 和 DB2 不能通过 WHERE 子句指定条件来删除部分数据 数据的更新（UPDATE语句的使用方法）UPDATE 语句123UPDATE &lt;表名&gt; SET &lt;列名&gt; = &lt;表达式/NULL&gt; -- NULL 只限于未设置 NOT NULL 约束的列 WHERE &lt;条件&gt;; 多列更新 法一：所有 DBMS 通用 12345-- 使用逗号对列进行分隔排列UPDATE &lt;表名&gt; SET &lt;列名1&gt; = &lt;表达式1&gt;, &lt;列名2&gt; = &lt;表达式2&gt; WHERE &lt;条件&gt;; 法二：非通用 1234-- 将列用()括起来的清单形式UPDATE Product SET (列名1, 列名2) = (表达式1, 表达式2) WHERE &lt;条件&gt;; 事务什么是事务需要在同一个处理单元中执行的一系列更新处理的集合。例如： 现要求完成往表1插入新数据并更新一些旧数据的任务。要完成该任务，插入和更新两种操作都要完成，则一定要使用事务进行处理（将一起要完成的操作打包进一个事务中进行处理） 创建事务12345事务开始语句;DML语句①;DML语句②;……事务结束语句（ COMMIT或者ROLLBACK） ; 在标准 SQL 中并没有定义事务的开始语句，而是由各个 DBMS 自己来定义的 SQL Server、PostgreSQL： BEGIN TRANSACTIONBEGIN TRANSACTION MySQL： START TRANSACTION Oracle、DB2：无 实际上，几乎所有的数据库产品的事务都无需开始指令。因为大部分情况下，事务在数据库连接建立时就已经开始，并不需要用户再明确发出开始指令 事务结束语句在所有的 RDBMS 中都是通用，只有 COMMIT 和 ROLLBACK 两种： COMMIT — 是提交事务包含的全部更新处理的结束指令，相当于文件处理中的覆盖保存。一旦提交，就无法恢复到事务开始前的状态了 ROLLBACK — 是取消事务包含的全部更新处理的结束指令，相当于文件处理中的放弃保存。一旦回滚，数据库就会恢复到事务开始之前的状态 在不使用指令而悄悄开始事务的情况下，区分各个事务有以下两种模式（通常 DBMS 都可以设置任选其一）： 自动提交模式 — 每条SQL语句就是一个事务（MySQL、SQL Server 和 PostgreSQL 默认使用） 直到用户执行 COMMIT 或者 ROLLBACK 为止算作一个事务（Oracle 默认使用） 若使用 DELETE 语句删除了数据表： 自动提交模式下，无法回滚恢复 非自动提交模式下，可以通过 ROLLBACK 命令取消该事务的处理，恢复表中的数据。但这仅限于明示开始事务，或者关闭自动提交的情况 ACID特性 原子性（Atomicity） 原子性是指在事务结束时，其中所包含的更新处理要么都执行，要么都不执行 一致性（Consistency）/完整性 一致性指的是事务中包含的处理要满足数据库提前设置的约束 隔离性（Isolation） 隔离性指的是保证不同事务之间互不干扰的特性。该特性保证了事务之间不会互相嵌套。此外，在某个事务中进行的更改，在该事务结束之前，对其他事务而言是不可见的 持久性（Durability） 持久性指的是在事务（不论是提交还是回滚）结束后， DBMS 能够保证该时间点的数据状态会被保存的特性。即使由于系统故障导致数据丢失，数据库也一定能通过某种手段进行恢复，如日志系统 复杂查询视图视图：保存好的 SELECT 语句 视图和表区别：表中存储的是实际数据，而视图中保存的是从表中取出数据所使用的SELECT语句 视图的优点： 无需保存数据，节省存储设备的容量 将频繁使用的 SELECT 语句保存成视图，不用重写重新执行，以提高效率 创建视图123CREATE VIEW 视图名称(&lt;视图列名1&gt;, &lt;视图列名2&gt;, ……)AS&lt;SELECT语句&gt; SELECT 语句中列的排列顺序和视图中列的排列顺序相同 多重视图：以视图为基础创建视图。多重视图会降低 SQL 的性能应尽量避免 使用视图查询 首先执行定义视图的 SELECT 语句 根据得到的结果，再执行在 FROM 子句中使用视图的 SELECT 语句 视图的限制 定义视图时不能使用 ORDER BY 子句 视图和表一样， 数据行都是没有顺序的 对视图进行更新 标准 SQL 中规定，想要视图可以被更新，定义视图的 SELECT 语句需要满足某些条件（非通过汇总）： SELECT 子句中未使用 DISTINCT FROM 子句中只有一张表 未使用 GROUP BY 子句 未使用 HAVING 子句 原因：视图和表需要同时进行更新，以保持数据一致性，因此通过汇总得到的视图无法进行更新 删除视图1DROP VIEW 视图名称(&lt;视图列名1&gt;, &lt;视图列名2&gt;, ……)； 子查询子查询和视图子查询就是将用来定义视图的 SELECT 语句直接用于 FROM 子句当中（为查询结果命别名） 子查询作为内层查询会首先执行 原则上子查询必须设定名称（使用 AS 关键字） 标量子查询标量子查询就是返回单一值的子查询 优点：返回的是单一值，可以用在 = 或者 &lt;&gt; 等比较运算符之中 书写位置：能够使用常数或者列名的地方 注意事项：子查询中只能返回单一值 关联子查询（建议刷题理解）与普通的子查询的区别在子查询中添加的 WHERE 子句的条件 为区别表对应不同的场景，在表所对应的列名之前加上表的别名，形式为“&lt;表名&gt;.&lt;列名&gt;” 适合在细分的组内进行比较时使用 结合条件一定要写在子查询中 函数、谓词、CASE表达式函数函数的种类 算术函数：数值计算 字符串函数：字符串操作 日期函数：日期操作 转换函数：转换数据类型和值 聚合函数：数据聚合 算术函数 绝对值函数 1ABS(数值) 求余 1MOD(被除数，除数) [注] SQL Server 不支持 MOD()，而使用 “%” 求余 四舍五入 1ROUND(对象数值，保留小数的位数) 字符串函数 拼接 1字符串1 || 字符串2 || 字符串3 || …… [注] SQL Server 不支持 ||，而使用 “+” 拼接字符串 MySQL 不支持 ||，而使用 CONCAT() 拼接字符串 字符串长度 1LENGTH(字符串) [注] SQL Server 不支持 LENGTH()，而使用 LEN() 拼接字符串 同样是 LENGTH 函数，不同 DBMS 的执行结果也不尽相同。MySQL 中的 LENGTH() 以字节为单位的函数进行计算,此外还存在计算字符串长度的自有函数 CHAR_LENGTH() 大小写转换 1UPPER/LOWER(字符串) [注] UPPER/LOWER 函数只能针对英文字母使用，将参数中的字符串全都转换为大/小写 字符串替换 1REPLACE(对象字符串，替换前的字符串，替换后的字符串) 字符串截取 1SUBSTRING（对象字符串 FROM 截取的起始位置 FOR 截取的字符数） [注] 标准 SQL，但只有 PostgreSQL 和 MySQL 支持该语法 SQL Server 版本： SUBSTRING(对象字符串，截取的起始位置，截取的字符数) Oracle 版本： SUBSTR(对象字符串，截取的起始位置，截取的字符数) 日期函数 当前日期 1CURRENT_DATE [注] SQL Server 不支持 CURRENT_DATE，而使用 CAST(CURRENT_TIMESTAMP AS DATE) 获取当前日期 当前时间 1CURRENT_TIME [注] SQL Server 不支持 CURRENT_TIME，而使用 CAST(CURRENT_TIMESTAMP AS TIME) 获取当前时间 当前日期和时间 1CURRENT_TIMESTAMP 截取日期元素 1EXTRACT(日期元素 FROM 日期) [注] 日期元素： YEAR MONTH DAY HOUR MINUTE SECOND SQL Server 不支持 EXTRACT 函数，而使用 DATEPART 函数 获取当前时间 DATEPART(日期元素 , CURRENT_TIMESTAMP) 转换函数 CAST —— 数据类型转换 1CAST（转换前的值 AS 想要转换的数据类型） [注] 使用场景： 插入与表中数据类型不匹配的数据 在进行运算时由于数据类型不一致发生了错误 进行自动类型转换会造成处理速度低下 COALESCE —— 将NULL转换为其他值 返回可变参数 A 中左侧开始第 1个不是 NULL 的值 1COALESCE(数据1，数据2，数据3……) 谓词 — 返回值是真值的函数LIKE — 模糊查询123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; LIKE '模式'; % 代表“0 个字符以上的任意字符串” _（下划线）代表了“任意 1 个字符” 模式 匹配 abc% abcqwe %abc% qweabcqwe %abc qweabc abc__ abcqw abc eabcq _abc qabc BETWEENT —— 范围查询123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; BETWEEN &lt;上限&gt; AND &lt;下限&gt;; 数据可以是数值、文本或者日期 BETWEEN 的结果包含 &lt;上限&gt; 和 &lt;下限&gt;。不想让结果包含临界值则使用 &lt; 和 &gt; IS (NOT) NULL —— 判断是否为NULL选取出某些值为（不为） NULL 的列的数据只能使用特定的谓词 IS (NOT) NULL 123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; IS (NOT) NULL; IN 谓词 —— OR 的简便用法123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; (NOT) IN (集合元素……/子查询); 使用 IN 和 NOT IN 无法选取出 NULL 数据，因为 NULL 只能使用 IS (NOT) NULL 选取 EXIST 谓词判断是否存在满足某条件的记录 123SELECT * FROM &lt;表名&gt; WHERE &lt;列名&gt; (NOT) EXIST (集合元素……/子查询); 通常指定关联子查询作为 EXIST 的参数 由于 EXIST 只关心记录是否存在，因此子查询中返回哪些列都没有关系（建议统一在 EXIST 的子查询中书写 SELECT *） 与 in 执行时的区别：in 先执行子查询中的查询，再执行主查询；exists 先执行主查询，即外层表的查询，再执行子查询。效率视情况而定 CASE 表达式 — 区分情况执行CASE表达式 搜索 CASE 表达式语法 123456CASE WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; WHEN &lt;求值表达式&gt; THEN &lt;表达式&gt; …… ELSE &lt;表达式&gt;END 执行过程： 第一条 WHEN 子句中的“&lt; 求值表达式 &gt;”求值 结果为真（TRUE），返回 THEN 子句中的表达式， 执行完毕；结果不为真，顺次转到下一条 WHEN 子句进行求值…… 若到最后一条 WHEN 子句为止返回结果都不为真，则返回 ELSE 中的表达式 执行完毕 ELSE 子句也可以省略不写，这时会被默认为 ELSE NULL，但不建议省略 END 不能省略 集合运算表的集合运算（以行方向为单位进行操作）Def进行这些集合运算时，会导致记录行数的增减，但不会导致列数的改变 UNION（并集）表 a 和 表 b 的并集 12345SELECT &lt;列名&gt;, …… FROM &lt;a 表名&gt; UNION （ALL）SELECT &lt;列名&gt;, …… FROM &lt;b 表名&gt;; INTERSECT（交集）表 a 和 表 b 的交集 12345SELECT &lt;列名&gt;, …… FROM &lt;a 表名&gt;INTERSECT （ALL）SELECT &lt;列名&gt;, …… FROM &lt;b 表名&gt;; EXCEPT（差集）表 a 和 表 b 的差集，即表 a 除去与表 b 交集部分所剩余的部分 12345SELECT &lt;列名&gt;, …… FROM &lt;a 表名&gt;EXCEPTSELECT &lt;列名&gt;, …… FROM &lt;b 表名&gt;; MySQL 不支持 EXCEPT Oracle 中求差集将 “EXCEPT” 改为 “MINUS” 集合运算注意事项 集合运算符会除去重复的记录。使用 ALL 选项，可以保留重复行 作为运算对象的记录的列数必须相同 作为运算对象的记录中列的类型必须一致 可以使用任何 SELECT 语句，但 ORDER BY 子句只能在最后使用一次 联结（以列为单位对表进行联结）Def将其他表中的列添加过来，进行“添加列”的运算 内联结——INNER JOIN以两张表中都包含的列（联结键）作为桥梁，将只存在于一张表内的列汇集到同一结果之中 12345SELECT &lt;表1别名&gt;.&lt;列名&gt;, …， &lt;表2别名&gt;.&lt;列名&gt;, … FROM &lt;表1&gt; AS &lt;表1别名&gt; INNER JOIN &lt;表2&gt; AS &lt;表2别名&gt; ON 表1.&lt;共同列&gt; = 表2.&lt;共同列&gt; WHERE …… 外联结——OUTER JOIN通过 ON 子句的联结键将两张表进行联结，并从两张表中同时选取相应的列 12345SELECT &lt;表1别名&gt;.&lt;列名&gt;, …， &lt;表2别名&gt;.&lt;列名&gt;, … FROM &lt;表1&gt; AS &lt;表1别名&gt; LEFT/RIGHT JOIN &lt;表2&gt; AS &lt;表2别名&gt; ON 表1.&lt;共同列&gt; = 表2.&lt;共同列&gt; WHERE …… 与内联结区别 内联结只能选取出同时存在于两张表中的数据 对于外联结，只要数据存在于某一张表当中，就能够读取出来 外联结中使用LEFT、RIGHT来指定主表，最终的结果中会包含主表内所有的数据 多表联结通过 ON 子句的联结键将多张表进行联结。 1234567SELECT &lt;表1别名&gt;.&lt;列名&gt;, …， &lt;表2别名&gt;.&lt;列名&gt;, … FROM &lt;表1&gt; AS &lt;表1别名&gt; INNER JOIN &lt;表2&gt; AS &lt;表2别名&gt; ON 表1.&lt;共同列&gt; = 表2.&lt;共同列&gt; INNER JOIN &lt;表3&gt; AS &lt;表3别名&gt; ON 表1.&lt;共同列&gt; = 表3.&lt;共同列&gt; WHERE …… 如下例子中表 1 与表 2 联结，表 1 与表 3 联结，则表 2 与表 3 无需再联结 “表 1 与表 3 联结”改为“表 2 与表 3 联结”，效果一样 SQL 高级处理窗口函数窗口函数的语法12&lt;窗口函数&gt; OVER (PARTITION BY &lt;列清单&gt;) ORDER BY &lt;排序用列清单&gt;) 通过 PARTITION BY 分组后的记录集合称为“窗口”（意指“范围”）。PARTITION BY 并非必需，不指定 PARTITION BY 时，将整个表作为一个大的窗口来使用 OVER 子句中的 ORDER BY 只是用来决定窗口函数按照什么样的顺序进行计算的，对结果的排列顺序并没有影响。对结果排序需要在 SELECT 语句的最后使用 ORDER BY 子句，此时两个 ORDER BY 功能完全不同 窗口函数兼具分组和排序两种功能 能够作为窗口函数使用的函数 能够作为窗口函数的聚合函数（SUM、AVG、COUNT、MAX、MIN） RANK、DENSE_RANK、ROW_NUMBER 等专用窗口函数 以专用窗口函数 RANK 为例 1234SELECT &lt;列1&gt;, &lt;列2&gt;, &lt;列3&gt;， … RANK () OVER (PARTITION BY &lt;列2&gt; ORDER BY &lt;列3&gt;) AS ranking FROM &lt;表名&gt;; PARTITION BY 能够设定排序的对象范围 ORDER BY 能够指定按照哪一列、何种顺序进行排序 专用窗口函数 RANK函数 计算排序时，如果存在相同位次的记录，则会跳过之后的位次。 例：有 3 条记录排在第 1 位时：1 位、1 位、1 位、4 位…… DENSE_RANK函数 同样是计算排序，即使存在相同位次的记录，也不会跳过之后的位次。 例：有 3 条记录排在第 1 位时：1 位、1 位、1 位、2 位…… ROW_NUMBER函数 赋予唯一的连续位次。 例：有 3 条记录排在第 1 位时：1 位、2 位、3 位、4 位…… 专用窗口函数无需参数，因此通常参数括号中都是空的 作为窗口函数使用的聚合函数以专用 AVG() 为例 123SELECT &lt;列1&gt;, &lt;列2&gt;, &lt;列3&gt;， …， AVG (指定列) OVER (ORDER BY &lt;指定列&gt;) AS ranking FROM &lt;表名&gt;; 得到的结果按照 ORDER BY 子句指定列的升序排列，一行一行逐渐添加计算对象，累计进行聚合函数运算 聚合函数作为窗口函数时的最大特征：以当前记录作为基准进行统计 窗口函数的适用范围原则上，窗口函数只能书写在 SELECT 子句中。 在 DBMS 内部，窗口函数是对 WHERE 子句或者 GROUP BY 子句处理后的“结果”进行的操作。大家仔细想一想就会明白，在得到用户想要的结果之前，即使进行了排序处理，结果也是错误的。在得到排序结果之后，如果通过 WHERE 子句中的条件除去了某些记录，或者使用 GROUP BY 子句进行了汇总处理，那好不容易得到的排序结果也无法使用了。反之，之所以在 ORDER BY 子句中能够使用窗口函数，是因为 ORDER BY 子句会在 SELECT 子句之后执行，并且记录保证不会减少。 因此，在 SELECT 子句之外“使用窗口函数是没有意义的。 计算移动平均移动平均（moving average）常用于希望实时把握“最近状态”的场景，如数据的实时跟踪 123SELECT &lt;列1&gt;, &lt;列2&gt;, &lt;列3&gt;， …， AVG &lt;指定列&gt; OVER (ORDER BY &lt;指定列&gt; ROWS n PRECEDING)FROM &lt;表名&gt;; 使用了 ROWS（“行”）和 PRECEDING（“之前”）两个关键字，将框架指定为“截止到之前 n 行”，即：自身（当前记录）、之前第 1 行、 ……、 之前第 ~ 行，共 n+1 行 框架:在窗口中指定更加详细的汇总范围 FOLLOWING（“之后”）可替换 PRECEDING，指定“截止到之后 ~ 行”作为框架 GROUPING 运算符ROLLUP123SELECT GROUPING(聚合键)， SUM(求和列) AS sum_指定列 FROM &lt;表名&gt; GROUP BY ROLLUP(聚合键); 可一次计算出按不同聚合键组合的求和结果 此处聚合键和 GROUP BY 子句使用一样，可以为 NULL，可以指定多列 GROUP BY 不指定聚合键时会默认使用 NULL 作为聚合键（相当于没有使用 GROUP BY），此时会得到全部数据的合计行的记录，称超级分组 GROUPING 函数在其参数列的值为超级分组记录所产生的 NULL 时返回 1，其他情况返回 0，以分辨出原始数据中的 NULL 和超级分组记录中的 NULL 在 MySQL 中 GROUP BY 子句应改写为“GROUP BY &lt;指定列&gt; WITH ROLLUP;” CUBECUBE 将 GROUP BY 子句中聚合键的“所有可能的组合”的汇总结果集中到一个结果中。因此，组合的个数就是 $2^n$（n 是聚合键的个数） 123SELECT GROUPING(聚合键)， SUM(求和列) AS sum_指定列 FROM &lt;表名&gt; GROUP BY CUBE(聚合键); 可以把 CUBE 理解为将使用聚合键进行切割的模块堆积成一个立方体 GROUPING SETSCUBE 的结果就是根据聚合键的所有可能的组合计算而来的，使用 GROUPING SETS 可以取得部分组合的结果 123SELECT GROUPING(聚合键)， SUM(求和列) AS sum_指定列 FROM &lt;表名&gt; GROUP BY GROUPING SETS(&lt;聚合键组合1&gt;, &lt;聚合键组合2&gt;, …);]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>collection</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 基础]]></title>
    <url>%2F2020%2F03%2F30%2Fmysql-basic%2F</url>
    <content type="text"><![CDATA[[Updated] 本文基于《SQL基础教程》整理了 MySQL 基础知识。 MySQL 基础基础架构Server 层包括：连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 连接器负责跟客户端建立连接、获取权限、维持和管理连接： 建立连接 客户端连接服务端，完成 TCP 握手后，连接器开始进行用户认证。 获取权限 用户认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。即使用管理员账号对当前连接用户进行权限修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。 连接维持 连接完成后，如果你没有后续的动作，连接就处于空闲状态（可使用 show processlist 命令察看）。在一定时间（由参数 wait_timeout 控制，默认 8h）内一直处于空闲，连接器会自动断开。 连接类型： 短连接 每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。 长连接 连接成功后，如果客户端持续有请求，则一直使用同一个连接。由于建立连接的过程通常是比较复杂的，所以应尽量减少建立连接的动作，即尽量使用长连接。 如何解决长连接累积消耗内存导致 OOM？ MySQL 在执行过程中临时使用的内存资源是管理在连接对象里面的，直至连接断开时才释放，因此长连接累积可能导致 OOM。解决方案： 定期断开长连接 使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 重新初始化连接资源 MySQL 5.7 之后的版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。该过程无需重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存 不建议使用的原因 只要对一个表执行更新操作，该表上所有的查询缓存都会被清空。对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非业务是静态表，很少更新数据，如：系统配置表。 MySQL 8.0 后的版本以移除查询缓存功能。 分析器 词法分析 识别 SQL 语句的成分（关键字、表名、列名、运算符、函数……）. 语法分析 根据语法规则和词法分析的结果生成一颗解析树，通过检查解析树的合法性判断你输入 SQL 是否满足 MySQL 语法。比如：表和列名是否存在，别名歧义等。 优化器 在表里面有多个索引的时候，决定使用哪个索引； 在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 执行器 判断当前连接对操作表的权限； 打开表，根据表的引擎定义调用引擎借口进行操作（每取一行调用一次引擎接口）。 引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。 InnoDBInnoDB 是现在最常用的存储引擎，MySQL 5.5.5 版本开始成为默认存储引擎。InnoDB 基于 B+ 树，采用聚集索引，即索引文件本身就是数据文件。 MyISAMMyISAM 也是基于 B+ 树，但采用非聚集索引，即索引和实际存储数据分开，其索引指针指向存储实际数据地址。 InnoDB 和 MyISAM 区别 InnoDB MyISAM 事务支持 支持 不支持 存储结构 表结构、数据文件 表定义文件.frm、数据文件.MYD、索引.MYI 操作效率 INSERT、DELETE、UPDATE 更安全更快 SELECT 更快 全表行数 不保存，需要全表扫描 保存，直接读出；使用 WHERE 需要全表扫描 索引 InnoDB 5.6+ 支持全文索引，不支持压缩索引 支持全文索引、压缩索引 主键 无主键时，自动创建自增索引作主键 允许无任何主键和索引 外键 支持 不支持 清表操作 逐行删除 重建表 锁支持 行级锁、表级锁 只表级锁 适用场景 i. 并发，要求可靠性高、事务处理 i. 查询、插入频繁，修改不频繁 ii. 修改频繁 ii. 常做全表 COUNT 事务事务定义 begin/start transaction 和 commit 之间的操作集合是一个事务。【注】begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动 没有显式地使用 begin/commit 时，每个 SQL 语句本身就是一个事务，语句完成的时候会自动提交。 事务特性：ACIDAtomicity 原子性 定义 事务是一个不可再分割的工作单位，事务中的操作要么都发生，要么都不发生。 实现 日志 Consistency 一致性 定义 事务开始之前和事务结束以后，数据库的完整性约束没有被破坏。这是说数据库事务不能破坏关系数据的完整性以及业务逻辑上的一致性。 实现 设置约束和触发器 事务隔离 锁机制 Isolation 隔离性 定义 多个事务并发访问时，事务之间是隔离的，一个事务不应该影响其它事务运行效果。 实现 事务隔离 Durability 持久性 定义 事务完成以后，该事务所对数据库所作的更改便持久的保存在数据库之中，并不会被回滚。 实现 WAL（Write-Ahead Logging） 总结 DBMS 采用日志系统来保证事务的原子性、一致性和持久性。 日志系统记录了事务对数据库所做的更新，如果某个事务在执行过程中发生错误，就可以根据日志，撤销事务对数据库已做的更新，使数据库退回到执行事务前的初始状态。 DBMS 采用锁机制来实现事务的隔离性。 当多个事务同时更新数据库中相同的数据时，只允许持有锁的事务能更新该数据，其他事务必须等待，直到前一个事务释放了锁，其他事务才有机会更新该数据。 日志系统 key word redo log binlog 保证事务的原子性、一致性、持久性 redolog格式一个大小固定的循环队列，记录“哪个数据页做了哪些改动”。 WAL 关键 Write-Ahead Logging：先写日志，再写磁盘 过程 记录需要更新，先写入 redo log，并在内存更新 InnoDB 择机（系统空闲时）将更新记录更新到磁盘 因为 redo log 是固定大小的（类似循环队列），当没有空间存储更新记录时（队满，无法入队），InnoDB 会将 redo log 数据清除一部分（出队）并刷入磁盘 作用 保证 crash-safe：即使数据库发生异常重启，之前提交的记录都不会丢。 binlog两种格式 statement 格式 记录完整的 sql 语句。 row 格式 记录数据行的内容，包含两部分：更新前、更新后。 特点 binlog 日志只能用于归档 只有 binlog 的 MySQL 不具备 crash-safe 能力 redolog 和 binlog 区别 功能层次 redo log 是引擎层 InnoDB 引擎特有的 binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用 记录内容 redo log 是物理日志，记录的是“在某个数据页上做了什么修改” binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ” 空间大小 redo log 是循环写的，空间固定会用完 binlog 是可以追加写入的，即 binlog 文件写到一定大小后会切换到下一个新文件，并不会覆盖以前的日志 redo log 两阶段提交redo log 的写入有两个步骤：prepare 和 commit，即“两阶段提交”。 过程以 update 语句将某行某字段 c 加 1 为例： 执行器向引擎取该行数据 行数据所在数据页存在内存，直接返回给执行器；若不存在，从磁盘取出所在数据页，再向执行器返回 执行器将值加 1，得到新的行数据，再调用引擎接口写入新的行数据 引擎将行数据更新到内存，同时将更新操作写入 redo log，此时，redo log 处于 prepare 状态，并告知执行器执行完成，可以提交事务 执行器生成该更新操作的 binlog，并将其写入磁盘 执行器调用引擎提交事务借口，将刚写入的 redo log 改成 commit 状态，至此更新完成 目的redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑一致。 不用两阶段提交情况下，假设执行 update 语句某行某字段 c 值由 0 更新为 1 ，过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，可能发生以下错误： 先写 redo log 后写 binlog。 假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。 redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 如果后期需要用这个 binlog 来恢复临时库的话，由于这个 binlog 丢失该 update 语句，所恢复的临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，异于原库。 先写 binlog 后写 redo log。 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。 如果后期用这个 binlog 恢复数据时就多了一个更新事务，恢复出来该行 c 的值就是 1，与原库的值不同。 数据误删恢复过程 首先找到最近一次全量备份，从该全量备份恢复到临时库 从备份时间点开始，将备份的 binlog 依次取出来，重放到误删表之前的那个时刻 此时临时库就跟误删之前的线上库一样了，再将表数据从临时库取出，按需恢复到线上库即可 事务隔离MVCC定义MVCC(Mutil-Version Concurrency Control)，即多版本并发控制，在 RDBMS 中实现对数据库的并发访问。 在 MySQL 里，有两个“视图”的概念： 视图 view view 是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。 创建视图的语法是 create view … ，其查询方法与表一样。 一致性读视图 consistent read view InnoDB 在实现 MVCC 时用到的一致性读视图，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 快照在可重复读隔离级别下，事务在启动时会拍一个快照，该快照是基于整个库的。基于整个库意味着：一个事务内，整个库的修改对于该事务都是不可见的(对于快照读的情况)。如果在事务内 select 表 t1，另外的事务执行了DDL 表 t1，根据发生时间，要么锁住要么报错。 InnoDB 利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力。 多版本 transaction id InnoDB 中每个事务都有唯一的事务 ID，即 transaction id，在事务开始的时候向 InnoDB 的事务系统申请，按申请顺序严格递增的。 row trx_id 每行数据都有多个版本。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。 即将“更新该数据行的事务 ID ”作为“产生该数据行版本的 ID”。 版本查看 数据每次更新会产生 undo log 并且记录当前版本的 row trx_id。通过当前版本 + undo log 依次往前计算，可以顺次得到之前的版本。 数据版本的可见性规则视图数组和高水位，就组成了当前事务的一致性视图（read-view）： 视图数组 InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。 高低水位 数组里面事务 ID 的最小值记为低水位；当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。 基于数据的 row trx_id 和这个一致性视图的对比结果得到数据可见性。对于当前事务的启动瞬间来说，一个数据行的 row trx_id 有以下几种可能： [已提交事务|&lt;低水位&gt;|未提交事务集合|&lt;高水位&gt;|未开始事务] 如果在 “已提交事务” 部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果在 “未开始事务” 部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果在 “未提交事务集合” 部分，那就包括两种情况 若 row trx_id 在图数组中，表示这个版本是由还没提交的事务生成的，不可见； 若 row trx_id 不在视图数组中，表示这个版本是已经提交了的事务生成的，可见。 更新逻辑 “当前读”（current read） 更新数据都是先读后写的，而这个读，只能读当前（数据行的最新版本）的值，称为“当前读”（current read）。 根据两阶段锁协议，当前事务的当前读必须要读取最新版本，而且必须加锁，所以要等到其他事务更新完毕，当前事务才能完成当前读。 除了 update 语句外，select 语句如果加锁（lock in share mode 或 for update），也是当前读。 总结 InnoDB 的行数据有多个版本，每个数据版本有自己的 row trx_id，每个事务或者语句有自己的一致性视图。 普通查询语句是一致性读，一致性读会根据 row trx_id 和一致性视图确定数据版本的可见性。 可重复读实现的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。 读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是： 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 可见性总结 对于可重复读，查询只承认在事务启动前就已经提交完成的数据； 对于读提交，查询只承认在语句启动前就已经提交完成的数据； 对于当前读，总是读取已提交完成的最新版本。 问题 为什么表结构不支持“可重复读”？ 因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑。 为什么 rr 能实现可重复读而 rc 不能？ 快照读的情况下, rr 不能更新事务内的 up_limit_id（高水位）, 而 rc 每次会把 up_limit_id 更新为快照读之前最新已提交事务的 transaction id,则 rc 不能可重复读 当前读的情况下, rr 是利用 record lock + gap lock 来实现的，而 rc 没有 gap，所以 rc 不能可重复读 事务执行问题脏读（dirty read） 关键：读取未提交数据。 事务 A 读取了事务 B 未提交数据后，事务 B 发生执行错误并进行回滚，那么事务 A 读取数据即为脏数据。 解决：读提交、可重复读、串行化。 丢失更新 关键：更新覆盖。 第一类： 事务 A 和事务 B 都瞄准了同一数据行，事务 A 回滚时将事务 B 的更新数据覆盖。 解决：可重复读、串行化。 第二类： 事务 A 和事务 B 都瞄准了同一数据行，并先后执行了更新操作，那么事务 A 的更新数据就有可能被事务 B 覆盖。 解决：悲观锁、乐观锁。 不可重复读（non-repeatable read） 关键：前后多次读取，数据内容不一致。 事务 A 执行内容较多，在较长的时间间隔先后读取了同一行数据，期间该行数据被其他事务执行了更新操作（insert / update），导致前后读取的数据不一致，无法取到重复的数据。 解决：行级锁，锁定该行。 幻读（phantom read） 关键：前后多次读取，数据总量不一致。 事务 A 执行内容较多，在较长的时间间隔先后查询了数据总量，期间其他事务执行了增减数据的操作（insert / update），导致前后统计的数据总量不一样，仿佛产生幻觉。 解决：表级锁，锁定整张表。 隔离级别为解决事务执行过程中可能出现的问题，产生了“隔离级别”的概念。 读未提交（read uncommitted）一个事务还未提交，它所做的变更就可以被别的事务看到。 读提交（read committed）一个事务提交之后，它所做的变更才可以被别的事务看到。 可重复读（repeatable read）一个事务执行过程中看到的数据始终一致，未提交的更改对其他事务是不可见的。 串行化（serializable）对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行。 隔离级别程度 ↓程度递增→ 脏读 丢失更新 不可重复读 幻读 读未提交 √ √ √ √ 读提交 X √ √ √ 可重复读 X X X √ 串行化 X X X X 注意： √ - 允许，X - 禁止 隔离剂级别越高越安全，但性能越低 索引 索引的作用：提高数据查询的效率，类似书的目录 索引常见模型 索引模型是数据库底层存储的核心，有助于理解分析数据库的适用场景 哈希表 特点：键值对 优点：由哈希算法直接由值得到位置 缺点：可能发生冲突。冲突键一般用链表解决，退化为普通查询 适用场景：等值查询 效率：无冲突O(1);冲突O(n) 有序数组 特点：数据按顺序存储 优点：查询效率高，等值查询和范围查询皆适用 缺点：更新效率低 适用场景：只适用于静态存储引擎 效率：二分法O(log(N)) 搜索树二叉树是树搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树 不使用二叉树原因索引不止存在内存中，还要写到磁盘上。对于高为 h 的搜索树，一次查询可能需要访问 h 个数据块，即读 h 次盘。而二叉树的高度往往过高，会导致查询过程中在磁盘读取上消耗过多时间 为何使用 N 叉树为了查询过程访问尽量少的数据块，适用 N 叉树降低树高，“N” 取决于数据块的大小 B+ 树能够很好地配合磁盘的读写特性，减少单次查询的磁盘访问次数，被广泛应用在数据库引擎中 InnoDB 的索引模型 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同 索引模型：B+树在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表，数据存储在 B+ 树中 索引类型 主键索引的叶子节点存的是整行数据在 InnoDB 里，主键索引也被称为聚簇索引（clustered index） 非主键索引的叶子节点内容是主键的值在 InnoDB 里，非主键索引也被称为二级索引（secondary index） 基于主键索引和普通索引的查询的区别 主键索引只要根据主键值搜索主键索引树即可拿到数据 普通索引先搜索索引树拿到主键值，再到主键索引树搜索一次(回表) 基于非主键索引的查询需要多扫描一棵索引树，故应该尽量使用主键查询 索引维护为维护索引有序性，根据 B+ 树模型，一个数据页满了会进行页分裂(新增一个数据页)，导致性能下降，空间利用率降低大概 50%；反之，相邻数据页利用率很低时会进行页合并 自增索引自增主键是指自增列上定义的主键 定义： 1NOT NULL PRIMARY KEY AUTO_INCREMENT 特点： 插入新记录的时候可以不指定 ID 的值，系统会获取当前 ID 最大值加 1 作为下一条记录的 ID 值 是否应该使用自增索引 结论：从性能和存储空间方面考量，自增主键往往是更合理的选择，K-V 场景除外 适用场景 性能角度：递增插入。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂；若使用业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高 空间角度：主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小 特殊场景 K-V 场景 只有一个索引 该索引必须是唯一索引 由于没有其他索引，故不用考虑其他索引的空间问题，此时优先考虑“尽量使用主键查询”原则 覆盖索引覆盖索引：待查值已在索引树上（即索引树已覆盖查询需求），因此可以直接提供查询结果，而不需要回表。使用覆盖索引可以减少树的搜索次数（IO磁盘读写次数），显著提升查询性能 最左前缀原则B+Tree 这种索引结构，可以利用索引的”最左前缀”来定位记录：可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。 联合索引内的字段顺序安排 关键：索引的复用能力。 因为可以支持最左前缀，所以当已经有了 (a,b) 这个联合索引后，一般就不需要单独在 a 上建立索引。因此，第一原则是，若通过调整索引字段顺序可以少维护一个索引，那么该顺序往往就是需要优先考虑采用的。 索引下推MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 普通索引和唯一索引的选择从查询语句看 对于普通索引 查找到第一个满足条件的记录后，继续查找下一个记录，直至碰到第一个不满足条件的记录 对于唯一索引 由于索引定义了唯一性，查找到第一个满足条件的记录即停止检索 因为 InnoDB 的数据是按数据页为单位来读写的，寻找记录时会将记录所在的整个数据页（对于整型字段，一个数据页可以放近千个 key）读取到内存，故普通索引虽然会比唯一索引多出几次寻找和判断，但是对于现在的 CPU 来说计算性能差距微乎其微（记录跨两个数据页的情况除外，发生概率较低）。 change buffer当需要更新一个数据页： 数据页在内存中则直接更新 若数据页不在内存中，为不影响数据一致性，InnoDB 会将更新操作缓存在 change buffer 中，而不会从磁盘中读取所在数据页进行更新 在下次查询需要访问该数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作，再查询该数据页，以保证数据逻辑的正确性 锁机制根据加锁的范围，MySQL 里面的锁大致可以分成全局锁、表级锁和行锁三类。 全局锁全局锁作用对整个数据库实例加锁，使整个数据库处于只读状态，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 加全局读锁命令：FTWRL1Flush tables with read lock; 使用场景全库逻辑备份，即“把整库每个表都 select 出来存成文本”。 缺点 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 为何加锁不加锁的话，备份系统备份得到的库不是一个逻辑时间点，视图逻辑不一致。 不建议使用 “set global readonly=true” 设置全库只读的原因 修改 global 变量的方式影响面更大。 在有些 DBMS 中，readonly 的值会被用来做其他逻辑，比如判断一个库是主库还是备库。 异常处理机制上有差异。 如果执行 FTWRL 命令之后由于客户端发生异常断开，MySQL 会自动释放全局锁，整个库恢复可以正常更新的状态。 而将整个库设置为 readonly 之后，如果客户端发生异常，数据库会一直保持 readonly 状态，就会导致整个库长时间处于不可写状态，风险较高。 表级锁表级锁分类 表锁 元数据锁（meta data lock，MDL) 表锁 作用 若某线程 “lock tables t1 read, t2 write;”： 该线程自身只能 “读表 t1、读写 t2”，而不能访问其他表 其他线程 “写表 t1、读写 t2” 的语句都会被阻塞 命令 12345-- 上锁lock tables … read/write;-- 释放锁unlock tables; 使用场景 在没有更细粒度的锁的情况下，用于处理并发，如 MyISAM 引擎。而对于拥有更细粒度的行锁 InnoDB 一般使用行锁控制并发，因为行锁的影响面小于表锁。 MDL 作用 MDL 不需要显式使用，在访问一个表的时候会被自动加上，以保证读写的正确性。 MySQL 5.5+ 版本引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作（如：加字段）的时候，加 MDL 写锁。 特点 读锁之间不互斥 可以有多个线程同时对一张表增删改查。 写锁 与 读/写锁 都互斥 如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行，以保证变更表结构操作的安全性。 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。 如何安全地给小表加字段? 解决长事务 DDL 要变更时有长事务在执行，可以考虑先暂停 DDL，或者 kill 掉这个长事务。 在 DDL 语句里面设定等待时间 在指定的等待时间里拿不到 MDL 写锁，先放弃更高改，后期再进行重试，以免阻塞后面的业务语句。 行锁两阶段锁 定义 在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。 建议 如果事务中需要锁多个行，应该把最可能造成锁冲突、最可能影响并发度的锁尽量在事务中往后放，使行锁的时间最短，以最大程度地减少了事务之间的锁等待，提升了并发度。 死锁当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。 死锁解决策略 直接进入等待，直到超时。 设置 超时时间可以通过参数 innodb_lock_wait_timeout 来设置（InnoDB 中默认值是 50s）。 缺点 等待时长过长，影响线上服务效率和用户体验 等待时长过短，误伤时间稍长的锁等待 主动死锁检测 检测发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。 设置 死锁检测通过将参数 innodb_deadlock_detect 设置为 on 开启。 缺点 死锁检测要耗费大量的 CPU 资源。 假设有 1000 个并发线程要同时更新同一行，每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，一次检测时间复杂度是 O(n)，那么整体死锁检测操作达到 100 万量级。 虽然最终检测的结果可能是没有死锁，但是期间要消耗大量的 CPU 资源。则现象为：现虽然 CPU 利用率很高，但是每秒却执行不了几个事务。 应对死锁检测缺点的策略 确保业务无死锁的情况下，临时关闭死锁检测 操作有风险，可能会出现大量超时，导致业务有损；而出现死锁，进行事务回滚，再通过业务重试恢复的过程是业务无损的。 控制并发度 并发度控制在合适的数量级时，死锁检测的成本很低。 客户端并发控制 客户端数量很多，即使每个客户端都控制在较少的并发线程，汇总到服务端的数量级也很大。因此，并发控制主要做在服务端。 服务端并发控制 对于相同行的更新操作进行排队。措施： 使用中间件（消息队列） 修改 MySQL 源码（由 server 层进入引擎之前排队）]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>collection</tag>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则表达式语法速览]]></title>
    <url>%2F2019%2F03%2F21%2Fregex%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了正则表达式的常用语法。 正则表达式Def描述复杂文本规则的代码，用以匹配符合规则的字符串 转义字符用 “\” 转义。如：表达文本 “\”，表达式中应使用 “\“ 元字符 元字符 含义 表达式 匹配示例 . 除换行符以外任意字符 q.e qwe、qse ^ 字符串的开始（位置） ^qw （行首）qw $ 匹配字符串的结束（位置） we$ we（行尾） \b 单词的开头或结尾，即单词交界处（位置） \d 一位数字 q\dw q1w \s 空白字符(半/全角空格，制表符，换行符等) qw\se qw e \w 任意的字母，数字，下划线等 q\we q_e \A 仅匹配字符串开头 \Aqwe qwe \Ｚ 仅匹配字符串结尾 qwe\Z qwe [\u4E00-\u9FA5] 匹配汉字 q[\u4E00-\u9FA5]e q我e 限定符 限定符 含义 表达式 匹配示例 * 重复零次或更多次 qwe* qweeeee、qw + 重复一次或更多次 qw\s+e qw e ? 重复零次或一次 qw?e qwe、qe {n} 重复 n 次 qw{3}e qwwwe {n,} 重复 n 次或更多次 qw{3,}e qwwwwe {n,m} 重复 n 到 m 次(n ≠ m) qw{5,6}e qwwwwwwe | 分支条件 分支 含义 表达式 匹配示例 &#124; 使用分枝条件时，将会从左到右地测试每个条件，如果满足了某个分枝的话，就不会去再测试后面的条件，因此要注意各个条件的顺序 qw?e &#124; qw*q qe、qwwwwq 分组表达式中，被小括号括起来的子表达式称分组： 表达式左到右顺序按次遇到的左括号及其括号内容即编号第n个分组 分组作为一个整体，后可接数量词：(分组){重复次数} 分组中 “|” 仅分组内有效 分组 含义 表达式 匹配示例 (……) 分组 q(w &#124; e)r qer、qwr \&lt;编号&gt; 引用指定编号分组 (\d)qwe\1 1qwe1 (?&lt;别名&gt;) 为分组指定别名（仍可使用编号） (?P\d)abc(?P=id) 1abc1 (?=&lt;别名&gt;) 引用指定别名分组 (?P\d)abc(?P=id) 1abc1 反义查找不属于该定义的字符 反义代码 说明 表达式 匹配示例 \W 匹配任意不是字母，数字，下划线，汉字的字符 q\We q e \S 匹配任意不是空白符的字符 q\Se q!e \D 匹配任意非数字的字符 q\De qwe \B 匹配不是单词开头或结束的位置 q\Bwe qwe 字符集合 匹配除了字符集合中所有字符以外的任意字符 qwet qrt 后向引用 捕获 含义 (exp) 匹配exp,并捕获文本到自动命名的组里 (?&lt;组名&gt;exp) 匹配exp,并捕获文本到名称为组名的组里，也可以写成(?’组名’exp) (?:exp) 匹配exp,不捕获匹配的文本，也不给此分组分配组号 零宽断言 零宽断言 含义 (?=exp) 匹配exp前面的位置 (?&lt;=exp) 匹配exp后面的位置 (?!exp) 匹配后面跟的不是exp的位置 (?&lt;!exp) 匹配前面不是exp的位置 注释表达式中添加注释： (?#注释) 示例： \d{3}(?#区号)-\d{7} 加粗部分即注释 懒惰限定符匹配任意数量的重复，但是在能使整个匹配成功的前提下使用最少的重复 懒惰限定符 含义 *? 重复任意次，但尽可能少重复 +? 重复1次或更多次，但尽可能少重复 ?? 重复0次或1次，但尽可能少重复 {n,m}? 重复n到m次，但尽可能少重复 {n,}? 重复n次以上，但尽可能少重复]]></content>
      <categories>
        <category>regex</category>
      </categories>
      <tags>
        <tag>regex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成原理]]></title>
    <url>%2F2019%2F01%2F06%2Fcomputer-struct%2F</url>
    <content type="text"><![CDATA[[Updated] 本文梳理了《计算机组成原理》的基础知识 目录 Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapter 7 Chapter 8 总线 存储器 I/O 计算的运算方法 指令系统 CPU 控制单元 控制单元的设计 总线 判优控制 ！当总线上各个主设备同时请求占用总线时，总线控制器按一定优先等级确定某个设备可以占用总线。 ？总线特点为某一刻时刻只允许一个设备向总线发送信息，若两个以上部件同时向总线发送信息，势必导致信号冲突传输无效。 链式查询 1 BR、1 BS、1 BG 优：优先级固定；结构简单、易扩充 缺：电路故障敏感，第i个有故障，第i个以后皆无法工作 计数器定时查询 1 BR、1 BS、1bN设备地址线 优：优先级可不固定；电路故障不如链式查询敏感 缺：控制比链式查询复杂 独立请求 N BR、N BS、N BG 优：响应速度快；优先级控制灵活，可预先固定，也可通过程序改变；可屏蔽设备请求 缺：仲裁线路复杂 通信控制 ！解决通信双方如何获知传输开始和传输结束，以及通信双方如何协调配合。 ？因为总线由众多部件共享，在传送时间上只能用分时方式解决，故通信双方必须按某种约定的方式进行通信。 同步通信 采用公共时钟信号控制，统一传输周期（必须按最慢速度部件设计） 适用：总线长度较短，各部件存取时间相较一致 异步通信 没有公共时钟，采用应答式通信，无固定传输周期 全互锁（完全制约，可靠性最高）/半互锁（简单制约）/不互锁（无制约） 适用：总线各部件速度不一致 分离式通信 总线传输周期分为两个子周期供不同模块占用，总线上无等待时间，最充分发挥了总线的有效占用 半同步通信 既有公共时钟，又允许速度不同的模块和谐工作，采用插入等待周期的措施协调通信双方的配合问题 串行传输与并行传输 串行传输 数据在一条线路上按位依次传输 成本低，但速度慢，适合远距离的传输 并行传输 每个数据位都有一条独立传输线路，所有数据按位同时传输 成本高，速度快，适合近距离、高速传输 总线复用 不同信号（数据/地址）共用同一组物理线路，分时使用 需先给地址信号，然后用地址锁存信号将其保存 总线带宽（MBps）：单位时间总线上可传输数据位数，也称“数据传输率” 影响因素：总线宽度、传输距离、主频 总线带宽 = 一个传输周期传输字节数/时钟周期 | 一个传输周期传输字节数*时钟频率 存储器 芯片：16K×8位/16KB 地址线 = 14根 数据线 = 32根 引出线最少数目 = 数据线+地址线+2 多体结构存储器 将存储器分成若干个（n个）独立的模块，每个模块的容量和存取周期均相等，且可独立进行读写操作。将独立模块： 高位交叉编址 各模块分别响应不同请求源，实现多体并行 高位—存体号，低位—选择存储体内的字 低位交叉编址， 不改变存取周期的前提下，增加存储器带宽，n个模块则带宽提高至n倍 高位—选择存储体内的字，低位—存体号 存取周期T，总线传输周期t，连续读取n个字时间=T+（n-1）t 提高访存的措施 采用高速器件，选用存取周期短的芯片，提高存储器速度 采用缓存，CPU将近期要用的信息先调入缓存，而缓存速度比主存快得多，CPU从缓存存取信息则缩短访存时间，提高了访存速度 调整主存结构，如采用单体多字结构或多体结构（都增加存储器带宽） 程序访问的局部性原理 由于指令和数据在主存的地址分布不是随机的，而是相对地聚簇，故程序执行时对存储器的访问使不均匀的 利用该原理：对缓存-主存，把CPU最近期执行的程序放在容量较小速度较高的缓存中；对主存-辅存，把程序中访问频度高、比较活跃的部分放在主存中。既提高了访存速度又扩大了存储器容量 地址映射（硬件完成） 直接 假设C块缓存，每个主存块j只与一个缓存块i对应：i = j mod C 映射简单，但主存块只能固定对应某个缓存块，不够灵活、命中率低 全相联 主存任一块都可以映射到缓存中的任一块上 灵活、命中率高，但所需电路多、成本高 组相联 把缓存分Q组，组内分R块，主存块号j映射到缓存组号i内任一块：i = j mod Q，缓存内1~R任一块 比直接灵活、命中率高，比全相联成本低，是两者的折中，广泛应用 三级存储系统（平衡—速度、容量、价格） 高速缓存 解决：CPU和主存速度匹配，提高访存速度缓存 管理：硬件和操作系统完成 地址对用户透明 虚存 解决：扩大存储容量 管理：硬件和操作系统完成 CPU不直接访问二级存储器 RAM刷新 方式：集中/分散/异步 原因：存储电荷电容放电 I/O I/O编址方式 独立编址：I/O地址与主存地址分开，不占主存容量，但需要专用I/O指令访I/O 统一编址：在主存地址划出一定范围作I/O地址，通过访存指令访问I/O，但减少了主存容量 主机与I/O交换信息的控制方式 程序查询 主机与I/O串行工作 程序中断 主机与I/O并行工作,主程序和信息传送串行 DMA 主机与I/O并行工作,主程序和信息传送并行 通道 I/O处理机 程序查询 CPU启动I/O后停止现行程序，插入一段程序时刻查询I/O设备准备状况，等待I/O准备就绪时可实现信息交换，存在“踏步”现象 程序中断 管理（多重）中断硬件 中断请求触发器（INT）：标志中断源向CPU提出中断请求 中断屏蔽触发器（MASK）：为“1”表示屏蔽该中断源 排队器：中断判优 向量地址形成部件：产生中断源向量地址 允许中断触发器（EINT）：为“1”允许处理中断 中断标志触发器（INTR）：标志进入中断周期 堆栈：保护现场 中断查询信号电路：每条指令执行周期结束时刻，向各中断源发查询信号 过程： 中断请求：CPU启动I/O设备，I/O准备就绪后向CPU提出中断请求 中断判优：中断判优逻辑选择优先级最高的中断请求，待CPU处理 中断响应：若INT（中断请求触发器）为”1”且请求中断设备未被屏蔽，系统进入中断响应周期—CPU自动执行中断隐指令[ 硬件完成：保护程序断点(即PC内容)、硬件关中断、向量地址送PC（硬件向量法）或中断识别程序入口地址送PC（软件查询法） ] 中断服务：中断响应周期结束，CPU转入取指周期，按向量地址取出无条件转移指令（或按向量地址查入口地址表）；转至向量地址对应的中断程序服务入口地址，开始执行中断服务程序[ 保护现场（PC内容—中断隐指令；寄存器内容—软件编程）、与I/O传送信息、恢复现场 ] 中断返回：中断服务程序最后一条即中断返回指令（返回程序断点） 响应条件和时间： 条件：EINT为“1”（即开中断）；中断请求未被屏蔽，且排队后被选中 时间：指令执行阶段的结束时刻，CPU发出中断查询信号，才能获取中断请求信号 向量地址 存放服务程序入口地址的存储单元地址，由硬件形成 当有中断请求且排队选中时，通过自由组合逻辑电路组成的向量地址形成部件可形成向量地址 输入：排队器；输出：中断周期送至PC；传送：数据总线 开/关中断 EINT为“1”时，允许CPU响应中断；EINT为“0”时，CPU不能响应中断 关中断即将EINT置“0”；开中断即置“1” 多重中断(主要区别在中断服务程序)：CPU处理中断过程中出现新的中断请求，暂停现行中断处理转至处理新的中断 多重中断条件 必须重新开中断 优先级更高的中断请求才能中断现行程序（内部中断&gt;不可屏蔽中断&gt;可屏蔽中断） 单重中断：保护现场-&gt;设备服务-&gt;恢复现场-&gt;开中断-&gt;中断返回 多重中断：保护现场-&gt;开中断-&gt;设备服务-&gt;恢复现场-&gt;中断返回 中断服务程序与调用子程序区别 中断服务程序与中断时CPU正在运行程序相互独立；子程序与CPU正在运行程序是同一程序的两部分 除了软中断，中断通常随机产生；子程序调用由CALL指令引起 中断服务程序入口地址可通过硬件向量法产生向量地址，再由向量地址找到入口地址；子程序调用入口地址由CALL指令地址码给出 中断需要对多个同时发生的中断进行裁决；子程序调用无此操作 都要保护程序断点：前者中断隐指令完成；后者CALL指令完成 都要保护寄存器内容的操作 中断和DMA区别 数据传送：中断靠程序传送；DMA靠硬件传送 CPU响应时间：中断在一条指令执行结束时响应；DMA在存取周期结束时响应 异常处理能力：中断有；DMA无 保护现场：中断需中断现行程序，需保护现场；DMA不需中断现行程序，无需保护现场 优先级：DMA高于中断 DMA 特点： I/O和CPU并行工作 主存和I/O接口间有一条直接数据通路 不中断现行程序，无需保护、恢复现场 DMA请求占用总线时，若采用周期挪用，CPU暂停一个存取周期访问主存，但可继续自身内部操作（如乘法），即DMA传送和主程序并行 硬件：数据缓存寄存器、DAR、AR、WC、中断机构、DMA控制逻辑 过程： 预处理 指明数据传送方向输入（读）/输出（写） 设备地址送DAR（设备地址寄存器） 主存地址送AR（主存地址计数器） 传送数据字数送WC（字计数器） 启动设备 数据传送 主存地址送总线 数据送I/O设备（或主存） 修改主存地址和WC 重复直至数据块传送结束 后处理 由中断服务程序作DMA结束处理（测试传送过程是否出错、决定是否继续使用DMA传送数据） DMA和CPU分时使用主存： 停止CPU访存 DMA在传送数据时独占主存，CPU放弃总线使用权，基本处于不工作或保持原状态，直至DMA传送结束 周期挪用 一旦I/O有DMA请求，由I/O设备挪用一个存取周期。此时CPU可完成自身操作，但要停止访存 DMA和CPU交替访存 适用CPU工作周期比主存存取周期长时。CPU工作周期的上下半周期由DMA和CPU交替使用访存，使DMA传送和CPU工作效率最高，但硬件逻辑复杂 计算的运算方法 判溢出 定点机 参与运算的两个操作数符号相同，结果的符号与原操作数符号不同，则溢出 求和时最高进位与次高进位异或结果为1，则溢出 浮点机判溢出 当阶码大于最大正阶码时，则溢出 当阶码小于最小负阶码时，则按机器零处理 进位：影响加减运算速度的关键 进位链：传递进位的逻辑电路 先行进位：高位进位和低位进位同时产生 单重分组跳跃进位 n位全加器分若干小组，组内进位同时产生，组间串行进位 多重分组跳跃进位 n位全加器分若干大组，若干大组内又包含若干小组，大组内各小组进位同时产生，小组内进位同时产生，大组间串行进位 快于单重，但线路更复杂 指令系统 不同地址格式指令 地址格式 访存次数 备注 四地址 4 A4指出下条指令地址 三地址 4 PC指出下条指令地址 二地址 4 操作结果存回A1、A2或ACC 一地址 4 ACC存放操作数和结果 数据存放方式。存储字长32位，可按字节、半字、字寻址： 边界对准：数据字地址一定是4的整数倍。所存数据不满足该要求时，填充一个或多个空白字节（浪费存储空间） 边界不对准：数据字跨两个存储字时需两次访存，并对高低字节位置进行调整后才能取得数据字（影响取数时间） 间址/基址/变址：可扩大寻址范围 通过访存（多次间址多次访存）得到有效地址 间址 访存导致时间较长（T一次访存 &gt;&gt; T一次寄存器） 地址变换（R+A）得到有效地址 基址 基址寄存器内容由操作系统给定，且在程序执行过程中不可变 支持多道程序技术的应用 变址 变址寄存器内容由用户给定，且在程序执行过程中可变 适用于处理数组问题 相对/堆栈寻址 相对：EA = (PC) + A A为位移量（字节），决定寻址范围；可正可负，补码表示 便于程序浮动，用于转移指令 堆栈：SP +/- ▲ -&gt; PC 有效地址在SP中，指令中可少一个指令字段 ▲与主存编址方式相关：按字编址，▲取1；按字节编址，字长16位时▲取2，字长32时▲取4 RISC（CISC） 选用频度高简单指令，复杂指令功能由简单指令实现（指令系统复杂庞大） 指令长度固定，指令格式种类少，寻址方式种类少（不固定、多、多） 只有LOAD/STORE访存，其余指令皆在寄存器进行（可访存指令不受限制） CPU中有多个通用寄存器（设专用寄存器） 控制器采用组合逻辑控制（微程序） 采用流水技术，大部分指令1个时钟周期内完成（各指令执行时间相差大，大部分需多个时钟周期） 采用优化的编译程序（难以用优化编译生成高效代码） 与CISC比较： 提高指令执行速度 便于设计，可降低硬件设计复杂度 简化指令功能，有利于编译程序代码优化 不易实现指令系统兼容 CPU CPU 功能 指令控制：控制程序的顺序执行 操作控制：产生完成每条指令所需控制命令 时间控制：对各种操作加以时间上的控制 数据加工：对数据进行算术和逻辑运算 中断处理：处理计算机在运行过程中出现的异常情况和特殊请求 组成 寄存器 PC：存放现行指令地址，位数取决于存储器容量 IR：存放现行指令，位数取决于指令字长 通用寄存器：存放数据和地址，位数取决于机器字长 指令译码器 + 控制单元CU：根据指令译码在规定时间发出操作命令 ALU：算术逻辑运算 中断系统：处理中断 指令周期：取指+（间址）+执行+（中断） 执行 -&gt; 中断周期 -&gt; 取值 存取周期 -&gt; DMA周期 -&gt; 存取周期（指令周期任一阶段皆可） 指令流水 结构相关 硬件资源满足不了指令重叠执行的要求，发生资源冲突 如：同一时间，几条重叠的指令分别取值、取数、存数，发生访存冲突 数据相关 指令重叠执行，可能改变操作数的读写访问顺序，导致数据相关冲突 如：某条指令需要用到前面指令的执行结果，而这些指令在流水线中重叠执行，可能改变对操作数读写访问顺序 控制相关 流水线遇到分支指令或其他改变程序计数器PC的指令，造成指令执行顺序的改变 如：某条指令需等前面指令做出转移方向的决定才能进入流水线 流水线多发技术 超标量流水：每个时钟周期内可同时并发多条独立指令，处理器中需配置多个功能部件和指令译码电路，以便同时执行多个操作 超流水线：在原来的时钟周期内，功能部件被使用多次 超长指令字：对编译器要求高，充分挖掘指令间潜在并行性（一个时钟周期内，各功能部件无数据相关），把能并行的指令合成一条具有多个操作码（需相应个数功能部件）的超长指令 中断系统 INTR 与 EINT INTR 中断标志触发器：指示CPU是否进入中断周期 EINT 允许中断触发器：开放或关闭中断系统 置“1”：系统开放，允许中断（开中断指令） 置“0”：关中断（关中断指令、中断隐指令、硬件自动复位） 中断判优：在某一时刻可能有多个中断源（中断源请求随机）提出请求，而CPU只能响应一个，故须判优已解决响应优先次序 硬件排队：组合逻辑电路实现 软件排队：程序按优先级（从高至低）顺序查询各中断源 中断服务程序入口地址寻找 硬件向量法（向量中断）：当有中断请求时，由硬件产生该中断源对应的向量地址，再由向量地址找到服务程序的入口地址，然后暂停现行程序转至中断服务程序 排队器输出 -&gt; 向量地址形成部件 -&gt; 输出向量地址 向量地址寻找入口地址方式 向量地址单元内存放一条无条件转移指令 在向量地址单元内直接存放入口地址，形成一个中断向量地址表 软件查询法：编写中断识别程序实现 屏蔽 屏蔽触发器：内容即屏蔽字，每个中断源对应一个屏蔽字，为“1”时CPU不响应该中断源请求 优先级 响应优先级：CPU响应各中断源请求的优先次序，通常硬件线路已设置好，不便改动（不采用屏蔽时） 处理优先级：CPU实际对各中断源请求的处理优先次序（采用屏蔽） 采用屏蔽技术的中断服务流程 保护现场 -&gt; 置屏蔽字 -&gt; 开中断 -&gt; 中断服务 -&gt; 关中断 -&gt; 恢复现场 -&gt; 恢复屏蔽字 -&gt; 开中断 -&gt; 中断返回 作用 改变处理优先级 为实现多重中断，屏蔽低级别中断请求对现行中断处理程序的干扰 封锁部分中断请求，使程序控制更灵活 控制单元 控制单元CU 功能：发出各种操作命令(即控制信号) 受控制：指令寄存器(操作码)、时钟、标志、系统总线控制信号(中断) 多级时序 指令周期：完成（取出并执行）一条指令所需的时间 机器周期：指令执行过程中一个基准时间，通常以存取周期作为机器周期（因为完成指令都需取指，而一次访存时间固定）。一个机器周期内完成若干微操作，可通过节拍控制产生每一个微操作命令 时钟周期：主频（时钟信号的频率）的倒数，也可称为节拍（时钟信号控制产生，每个节拍宽度对应一个时钟周期）。一个节拍内完成若干需同时执行的操作，是控制计算机操作的最小时间单位 三者关系：每个指令周期含若干个机器周期，可不相等；每个机器周期含若干个时钟周期（节拍），可不相等 机器速度：同主频下， 机器周期中时钟周期数和指令周期中机器周期数不同，机器速度不同。（机器周期中含时钟周期少的机器速度更快） 控制方式 同步控制：微操作受统一基准时标时序信号控制。存取周期不统一时，取最长存取周期作为机器周期 采用定长的机器周期：采用完全统一、具有相同时间间隔和相同数目节拍 采用不定长的机器周期：机器周期内节拍数可不等；大多数微操作一个机器周期内完成，复杂微操作延长机器周期或增加节拍 采用中央控制和局部控制相结合的方法：大部分中央控制，少数局部控制 局部控制每一个节拍T*宽度与中央控制节拍宽度相同 局部控制节拍作为中央控制中机器节拍的延续，插入中央控制的执行周期 异步控制：不存在基准时标信号，微操作时序由专用的应答线路控制。控制器发出某一个微操作命令后，等待执行部件完成该操作时所发回的应答信号，再开始执行下一个操作 联合控制：同步与异步结合。即大多数微操作在同步时序信号控制下进行，而对时间难以确定的微操作（如I/O相关）采用异步控制 控制单元设计 组合逻辑控制器 采用硬连线逻辑：一个微操作命令对于一个逻辑电路 思路清晰，简单明了 结构复杂，线路复杂。一旦构成，除非物理上重新连线，否则无法增加新的控制功能 组合逻辑与微程序控制组成异同 同：均有PC、IR、时序电路、中断系统、状态条件 异： 微操作命令序列形成部件不同。组合逻辑核心部件—门电路；微程序核心部件—控制存储器ROM(存放全部微程序) 微操作命令及节拍安排的主要差别： 取指阶段 12OP(IR) -&gt; ID //组合逻辑：指令操作码送指令译码器OP(IR) -&gt; 微地址形成部件 //微程序：指令操作码送微地址形成部件 微程序每条指令都要增加一个将微指令下地址字段送CMAR的微操作 1Ad(CMDR) -&gt; CMAR 微程序控制器 采用存储逻辑：每条机器指令编写成一个微程序，每一个微程序包含若干条微指令（操作控制字段+顺序控制字段），每一条微指令对应一个或几个微操作命令 优点：规整形、灵活性、可维护性 控制器中微程序个数 = 机器指令数 + 3（取指/间址/中断周期) 组成 控存：存放全部微程序 CMAR（控存地址寄存器）：存放欲读出微指令地址。采用增量计数器法形成后续微指令地址时，有计数功能 CMDR（控存数据寄存器）：存放取出的微指令 顺序逻辑：控制微指令序列 输入：微地址形成部件、微指令下地址字段、外来标志 输出：CPU内部和系统总线的控制信号 微指令编码方式 直接编码（直接控制）：操作控制字段每一位代表一个微命令 简单直观，输出直接用于控制，执行速度快 微指令字较长，使控存容量大 字段直接编码（显示编码）：操作控制字段分段，每个字段经译码发出微操作命令，且互斥 缩短字长，以较少二进制信息表示较多微命令信号 增加译码电路，执行速度降低 字段间接编码（隐式编码）：一个字段某些命令需由另一字段某些微命令解释 更能缩短微指令字长，但速度更慢 微指令序列地址形成 直接由微指令的下地址字段给出 根据机器指令的操作码形成 增量计数器法，即 （CMAR）+ 1 -&gt; CMAR 根据各钟标志决定微指令分支转移的地址 测试网络形成 硬件直接产生微程序入口地址 微指令格式 水平型：一次能定义多个并行操作的微命令。直接编码、字段直接编码、字段间接编码以及直接及混合编码都属于水平型指令格式 大多数微命令可直接控制对象，故每条微指令执行时间短 微指令字长较长，故可用较少微指令数实现一条机器指令的功能 垂直型：采用类似机器指令操作码方式，在微指令中设置微操作码字段，由微操作码规定微指令功能 经过译码控制对象，影响执行时间 微指令字长较短，实现一条机器指令微程序比水平型微指令长的多，以较长微程序结构换取较短微指令结构]]></content>
      <categories>
        <category>计算机基础</category>
      </categories>
      <tags>
        <tag>collection</tag>
        <tag>计组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo + NexT 的问题汇总]]></title>
    <url>%2F2018%2F06%2F30%2Fhexo_problem_list%2F</url>
    <content type="text"><![CDATA[[Updating] 本文记录了 Hexo + NexT 使用过程中的一些问题 版本声明 hexo: 3.9.0 next: 7.0.1 local_search 不能用打开首页（本地or线上），打开浏览器开发工具 Network 选项卡，点击首页“搜索”按钮，观察 search.xml 状态： 200 问题：Algolia 问题 解决：主题配置文件中关闭 Algolia 404 其他 问题：存在非法字符 解决：sublime 排查 .md 文件中所存在非法字符 post_meta 不显示更新时间修改主题配置文件，post_meta 模块中设置：123updated: enable: true anotherday: false 修改新建文章模板hexo 会根据 scaffolds 文件夹内相应的模板文件来建立文件：1hexo new &lt;模板：post/page/draft&gt; &lt;filename&gt; -- 以 /scaffolds/&lt;模板：post/page/draft&gt;.md 为模板新建名为 &lt;filename&gt; 的文章 post 模板添加内容如下：123456789101112---title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;updated: &#123;&#123; date &#125;&#125;tags:categories:---&lt;font color=#008000&gt; [Updated] &lt;/font&gt; 已完成&lt;font color=#FF0000&gt; [Updating] &lt;/font&gt; 更新中&lt;!--more--&gt; 数学公式不显示 更换渲染引擎为 hexo-renderer-marked 12npm uninstall hexo-renderer-marked --save # 卸载 hexo 默认引擎 hexo-renderer-markednpm install hexo-renderer-kramed --save Next 配置文件开启 MathJax 找到 math 模块，开启： 1234math： enable： true # 此处设为 true …… 在需要渲染公式的文章开头的 Front-matter 设置 MathJax 123456---title: ……date: ……tags:mathjax: true # 添加此项--- 图片不显示传统的 Makrdown 插入图片的语法无法显示图片。 查阅官方文档，发现可用资源文件来解决。 修改 hexo 的配置文件 _config.yml，将 post_asset_folder 选项设置为 true； 插入图片的语法： 1&#123;% asset_img example.jpg This is an example image %&#125; 在资源文件夹中放入图片 example.jpg 即可。 注：安装 hexo-asset-image 插件，每次新建文章后会自动在 source/_post 文件夹下创建文章同名的资源文件夹：1npm install hexo-asset-image --save]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Github + Hexo 搭建博客]]></title>
    <url>%2F2018%2F05%2F30%2Fhexo%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了使用Github和Hexo搭建该博客的过程 环境Windows/Ubuntu 皆可。 Hexo 的安装Node.js 的安装安装方法 官网下载指定版本 先下载 nvm，再通过 nvm 下载并管理切换不同版本的 Node.js（解决 hexo 版本不兼容） 安装结果测试12nodejs -v # 显示 Node.js 版本信息npm -v # 显示 npm 版本信息 注： 可能需重启 bash Windows 下可能需要手动添加环境变量 优化选项加快 npm 下载有以下优化方向： 安装时使用指定国内镜像 1npm install -gd express --registry=http://registry.npm.taobao.org 使用 cnpm 替代 npm 12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm -v # 查看 cnpm 版本 Hexo 的安装安装12npm install -g hexo # 全局安装hexo -v # 显示 hexo 版本信息则安装成功 生成 blog1234mkdir blog &amp;&amp; cd blog # 创建存放 hexo blog 的文件夹hexo init # 初始化 hexo 基础配置文件hexo generate # 生成静态文件hexo s # 启动服务，访问 http://localhost:4000 即可看到blog 内容 Hexo 搭建在 Github 上Git 的安装详见 git 相关笔记。 blog 推送至 GitHub Github 创建 Repository Repository 的名称为 12342. 本地生成 blog 静态文件并推送至 Github 1. 先修改 _config.yml，添加 deploy 信息如下 deploy: type: git repo: https://github.com/&lt;github_username&gt;/&lt;github_username&gt;.github.io.git branch: master 1234567892. 生成 blog 并推送，步骤如下： ```shell # 标准流程 hexo clean # 1.清除已生成内容 hexo generate # 2.生成 blog 静态文件 hexo deploy # 3.将 public 目录中的文件和目录推送至 _config.yml 中 deploy 指定仓库与分支，并且完全覆盖该分支下的已有内容 # 一步到位 hexo g -d # 生成并推送 打开 https://&lt;github_username&gt;.github.io 即可见 blog。 404 可能原因及相应解决方案如下： 可能原因 解决方案 前网络条件下无法访问 挂梯子 缓存未更新 清缓存；等待一段时间 Github Repository 状态为 private Repository 状态改为 public]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从 pip 到 Pipfile]]></title>
    <url>%2F2018%2F03%2F21%2Fpip%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 Python 软件包管理相关的 pip、requirements.txt、Pipfile，以及 Python 虚拟环境的内容。 从 pip 到 Pipfilepippip 是一个通用的 Python 软件包管理工具，可以从 Python 官方的第三方库仓库 PyPI （默认）或其他索引安装软件包。 pip 安装注：Python 2.7.9 + 或 Python 3.4+ 内置 pip。 在终端使用以下命令在当前全局 Python 解释器下安装 pip： sudo python get-pip.py 查看安装版本： pip --version 升级 pip 到最新版本： pip install -U pip pip 使用查看 pip 用法： pip --help pip 常用命令： 查看已安装包列表 pip list 查找/安装/卸载包 pip search/install/uninstall &lt;Package&gt; # 安装时使用 “&lt;Package&gt;=版本号” 可指定下载版本 升级包 pip install --upgrade &lt;Package&gt; 查看包的详细信息 pip show -f &lt;Package&gt; 虚拟环境虚拟环境作用Python 虚拟环境允许将 Python 包安装在特定应用程序的隔离位置，而不是全局安装，避免了不同程序因 Python 包的依赖版本不同而导致彼此的运行受到影响。 因为不同的项目通常会依赖不同版本的库或 Python 版本，所以建议使用虚拟环境为每一个项目创建独立的 Python 环境。 虚拟环境的使用 安装虚拟环境： pip install virtualenv 项目根目录下创建虚拟环境： virtualenv --no-site-packages &lt;虚拟环境名&gt; # --no-site-packages 参数：不复制全局 python 解释器下的任何第三方库 此时，项目根目录下会创建 venv 目录，用以存放虚拟环境。 激活虚拟环境： source venv/bin/activate 此时，终端前缀会出现字样“(&lt;虚拟环境名&gt;)”，表示已进入虚拟环境。 关闭当前虚拟环境： deactivate 此时，终端前缀消失，回到全局环境。 requirements.txt为了解决不同环境下构建项目时可能产生的依赖问题，Python 项目中一般会包含 requirements.txt 文件，文件内记录了该项目所使用到全部依赖包及其精确版本号，以便在新环境中安装该项目所需要的运行环境依赖。 生成 requirements.txt在 Python 项目的根目录终端执行以下命令： pip freeze &gt; requirements.txt 使用 requirements.txt使用 requirements.txt 安装依赖 pip install -r requirements.txt # -r 可以防止安装过程中的某个错误而提前终止安装 缺点requirements.txt 是一个简单的纯文本文件，需要手动维护，灵活性低。 Pipfile 和 Pipfile.lockPipfilePipfile 与 Pipfile.lock 是社区拟定的依赖管理文件，用于替代 requirements.txt。一个项目对应一个 Pipfile，支持开发环境与正式环境区分。默认提供 default 和 development 区分。 与 requirements.txt 区别： Pipfile 文件是 TOML 格式； requirements.txt 是纯文本。 Pipfile.lock根据 Pipfile 和当前环境自动生成的 JSON 格式的依赖文件。 注： 不可手动修改！ pipenvpipenv 结合了 Pipfile 、pip 和 virtualenv。其作用主要有如下几点： 自动在项目目录的 .venv 目录创建虚拟环境。 自动生成 Pipfile 和 Pipfile.lock。 自动管理 Pipfile 新安装和删除的包。 使用 pipenv 创建虚拟环境并安装 Pipfile 中所列的所有包： 在 Python 项目的根目录终端执行以下命令： pipenv install 此时，当前项目根目录下会创建 .venv 文件夹，并安装 Pipfile 中的所有第三方软件包。 确认 Pipfile 中所有包已安装： pipenv lock 确认并根据安装版本生成 Pipfile.lock。 激活虚拟环境： 在 Python 项目的根目录终端执行以下命令： pipenv shell 运行程序： pipenv run python **.py # 运行 python 文件 pipenv run flask run # 启动 flask 程序 注： 这里会调用虚拟环境中的 python 解释器，而不是全局的 python 解释器。 在激活虚拟环境后，pipenv run 可以省略，但养成使用 pipenv run 的习惯可以避免运行时忘记启用虚拟环境。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pip</tag>
        <tag>虚拟环境</tag>
        <tag>pipfile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 命令]]></title>
    <url>%2F2018%2F02%2F27%2Fmycli-cmd%2F</url>
    <content type="text"></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 使用速览]]></title>
    <url>%2F2018%2F02%2F23%2Flinux-quick-use%2F</url>
    <content type="text"><![CDATA[[Updated] 本文整理了能够快速上手 Linux 的一些常用操作和基本知识。 Linux 使用速览终端快捷键进程控制 Ctrl+d ： 键盘输入结束或退出终端 Ctrl+C ： 杀死当前进程(也可以用来清空当前行) Ctrl+s ： 暂停当前程序，暂停后按下任意键恢复运行 Ctrl+z ： 将当前程序放到后台运行，恢复到前台为命令fg 编辑控制 Ctrl+A ： 将光标移至输入行头，相当于Home键 Ctrl+E ： 将光标移至输入行末，相当于End键 Ctrl+F ： 向前移动一个字符 Ctrl+B ： 向后移动一个字符 Ctrl+U ： 剪切文本直到行的起始(可以用于清空行) Ctrl+K ： 剪切文本直到行的末尾 Ctrl+Y ： 粘贴最近剪切的文本 Alt+Backspace ： 向前删除一个单词 Ctrl+P / Ctrl+N ： 上下历史记录，相当于↑↓键 Shift+PgUp ： 将终端显示向上滚动 Shift+PgDn ： 将终端显示向下滚动 通配符 通配符 含义 * 匹配 0 或多个字符 ? 匹配任意一个字符 [list] 匹配 list 中的任意单一字符 [!list] 匹配 除list 中的任意单一字符以外的字符 [c1-c2] 匹配 c1-c2 中的任意单一字符 如：[0-9] [a-z] {string1,string2,…} 匹配 string1 或 string2 (或更多)其一字符串 {c1..c2} 匹配 c1-c2 中全部字符 如{1..10} 获取帮助man 的用法1man k（区段） 内容 如： 1man 1（一般命令） ls（命令） 区段 k 含义 1 一般命令 2 系统调用 3 库函数，涵盖了C标准函数库 4 特殊文件（通常是/dev中的设备）和驱动程序 5 文件格式和约定 6 游戏和屏保 7 杂项 8 系统管理命令和守护进程 man 手册快捷键 /关键字 ： 搜索 n ： 切换到下一个关键字所在处 shift+n ： 切换到上一个关键字所在处 Space ： 翻页 Enter ： 向下滚动一行 用户及文件权限管理用户管理 查看用户 1who -[a/d/q/u] 创建用户 1sudo adduser USERNAME 删除用户 1sudo deluser USERNAME ：remove-home 用户组 查找所属用户组 1groups USERNAME 查看 /etc/group 1cat /etc/group | sort 将其他用户加入 sudo 用户组 1sudo usermod -G sudo USERNAME 权限管理查看权限 文件类型 d ： 目录 l ： 软链接 b ： 块设备 c ： 字符设备 s ： socket p ： 管道 文件权限 r ： 读 w ： 写 x ： 执行 链接数 连接到该文件所在的 inode 结点的文件名数目 文件大小 以 inode 结点大小为单位来表示的文件大小（ls -lh：直观的查看文件的大小） 查看权限1ll FILENAME 显示内容依次为： 1drwxrwxrwx INODE_LINKS USERNAE GROUPNAME INODE_SIZE TIME FILENAME 修改文件所有者1sudo chown USEENAME FILENAME 修改文件权限 二进制修改：rwx (421) 1chmod 777 FILENAME 加减赋值修改：[ u | g | o ] [+ | -] [r | w | x] 1chmod ugo+rwx FILENAME Linux 文件基本操作及目录结构基本操作文件操作文件名以 “test_file” 为例；以下&lt;位置&gt;默认为当前工作目录&lt;.&gt; 创建 1touch &lt;创建位置&gt;/test_file 移动/重命名 1mv &lt;文件位置&gt;/test_file &lt;移动位置&gt; 重命名 当移动位置为不存在的目录名，即将文件 test_file 重命名为该目录名 复制 1cp &lt;文件位置&gt;/test_file &lt;移动位置/复制名&gt; 删除 1rm &lt;文件位置&gt;/test_file &lt;移动位置/复制名&gt; 查看 1cat test_file &lt;移动位置/复制名&gt; 一次性查看文件所有内容 1less test_file &lt;移动位置/复制名&gt; 可以上下键翻页查看内容，按 q 退出 1head/tail test_file &lt;移动位置/复制名&gt; 查看文件内容的前/后10行 参数 (-n 行数)：查看前/后几行 编辑 通常 Linux 的自带编辑器有终端下的 vi/vim 编辑器（vim 可看做 vi 的高级版，对 vi 完全兼容），以及可视化编辑器 gedit。使用下方命令新建或打开已有文件： 12vi file.txtgedit file.txt 目录操作目录名以 “test_dir” 为例；缺省位置为当前工作目录&lt;.&gt; 创建 1mkdir test_dir &lt;创建位置/目录名&gt; 移动/重命名 1mv test_dir &lt;移动位置/目录名&gt; 重命名 当移动位置为不存在的目录名，即将目录 test_dir 重命名为该目录名 复制目录及目录内容 1cp -r test_dir &lt;创建位置/目录名&gt; 删除 1rm test_dir &lt;删除位置/目录名&gt; 参数 -r：递归删除 -f：强制删除 Linux 目录结构 / : 根目录 bin : 一般用户可用，启动时用到的命令 boot : 启动项 grub : 开关机设置相关文件 内核文件（vmlinuz） dev : 存放设备文件 etc : 包含系统特有的可配置文件，即用于控制程序运行的本地文件 home : 用户家目录 lib : 用于存放程序的动态库和模块文件 media : 挂载本地磁盘或其他存储设备。如：cdrom、floppy、U 盘 mnt : 用于挂载其他临时文件系统 opt : 发行版附加软件包的安装目录 root : 根用户的家 sbin : 存放大多数 root 用户才能执行的命令，系统进行更新、备份、还原和开关机所用的命令 srv : 存放服务进程所需的数据文件（如 ftp 服务）和一些服务的执行脚本 tmp : 存放各种临时文件 usr : 存储只读用户数据的第二层次，包含大多数用户工具和应用程序 bin : 非必要可执行文件，面向所有用户 include : 包含标准头文件 lib : /usr/bin 和 /usr/sbin 中二进制文件的库 local : 本地数据的第三层次，具体到本台主机 bin etc include lib share src share : 共享数据 sbin : 非必要的系统二进制文件 src : 源代码 var : 变量文件 account cache lib lock log run tmp spool mail]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 数学公式语法]]></title>
    <url>%2F2018%2F01%2F28%2Fmarkdown-formula%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 Markdown 数学公式语法 Markdown 数学公式语法公式公式类型行内公式将公式插入到本行内。使用一个 “$” 包裹数学公式。 极限：$\displaystyle \lim_{n \to +\infty}{x_n = \alpha}$ 独行公式将公式插入到新的一行内，并且居中。使用两个 “$$” 包裹数学公式。极限： \displaystyle \lim_{n \to +\infty}{x_n = \alpha}上下标 上下标 语法 预览 ^ x^2 $x^2$ _ x_1 $x_1$ 运算符普通运算 运算 语法 预览 加 + $+$ 减 - $-$ 乘 \times $\times$ 除 \div $\times$ 开方 \sqrt[n]{x} $\sqrt[n]{x}$ 加减 \pm $\pm$ 减加 \mp $\mp$ 绝对值 \ \ $ x+y $ 分数 \frac{b}{a} $\frac{b}{a}$ 分数 {b} \voer {a} ${b} \over {a}$ 对数运算 运算 语法 预览 对数 \log $\log$ ln \ln $\ln$ lg \lg $\lg$ 集合运算 运算 语法 预览 空集 \emptyset $\emptyset$ 属于 \in $\in$ 不属于 \notin $\notin$ ⊂ \subset $\subset$ ⊃ \supset $\supset$ ⊆ \subseteq $\subseteq$ ⊇ \supseteq $\supseteq$ ⊇ \bigcap $\bigcap$ ⋃ \bigcup $\bigcup$ ⋁ \bigvee $\bigvee$ ⋀ \bigwedge $\bigwedge$ 逻辑运算 运算 语法 预览 因为 \because $\because$ 所以 \therefore $\therefore$ 任取 \forall $\forall$ 存在 \exists $\exists$ 不等于 \neq $\neq$ 不属于 \not\subset $\not\subset$ 微积分运算 运算 语法 预览 极限 \lim $\lim$ 无穷 \infty $\infty$ 积分 \int $\int$ 重积分 \int（i 的个数为重数） $\iiint$ 曲线积分 \oint $\oint$ 三角函数 运算 语法 预览 度数 90^\circ $90^\circ$ ∠ \angle $\angle$ sin \sin $\sin$ cos \cos $\cos$ tan \tan $\tan$ 希腊字母 字母 语法 预览 Δ \Delta $\Delta$ Θ \Theta $\Theta$ Σ \Sigma（’S’ 大写） $\Sigma$ Ω \Omega（’O’ 大写） $\Omega$ α \alhpa $\alpha$ β \beta $\beta$ γ \gamma $\gamma$ δ \delta $\delta$ ϵ \epsilon $\epsilon$ η \eta $\eta$ θ \theta $\theta$ κ \kappa $\kappa$ λ \lambda $\lambda$ μ \mu $\mu$ ν \nu $\nu$ π \pi $\pi$ σ \sigma（’s’ 小写） $\sigma$ τ \tau $\tau$ ω \omega（’o’ 小写） $\omega$ ξ \xi $\xi$ ρ \rho $\rho$ 栗子求和公式\displaystyle \sum_{i=1}^n \frac{1}{i} 预览： \displaystyle \sum_{i=1}^n \frac{1}{i}求积公式\displaystyle \prod_{i=1}^n i 预览： \displaystyle \prod_{i=1}^n i求积分\displaystyle \int_0^{1} x dx 预览： \displaystyle \int_0^{1} x dx求极限\displaystyle \lim_{n \rightarrow +\infty} \frac{1}{n(n+1)} 预览： \displaystyle \lim_{n \rightarrow +\infty} \frac{1}{n}]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown 基础语法]]></title>
    <url>%2F2018%2F01%2F22%2Fmarkdown-basic%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 Markdown 基础语法 Markdown 基础语法标题语法123456# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 预览一级标题二级标题三级标题四级标题五级标题六级标题 字体语法123456*这是斜体*_这是斜体_**这是粗体**__这是粗体__***这是粗斜体***___这是粗斜体___ 预览这是斜体 这是斜体 这是粗体 这是粗体 这是粗斜体 这是粗斜体 列表语法无序列表12345678* 第一项* 第二项+ 第一项+ 第二项- 第一项- 第二项 有序列表1231. 第一项2. 第二项3. 第三项 嵌套列表1234567891. 第一项2. 第二项 - 第一项 - 第二项- 第一项 - 第一项 - 第二项- 第二项 预览无序列表 第一项 第二项 第一项 第二项 第一项 第二项 有序列表 第一项 第二项 第三项 嵌套列表 第一项 第二项 第一项 第二项 第一项 第一项 第二项 第二项 区块语法12345678910111213&gt; 最外层&gt;&gt; 第一层嵌套&gt;&gt;&gt; 第二层嵌套&gt; 区块列表&gt; 1. 第一项&gt; 2. 第二项&gt; 3. 第三项- 第一项 &gt; 外层区块 &gt; 内外层区块- 第二项 预览 最外层 第一层嵌套 第二层嵌套 区块列表 第一项 第二项 第三项 第一项 区块 第二项 代码块语法单行代码1`import sys` 多行代码123```python &lt;-指定语言`import sys````. 或 1import sys &lt;-一个Tab指定代码区块 预览单行代码import sys 多行代码1import sys 或 import sys 表格语法1234col1 | col2 | col3- | - | -row1_1 | row1_2 | row1_3row2_1 | row2_2 | row2_3 预览 col1 col2 col3 row1_1 row1_2 row1_3 row2_1 row2_2 row2_3 插入链接/图片语法插入普通链接123[本站地址](https://xhzs.github.io/)这是本站地址：&lt;[链接地址](https://xhzs.github.io/)&gt; 插入图片1&lt;img src="图片的本地/线上地址" width="50%"&gt; 预览普通链接这是本站地址 这是本站地址：[链接地址](https://xhzs.github.io/) 图片]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 chrome 下 jupyter-notebook 异常问题]]></title>
    <url>%2F2018%2F01%2F16%2Fchrome-jupyter-exception%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 chrome 运行 jupyter-notebook 所出现的异常及解决方法 解决 chrome 下 jupyter-notebook 异常问题问题chrome 下使用 jupyter-notebook 出现以下问题： 按 tab 键补全后自动移至下一单元格 括号自动补全出问题。如：按 “(“ 打印 “(()” 原因开启了某些 chrome 扩展程序所致 解决方法采用排除法测试出引起问题的 chrome 拓展程序，将其关闭]]></content>
      <categories>
        <category>chrome 使用问题</category>
      </categories>
      <tags>
        <tag>chrome</tag>
        <tag>jupyter-notebook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决 chrome 安装插件失败问题]]></title>
    <url>%2F2018%2F01%2F14%2Fchrome-install-extension%2F</url>
    <content type="text"><![CDATA[[Updated] 本文记录了 chrome 离线安装插件失败的问题及解决方法 解决 chrome 安装插件失败问题问题chrome 离线安装插件失败。页面左上角显示： Package is invalid:’CRX_HEADER_INVALID’ 解决以安装 example.crx 为例： 备份 example.crx，以防误操作或错误情况 将 example.crx 更改后缀名为 .rar 或 .zip 解压 example.rar(.zip) 到 example 文件夹，文件夹内包含：example.js、icon.png、manifest.json 打开 chrome 拓展页面，右上角开关开启开发者模式后，左上角点击“加载已解压拓展程序”，选择步骤 3 中解压的 example 文件夹，安装成功]]></content>
      <categories>
        <category>chrome 使用问题</category>
      </categories>
      <tags>
        <tag>chrome</tag>
      </tags>
  </entry>
</search>
